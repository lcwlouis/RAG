{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d4475e4",
   "metadata": {},
   "source": [
    "# Generate Queries to test RAG System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a571d21",
   "metadata": {},
   "source": [
    "We put the entire text of the file into context and send it to Gemini-2.5-Flash and ask it to generate a list of queries that we can use to test the RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bcb68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yy/t6y4brw52tvdr8k9xbwc01d40000gn/T/ipykernel_17908/2799777735.py:61: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  response_schema=OutputSchema.schema()\n",
      "Key '$defs' is not supported in schema, ignoring\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\\n  \"test_cases\": [\\n    {\\n      \"citations\": [\\n        \"# Code owners file.\",\\n        \"This file controls who is tagged for review for any given pull request.\",\\n        \"*                                      @GoogleCloudPlatform/teams/generative-ai-devrel\"\\n      ],\\n      \"expected_answer\": \"The code owner for all files is the GitHub team `@GoogleCloudPlatform/teams/generative-ai-devrel`.\",\\n      \"query\": \"Who is the code owner for this repository?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"name: Bug report\",\\n        \"about: Create a report to help us improve\",\\n        \"title: \\'\\'\",\\n        \"labels: \\'\\'\",\\n        \"assignees: \\'\\'\",\\n        \"**Describe the bug**\",\\n        \"A clear and concise description of what the bug is.\",\\n        \"**To Reproduce**\",\\n        \"Steps to reproduce the behavior:\",\\n        \"1. Go to \\'...\\'\",\\n        \"2. Click on \\'....\\'\",\\n        \"3. See error\",\\n        \"**Expected behavior**\",\\n        \"A clear and concise description of what you expected to happen.\",\\n        \"**Screenshots**\",\\n        \"If applicable, add screenshots to help explain your problem.\",\\n        \"**Versions**\",\\n        \" - OS: [e.g. Windows, Mac, Linux]\",\\n        \" - ADK version:\",\\n        \" - Python version:\",\\n        \"**Additional context**\",\\n        \"Add any other context about the problem here.\"\\n      ],\\n      \"expected_answer\": \"The bug report template includes sections for describing the bug, steps to reproduce, expected behavior, screenshots, versions (OS, ADK, Python), and additional context.\",\\n      \"query\": \"What information should be included in a bug report?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"name: Feature request\",\\n        \"about: Suggest an idea for this project\",\\n        \"title: \\'\\'\",\\n        \"labels: \\'\\'\",\\n        \"assignees: \\'\\'\",\\n        \"**Is your feature request related to a problem? Please describe.**\",\\n        \"A clear and concise description of what the problem is.\",\\n        \"**Describe the solution you\\'d like**\",\\n        \"A clear and concise description of what you want to happen.\",\\n        \"**Describe alternatives you\\'ve considered**\",\\n        \"A clear and concise description of any alternative solutions or features you\\'ve considered.\",\\n        \"**Additional context**\",\\n        \"Add any other context or screenshots about the feature request here.\"\\n      ],\\n      \"expected_answer\": \"A feature request should describe the problem it\\'s related to, the desired solution, any alternative solutions considered, and additional context or screenshots.\",\\n      \"query\": \"What are the key sections of a feature request?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"allowedCopyrightHolders:\",\\n        \"  - \\\\\"Google LLC\\\\\"\",\\n        \"allowedLicenses:\",\\n        \"  - \\\\\"Apache-2.0\\\\\"\",\\n        \"ignoreFiles:\",\\n        \"  - \\\\\"**/requirements*.txt\\\\\"\",\\n        \"  - \\\\\"**/__init__.py\\\\\"\",\\n        \"  - \\\\\"**/constraints*.txt\\\\\"\",\\n        \"ignoreLicenseYear: true\",\\n        \"sourceFileExtensions:\",\\n        \"  - \\\\\"py\\\\\"\"\\n      ],\\n      \"expected_answer\": \"The header checker lint configuration allows \\\\\"Google LLC\\\\\" as a copyright holder and \\\\\"Apache-2.0\\\\\" as a license. It ignores license year and processes files with the \\\\\".py\\\\\" extension. Files matching \\\\\"**/requirements*.txt\\\\\", \\\\\"**/__init__.py\\\\\", and \\\\\"**/constraints*.txt\\\\\" are ignored.\",\\n      \"query\": \"What are the configurations for the header checker lint?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"  \\\\\"extends\\\\\": [\",\\n        \"    \\\\\"config:recommended\\\\\"\",\\n        \"  ],\",\\n        \"  \\\\\"prConcurrentLimit\\\\\": 0,\",\\n        \"  \\\\\"rebaseWhen\\\\\": \\\\\"never\\\\\",\",\\n        \"  \\\\\"dependencyDashboard\\\\\": true\"\\n      ],\\n      \"expected_answer\": \"The Renovate configuration extends `config:recommended`, sets `prConcurrentLimit` to 0, `rebaseWhen` to \\\\\"never\\\\\", and enables `dependencyDashboard`.\",\\n      \"query\": \"What are the general settings in the Renovate configuration file?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"  \\\\\"pip_requirements\\\\\": {\",\\n        \"    \\\\\"fileMatch\\\\\": [\",\\n        \"      \\\\\"requirements.txt\\\\\",\",\\n        \"      \\\\\"requirements-test.txt\\\\\",\",\\n        \"      \\\\\"requirements-composer.txt\\\\\",\",\\n        \"      \\\\\"constraints.txt\\\\\",\",\\n        \"      \\\\\"constraints-test.txt\\\\\"\",\\n        \"    ]\",\\n        \"  }\"\\n      ],\\n      \"expected_answer\": \"For pip requirements, Renovate matches files named \\\\\"requirements.txt\\\\\", \\\\\"requirements-test.txt\\\\\", \\\\\"requirements-composer.txt\\\\\", \\\\\"constraints.txt\\\\\", and \\\\\"constraints-test.txt\\\\\".\",\\n      \"query\": \"Which files are matched for pip requirements in Renovate?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"  \\\\\"ignorePaths\\\\\": [\",\\n        \"    \\\\\"**/target/**\\\\\"\",\\n        \"  ]\"\\n      ],\\n      \"expected_answer\": \"Renovate is configured to ignore paths under \\\\\"**/target/**\\\\\".\",\\n      \"query\": \"What paths does Renovate ignore?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"    {\",\\n        \"      \\\\\"matchDatasources\\\\\": [\\\\\"maven\\\\\"],\",\\n        \"      \\\\\"matchFilePatterns\\\\\": [\\\\\"pom.xml\\\\\"],\",\\n        \"      \\\\\"groupName\\\\\": \\\\\"Java Maven Dependencies\\\\\"\",\\n        \"    }\"\\n      ],\\n      \"expected_answer\": \"Maven dependencies matching `pom.xml` files are grouped under \\\\\"Java Maven Dependencies\\\\\".\",\\n      \"query\": \"How are Maven dependencies grouped in Renovate?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"    {\",\\n        \"      \\\\\"matchDatasources\\\\\": [\\\\\"pypi\\\\\"],\",\\n        \"      \\\\\"matchFilePatterns\\\\\": [\\\\\"requirements.txt\\\\\"],\",\\n        \"      \\\\\"groupName\\\\\": \\\\\"Python pip Dependencies\\\\\"\",\\n        \"    }\"\\n      ],\\n      \"expected_answer\": \"PyPI dependencies matching `requirements.txt` files are grouped under \\\\\"Python pip Dependencies\\\\\".\",\\n      \"query\": \"How are Python pip dependencies grouped in Renovate?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"    {\",\\n        \"      \\\\\"separateMinorPatch\\\\\": true,\",\\n        \"      \\\\\"matchPackageNames\\\\\": [\",\\n        \"        \\\\\"/pytest/\\\\\"\",\\n        \"      ]\",\\n        \"    }\"\\n      ],\\n      \"expected_answer\": \"For packages matching \\\\\"/pytest/\\\\\", minor and patch updates are separated.\",\\n      \"query\": \"What is the rule for pytest package updates in Renovate?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"    {\",\\n        \"      \\\\\"matchUpdateTypes\\\\\": [\",\\n        \"        \\\\\"minor\\\\\"\",\\n        \"      ],\",\\n        \"      \\\\\"extends\\\\\": [\",\\n        \"        \\\\\"schedule:monthly\\\\\"\",\\n        \"      ]\",\\n        \"    }\"\\n      ],\\n      \"expected_answer\": \"Minor updates are scheduled monthly.\",\\n      \"query\": \"What is the schedule for minor updates in Renovate?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"    {\",\\n        \"      \\\\\"matchUpdateTypes\\\\\": [\",\\n        \"        \\\\\"patch\\\\\"\",\\n        \"      ],\",\\n        \"      \\\\\"extends\\\\\": [\",\\n        \"        \\\\\"schedule:quarterly\\\\\"\",\\n        \"      ]\",\\n        \"    }\"\\n      ],\\n      \"expected_answer\": \"Patch updates are scheduled quarterly.\",\\n      \"query\": \"What is the schedule for patch updates in Renovate?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"    {\",\\n        \"      \\\\\"matchDatasources\\\\\": [\\\\\"maven\\\\\"],\",\\n        \"      \\\\\"matchUpdateTypes\\\\\": [\\\\\"minor\\\\\"],\",\\n        \"      \\\\\"groupName\\\\\": \\\\\"Java Minor Updates\\\\\",\",\\n        \"      \\\\\"extends\\\\\": [\\\\\"schedule:monthly\\\\\"]\",\\n        \"    }\"\\n      ],\\n      \"expected_answer\": \"Maven minor updates are grouped as \\\\\"Java Minor Updates\\\\\" and scheduled monthly.\",\\n      \"query\": \"How are Java minor updates handled by Renovate?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"    {\",\\n        \"      \\\\\"matchDatasources\\\\\": [\\\\\"maven\\\\\"],\",\\n        \"      \\\\\"matchUpdateTypes\\\\\": [\\\\\"patch\\\\\"],\",\\n        \"      \\\\\"groupName\\\\\": \\\\\"Java Patch Updates\\\\\",\",\\n        \"      \\\\\"extends\\\\\": [\\\\\"schedule:quarterly\\\\\"]\",\\n        \"    }\"\\n      ],\\n      \"expected_answer\": \"Maven patch updates are grouped as \\\\\"Java Patch Updates\\\\\" and scheduled quarterly.\",\\n      \"query\": \"How are Java patch updates handled by Renovate?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"  \\\\\"vulnerabilityAlerts\\\\\": {\",\\n        \"    \\\\\"schedule\\\\\": [\",\\n        \"      \\\\\"at any time\\\\\"\",\\n        \"    ]\",\\n        \"  }\"\\n      ],\\n      \"expected_answer\": \"Vulnerability alerts are scheduled to run \\\\\"at any time\\\\\".\",\\n      \"query\": \"When are vulnerability alerts scheduled to run according to Renovate config?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"  \\\\\"platformAutomerge\\\\\": true,\",\\n        \"  \\\\\"automergeType\\\\\": \\\\\"branch\\\\\"\"\\n      ],\\n      \"expected_answer\": \"Platform automerge is enabled, and the automerge type is \\\\\"branch\\\\\".\",\\n      \"query\": \"What are the automerge settings in Renovate?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"name: Publish Docs\",\\n        \"on:\",\\n        \"  push:\",\\n        \"    branches:\",\\n        \"      - main\",\\n        \"permissions:\",\\n        \"  contents: write\"\\n      ],\\n      \"expected_answer\": \"The \\\\\"Publish Docs\\\\\" workflow is triggered on pushes to the `main` branch and has `write` permissions for `contents`.\",\\n      \"query\": \"What triggers the \\'Publish Docs\\' workflow and what permissions does it have?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"jobs:\",\\n        \"  deploy:\",\\n        \"    runs-on: ubuntu-latest\",\\n        \"    steps:\",\\n        \"      - uses: actions/checkout@v4\",\\n        \"      - name: Configure Git Credentials\",\\n        \"        run: |\",\\n        \"          git config user.name github-actions[bot]\",\\n        \"          git config user.email 41898282+github-actions[bot]@users.noreply.github.com\",\\n        \"      - uses: actions/setup-python@v5\",\\n        \"        with:\",\\n        \"          python-version: 3.x\",\\n        \"      - run: echo \\\\\"cache_id=$(date --utc \\'+%V\\')\\\\\" >> $GITHUB_ENV\",\\n        \"      - uses: actions/cache@v4\",\\n        \"        with:\",\\n        \"          key: mkdocs-material-${{ env.cache_id }}\",\\n        \"          path: .cache\",\\n        \"          restore-keys: |\",\\n        \"            mkdocs-material-\",\\n        \"      - run: pip install -r requirements.txt\",\\n        \"      - run: mkdocs gh-deploy --force\"\\n      ],\\n      \"expected_answer\": \"The \\'deploy\\' job in the \\'Publish Docs\\' workflow runs on `ubuntu-latest`. It checks out the repository, configures Git credentials for `github-actions[bot]`, sets up Python 3.x, caches `mkdocs-material`, installs dependencies from `requirements.txt`, and then deploys the documentation using `mkdocs gh-deploy --force`.\",\\n      \"query\": \"Describe the steps involved in the \\'deploy\\' job of the \\'Publish Docs\\' workflow.\"\\n    },\\n    {\\n      \"citations\": [\\n        \"name: Python Lint\",\\n        \"on:\",\\n        \"  push:\",\\n        \"    branches:\",\\n        \"      - main\",\\n        \"    paths:\",\\n        \"      - \\\\\"samples/python/**\\\\\"\",\\n        \"      - \\\\\".github/workflows/python-lint.yaml\\\\\"\",\\n        \"  pull_request:\",\\n        \"    types:\",\\n        \"      - opened\",\\n        \"      - reopened\",\\n        \"      - synchronize\",\\n        \"    paths:\",\\n        \"      - \\\\\"samples/python/**\\\\\"\",\\n        \"      - \\\\\".github/workflows/python-lint.yaml\\\\\"\",\\n        \"  schedule:\",\\n        \"    - cron: \\\\\"0 0 * * 0\\\\\"\"\\n      ],\\n      \"expected_answer\": \"The \\\\\"Python Lint\\\\\" workflow is triggered on pushes to the `main` branch affecting `samples/python/**` or `.github/workflows/python-lint.yaml`, on pull request `opened`, `reopened`, or `synchronize` events affecting the same paths, and on a weekly schedule at midnight UTC on Sundays (`cron: \\\\\"0 0 * * 0\\\\\"`).\",\\n      \"query\": \"What are the triggers for the \\'Python Lint\\' workflow?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"jobs:\",\\n        \"  get_sample_dirs:\",\\n        \"    runs-on: ubuntu-latest\",\\n        \"    outputs:\",\\n        \"      sample_dirs: ${{ steps.get_dirs.outputs.sample_dirs }}\",\\n        \"    steps:\",\\n        \"      - uses: actions/checkout@v4\",\\n        \"        with:\",\\n        \"          ref: ${{ github.event.pull_request.head.sha }}\",\\n        \"      - name: Get Sample Directories\",\\n        \"        id: get_dirs\",\\n        \"        run: |\",\\n        \"          SAMPLE_DIRS=$(find samples/python/ -type d -maxdepth 5 ! -path \\\\\"samples/python\\\\\" | jq -R -s \\'split(\\\\\"\\\\\\\\n\\\\\")[:-1]\\' )\",\\n        \"          echo \\\\\"sample_dirs=$SAMPLE_DIRS\\\\\" >> $GITHUB_OUTPUT\",\\n        \"          echo \\\\\"SAMPLE_DIRS: $SAMPLE_DIRS\\\\\"  # For debugging\"\\n      ],\\n      \"expected_answer\": \"The `get_sample_dirs` job in the Python Lint workflow runs on `ubuntu-latest`. It checks out the repository and then uses `find` and `jq` to identify Python sample directories up to a depth of 5, excluding the root `samples/python` directory. The output `sample_dirs` is then made available for other jobs.\",\\n      \"query\": \"Explain the purpose and steps of the `get_sample_dirs` job in the Python Lint workflow.\"\\n    },\\n    {\\n      \"citations\": [\\n        \"  lint:\",\\n        \"    runs-on: ubuntu-latest\",\\n        \"    strategy:\",\\n        \"      matrix:\",\\n        \"        python-version: [\\\\\"3.9\\\\\", \\\\\"3.10\\\\\", \\\\\"3.11\\\\\", \\\\\"3.12\\\\\"]\",\\n        \"        sample_dir: ${{ fromJson(needs.get_sample_dirs.outputs.sample_dirs) }}\",\\n        \"      fail-fast: false\",\\n        \"    needs:\",\\n        \"      - get_sample_dirs\",\\n        \"    steps:\",\\n        \"      - uses: actions/checkout@v4\",\\n        \"        with:\",\\n        \"          ref: ${{ github.event.pull_request.head.sha }}\",\\n        \"      - name: Set up Python ${{ matrix.python-version }}\",\\n        \"        uses: actions/setup-python@v4\",\\n        \"        with:\",\\n        \"          python-version: ${{ matrix.python-version }}\",\\n        \"          cache: \\\\\"pip\\\\\"\",\\n        \"      - name: Install dependencies\",\\n        \"        run: |\",\\n        \"          python -m pip install --upgrade pip\",\\n        \"          pip install black flake8\",\\n        \"      - name: Run Black and Flake8 Linting in ${{ matrix.sample_dir }}\",\\n        \"        run: |\",\\n        \"          cd ${{ matrix.sample_dir }}\",\\n        \"          black .\",\\n        \"          flake8 .\"\\n      ],\\n      \"expected_answer\": \"The `lint` job in the Python Lint workflow runs on `ubuntu-latest` with a matrix strategy for Python versions 3.9, 3.10, 3.11, 3.12, and each sample directory. It depends on `get_sample_dirs` and does not fail fast. Steps include checking out the code, setting up Python with pip cache, installing `black` and `flake8`, and then running `black` and `flake8` in each sample directory.\",\\n      \"query\": \"Describe the `lint` job in the Python Lint workflow, including its strategy and steps.\"\\n    },\\n    {\\n      \"citations\": [\\n        \"name: Python Tests\",\\n        \"on:\",\\n        \"  push:\",\\n        \"    branches:\",\\n        \"      - main\",\\n        \"    paths:\",\\n        \"      - \\\\\"samples/python/**\\\\\"\",\\n        \"      - \\\\\".github/workflows/python-tests.yaml\\\\\"\",\\n        \"  pull_request:\",\\n        \"    types:\",\\n        \"      - opened\",\\n        \"      - reopened\",\\n        \"      - synchronize\",\\n        \"    paths:\",\\n        \"      - \\\\\"samples/python/**\\\\\"\",\\n        \"      - \\\\\".github/workflows/python-tests.yaml\\\\\"\",\\n        \"  schedule:\",\\n        \"    - cron: \\\\\"0 0 * * 0\\\\\"\"\\n      ],\\n      \"expected_answer\": \"The \\\\\"Python Tests\\\\\" workflow is triggered on pushes to the `main` branch affecting `samples/python/**` or `.github/workflows/python-tests.yaml`, on pull request `opened`, `reopened`, or `synchronize` events affecting the same paths, and on a weekly schedule at midnight UTC on Sundays (`cron: \\\\\"0 0 * * 0\\\\\"`).\",\\n      \"query\": \"What events trigger the \\'Python Tests\\' workflow?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"jobs:\",\\n        \"  build:\",\\n        \"    runs-on: ubuntu-latest\",\\n        \"    strategy:\",\\n        \"      matrix:\",\\n        \"        python-version: [\\\\\"3.9\\\\\", \\\\\"3.10\\\\\", \\\\\"3.11\\\\\", \\\\\"3.12\\\\\"]\",\\n        \"        sample_dir: ${{ fromJson(needs.get_sample_dirs.outputs.sample_dirs) }}\",\\n        \"      fail-fast: false # Important: Don\\'t stop if one matrix configuration fails\",\\n        \"    needs:\",\\n        \"      - get_sample_dirs\"\\n      ],\\n      \"expected_answer\": \"The `build` job in the Python Tests workflow runs on `ubuntu-latest` with a matrix strategy for Python versions 3.9, 3.10, 3.11, 3.12, and each sample directory. It depends on `get_sample_dirs` and is configured not to fail fast.\",\\n      \"query\": \"Describe the `build` job\\'s configuration in the Python Tests workflow.\"\\n    },\\n    {\\n      \"citations\": [\\n        \"      - name: Get Sample Directories\",\\n        \"        id: get_sample_dirs\",\\n        \"        run: |\",\\n        \"          SAMPLE_DIRS=$( find $PYTHON_DIR -type d -not -path \\\\\"*/.venv*\\\\\" -exec test -e \\'{}\\'/requirements.txt \\\\\\\\; -print )\",\\n        \"          echo \\\\\"sample_dirs=$SAMPLE_DIRS\\\\\" >> $GITHUB_OUTPUT\",\\n        \"          echo \\\\\"SAMPLE_DIRS: $SAMPLE_DIRS\\\\\"  # For debugging\"\\n      ],\\n      \"expected_answer\": \"The `get_sample_dirs` step in the Python Tests workflow finds directories under `$PYTHON_DIR` that are not `.venv` directories and contain a `requirements.txt` file. These directories are then outputted as `sample_dirs`.\",\\n      \"query\": \"How does the `get_sample_dirs` step identify sample directories in the Python Tests workflow?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"      - name: Install dependencies\",\\n        \"        run: |\",\\n        \"          python -m pip install --upgrade pip\",\\n        \"          pip install pytest coverage\",\\n        \"          pip install -r ${{ matrix.sample_dir }}/requirements.txt\",\\n        \"          pip install -r ${{ matrix.sample_dir }}/requirements-test.txt\"\\n      ],\\n      \"expected_answer\": \"The \\'Install dependencies\\' step upgrades pip, installs `pytest` and `coverage`, and then installs dependencies from `requirements.txt` and `requirements-test.txt` within the current sample directory.\",\\n      \"query\": \"What dependencies are installed in the Python Tests workflow?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"      - name: Run pytest in ${{ matrix.sample_dir }}\",\\n        \"        run: |\",\\n        \"          cd ${{ matrix.sample_dir }}\",\\n        \"          pytest | tee pytest.txt\"\\n      ],\\n      \"expected_answer\": \"The \\'Run pytest\\' step navigates to the sample directory and executes `pytest`, piping the output to `pytest.txt`.\",\\n      \"query\": \"How are pytest tests executed in the Python Tests workflow?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"      - name: Show failed tests and overall summary\",\\n        \"        run: |\",\\n        \"          grep -E \\\\\"FAILED tests|ERROR tests|[0-9]+ passed,\\\\\" pytest.txt\",\\n        \"          failed_count=$(grep -E \\\\\"FAILED tests|ERROR tests\\\\\" pytest.txt | wc -l | tr -d \\'[:space:]\\')\",\\n        \"          if [[ $failed_count -gt 0 ]]; then\",\\n        \"            echo \\\\\"$failed_count failed lines found! Task failed.\\\\\"\",\\n        \"            exit 1\",\\n        \"          fi\"\\n      ],\\n      \"expected_answer\": \"The \\'Show failed tests and overall summary\\' step greps `pytest.txt` for \\\\\"FAILED tests\\\\\", \\\\\"ERROR tests\\\\\", or \\\\\"[0-9]+ passed,\\\\\". It then counts lines with \\\\\"FAILED tests\\\\\" or \\\\\"ERROR tests\\\\\" and exits with an error if `failed_count` is greater than 0.\",\\n      \"query\": \"What is the purpose of the \\'Show failed tests and overall summary\\' step in the Python Tests workflow?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"      - name: Upload pytest test results\",\\n        \"        uses: actions/upload-artifact@v3\",\\n        \"        with:\",\\n        \"          name: pytest-results-${{ matrix.python-version }}-${{ matrix.sample_dir }}\",\\n        \"          path: |\",\\n        \"            pytest.txt\",\\n        \"            ./htmlcov/\",\\n        \"          retention-days: 30\",\\n        \"        if: ${{ always() }}\"\\n      ],\\n      \"expected_answer\": \"The \\'Upload pytest test results\\' step uploads `pytest.txt` and the `./htmlcov/` directory as an artifact named `pytest-results-${{ matrix.python-version }}-${{ matrix.sample_dir }}`. The artifact is retained for 30 days and is uploaded even if previous steps fail.\",\\n      \"query\": \"How are pytest test results uploaded and retained in the Python Tests workflow?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"__pycache__/\",\\n        \"*.py[cod]\",\\n        \"*$py.class\",\\n        \"*.so\",\\n        \".Python\",\\n        \"develop-eggs/\",\\n        \"dist/\",\\n        \"downloads/\",\\n        \"eggs/\",\\n        \".eggs/\",\\n        \"lib/\",\\n        \"lib64/\",\\n        \"parts/\",\\n        \"sdist/\",\\n        \"var/\",\\n        \"wheels/\",\\n        \"share/python-wheels/\",\\n        \"*.egg-info/\",\\n        \".installed.cfg\",\\n        \"*.egg\",\\n        \"MANIFEST\"\\n      ],\\n      \"expected_answer\": \"The `.gitignore` file includes common Python-related files and directories such as `__pycache__/`, `*.py[cod]`, `*$py.class`, `*.so`, `.Python`, `develop-eggs/`, `dist/`, `downloads/`, `eggs/`, `.eggs/`, `lib/`, `lib64/`, `parts/`, `sdist/`, `var/`, `wheels/`, `share/python-wheels/`, `*.egg-info/`, `.installed.cfg`, `*.egg`, and `MANIFEST`.\",\\n      \"query\": \"What are some of the Python-specific files and directories ignored by Git?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"htmlcov/\",\\n        \".tox/\",\\n        \".nox/\",\\n        \".coverage\",\\n        \".coverage.*\",\\n        \".cache\",\\n        \"nosetests.xml\",\\n        \"coverage.xml\",\\n        \"*.cover\",\\n        \"*.py,cover\",\\n        \".hypothesis/\",\\n        \".pytest_cache/\",\\n        \"cover/\"\\n      ],\\n      \"expected_answer\": \"The `.gitignore` file ignores unit test and coverage reports including `htmlcov/`, `.tox/`, `.nox/`, `.coverage`, `.coverage.*`, `.cache`, `nosetests.xml`, `coverage.xml`, `*.cover`, `*.py,cover`, `.hypothesis/`, `.pytest_cache/`, and `cover/`.\",\\n      \"query\": \"Which files related to unit tests and coverage reports are ignored by Git?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"*.manifest\",\\n        \"*.spec\"\\n      ],\\n      \"expected_answer\": \"PyInstaller-related files such as `*.manifest` and `*.spec` are ignored.\",\\n      \"query\": \"What PyInstaller files are ignored?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"pip-log.txt\",\\n        \"pip-delete-this-directory.txt\"\\n      ],\\n      \"expected_answer\": \"Installer logs like `pip-log.txt` and `pip-delete-this-directory.txt` are ignored.\",\\n      \"query\": \"Which installer log files are ignored?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"*.mo\",\\n        \"*.pot\"\\n      ],\\n      \"expected_answer\": \"Translation files with extensions `*.mo` and `*.pot` are ignored.\",\\n      \"query\": \"What translation files are ignored?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"*.log\",\\n        \"local_settings.py\",\\n        \"db.sqlite3\",\\n        \"db.sqlite3-journal\"\\n      ],\\n      \"expected_answer\": \"Django-related files such as `*.log`, `local_settings.py`, `db.sqlite3`, and `db.sqlite3-journal` are ignored.\",\\n      \"query\": \"What Django-specific files are ignored?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"instance/\",\\n        \".webassets-cache\"\\n      ],\\n      \"expected_answer\": \"Flask-related files and directories like `instance/` and `.webassets-cache` are ignored.\",\\n      \"query\": \"Which Flask-related files are ignored?\"\\n    },\\n    {\\n      \"citations\": [\\n        \".scrapy\"\\n      ],\\n      \"expected_answer\": \"The `.scrapy` directory is ignored for Scrapy projects.\",\\n      \"query\": \"What Scrapy-specific directory is ignored?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"docs/_build/\"\\n      ],\\n      \"expected_answer\": \"The `docs/_build/` directory is ignored for Sphinx documentation.\",\\n      \"query\": \"What Sphinx documentation directory is ignored?\"\\n    },\\n    {\\n      \"citations\": [\\n        \".pybuilder/\",\\n        \"target/\"\\n      ],\\n      \"expected_answer\": \"PyBuilder-related directories like `.pybuilder/` and `target/` are ignored.\",\\n      \"query\": \"What PyBuilder directories are ignored?\"\\n    },\\n    {\\n      \"citations\": [\\n        \".ipynb_checkpoints\"\\n      ],\\n      \"expected_answer\": \"Jupyter Notebook checkpoints (`.ipynb_checkpoints`) are ignored.\",\\n      \"query\": \"Are Jupyter Notebook checkpoints ignored?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"profile_default/\",\\n        \"ipython_config.py\"\\n      ],\\n      \"expected_answer\": \"IPython-related files and directories such as `profile_default/` and `ipython_config.py` are ignored.\",\\n      \"query\": \"What IPython files are ignored?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"celerybeat-schedule\",\\n        \"celerybeat.pid\"\\n      ],\\n      \"expected_answer\": \"Celery-related files like `celerybeat-schedule` and `celerybeat.pid` are ignored.\",\\n      \"query\": \"What Celery files are ignored?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"*.sage.py\"\\n      ],\\n      \"expected_answer\": \"SageMath parsed files with the `*.sage.py` extension are ignored.\",\\n      \"query\": \"What SageMath files are ignored?\"\\n    },\\n    {\\n      \"citations\": [\\n        \".venv\",\\n        \"env/\",\\n        \"venv/\",\\n        \"ENV/\",\\n        \"env.bak/\",\\n        \"venv.bak/\"\\n      ],\\n      \"expected_answer\": \"Various environment directories such as `.venv`, `env/`, `venv/`, `ENV/`, `env.bak/`, and `venv.bak/` are ignored.\",\\n      \"query\": \"Which environment directories are ignored?\"\\n    },\\n    {\\n      \"citations\": [\\n        \".spyderproject\",\\n        \".spyproject\"\\n      ],\\n      \"expected_answer\": \"Spyder project settings files like `.spyderproject` and `.spyproject` are ignored.\",\\n      \"query\": \"What Spyder project settings files are ignored?\"\\n    },\\n    {\\n      \"citations\": [\\n        \".ropeproject\"\\n      ],\\n      \"expected_answer\": \"Rope project settings (`.ropeproject`) are ignored.\",\\n      \"query\": \"Is the Rope project settings file ignored?\"\\n    },\\n    {\\n      \"citations\": [\\n        \".mypy_cache/\",\\n        \".dmypy.json\",\\n        \"dmypy.json\"\\n      ],\\n      \"expected_answer\": \"mypy-related files and directories such as `.mypy_cache/`, `.dmypy.json`, and `dmypy.json` are ignored.\",\\n      \"query\": \"What mypy files are ignored?\"\\n    },\\n    {\\n      \"citations\": [\\n        \".pyre/\"\\n      ],\\n      \"expected_answer\": \"The `.pyre/` directory for Pyre type checker is ignored.\",\\n      \"query\": \"Is the Pyre type checker directory ignored?\"\\n    },\\n    {\\n      \"citations\": [\\n        \".pytype/\"\\n      ],\\n      \"expected_answer\": \"The `.pytype/` directory for pytype static type analyzer is ignored.\",\\n      \"query\": \"Is the pytype static type analyzer directory ignored?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"cython_debug/\"\\n      ],\\n      \"expected_answer\": \"Cython debug symbols in `cython_debug/` are ignored.\",\\n      \"query\": \"Are Cython debug symbols ignored?\"\\n    },\\n    {\\n      \"citations\": [\\n        \".DS_Store\"\\n      ],\\n      \"expected_answer\": \"Mac Finder files like `.DS_Store` are ignored.\",\\n      \"query\": \"Are Mac Finder files ignored?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"docs/site\"\\n      ],\\n      \"expected_answer\": \"The `docs/site` directory for mkdocs documentation is ignored.\",\\n      \"query\": \"What mkdocs documentation directory is ignored?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"/site/public\",\\n        \".hugo_build.lock\"\\n      ],\\n      \"expected_answer\": \"Hugo-related files and directories like `/site/public` and `.hugo_build.lock` are ignored.\",\\n      \"query\": \"What Hugo files are ignored?\"\\n    },\\n    {\\n      \"citations\": [\\n        \".vscode/settings.json\"\\n      ],\\n      \"expected_answer\": \"VS Code personal settings in `.vscode/settings.json` are ignored.\",\\n      \"query\": \"Are VS Code personal settings ignored?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Contributions to this project must be accompanied by a\",\\n        \"[Contributor License Agreement](https://cla.developers.google.com/about) (CLA).\",\\n        \"You (or your employer) retain the copyright to your contribution; this simply\",\\n        \"gives us permission to use and redistribute your contributions as part of the\",\\n        \"project.\",\\n        \"If you or your current employer have already signed the Google CLA (even if it\",\\n        \"was for a different project), you probably don\\'t need to do it again.\",\\n        \"Visit <https://cla.developers.google.com/> to see your current agreements or to\",\\n        \"sign a new one.\"\\n      ],\\n      \"expected_answer\": \"Contributions require a Contributor License Agreement (CLA). The contributor or their employer retains copyright, but the CLA grants permission to use and redistribute contributions. If a Google CLA has been signed previously, it may not be necessary to sign again. The CLA can be managed at `https://cla.developers.google.com/`.\",\\n      \"query\": \"What is the requirement for contributions regarding the Contributor License Agreement (CLA)?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"We adhere to [Google\\'s Open Source Community Guidelines](https://opensource.google/conduct/).\",\\n        \"Please familiarize yourself with these guidelines to ensure a positive and\",\\n        \"collaborative environment for everyone.\"\\n      ],\\n      \"expected_answer\": \"Contributors must adhere to Google\\'s Open Source Community Guidelines, available at `https://opensource.google/conduct/`.\",\\n      \"query\": \"What community guidelines should contributors review?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Check the [GitHub Issues](https://github.com/google/adk-docs/issues) for bug\",\\n        \"reports or feature requests. Feel free to pick up an existing issue or open\",\\n        \"a new one if you have an idea or find a bug.\"\\n      ],\\n      \"expected_answer\": \"Contributors can find something to work on by checking GitHub Issues for bug reports or feature requests, or by opening a new issue.\",\\n      \"query\": \"How can a contributor find something to work on?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"1.  **Clone the repository:**\",\\n        \"    ```shell\",\\n        \"    git clone git@github.com:google/adk-docs.git\",\\n        \"    cd adk-docs\",\\n        \"    ```\",\\n        \"2.  **Create and activate a virtual environment:**\",\\n        \"    ```shell\",\\n        \"    python -m venv venv\",\\n        \"    source venv/bin/activate\",\\n        \"    ```\",\\n        \"3.  **Install dependencies:**\",\\n        \"    ```shell\",\\n        \"    pip install -r requirements.txt\",\\n        \"    ```\",\\n        \"4.  **Run the local development server:**\",\\n        \"    ```shell\",\\n        \"    mkdocs serve\",\\n        \"    ```\",\\n        \"    This command starts a local server, typically at `http://127.0.0.1:8000/`.\",\\n        \"    The site will automatically reload when you save changes to the documentation files.\",\\n        \"    For more details on the site configuration, see the mkdocs.yml file.\"\\n      ],\\n      \"expected_answer\": \"To set up for development, clone the repository, create and activate a virtual environment, install dependencies using `pip install -r requirements.txt`, and then run the local development server with `mkdocs serve`.\",\\n      \"query\": \"What are the steps for setting up the development environment?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"All contributions, including those from project members, undergo a review process.\",\\n        \"1.  **Create a Pull Request:** We use GitHub Pull Requests (PRs) for code review.\",\\n        \"    Please refer to GitHub Help if you\\'re unfamiliar with PRs.\",\\n        \"2.  **Review Process:** Project maintainers will review your PR, providing feedback\",\\n        \"    or requesting changes if necessary.\",\\n        \"3.  **Merging:** Once the PR is approved and passes any required checks, it will be\",\\n        \"    merged into the main branch.\",\\n        \"Consult [GitHub Help](https://help.github.com/articles/about-pull-requests/) for\",\\n        \"more information on using pull requests. We look forward to your contributions!\"\\n      ],\\n      \"expected_answer\": \"All contributions go through a review process. Contributors must create a Pull Request (PR) on GitHub. Project maintainers will review the PR and provide feedback. Once approved and all checks pass, the PR will be merged into the main branch.\",\\n      \"query\": \"Describe the code review process for contributions.\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Apache License\",\\n        \"Version 2.0, January 2004\",\\n        \"http://www.apache.org/licenses/\"\\n      ],\\n      \"expected_answer\": \"The project is licensed under the Apache License, Version 2.0, dated January 2004.\",\\n      \"query\": \"What is the license of this project?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"\\\\\"License\\\\\" shall mean the terms and conditions for use, reproduction,\",\\n        \"and distribution as defined by Sections 1 through 9 of this document.\"\\n      ],\\n      \"expected_answer\": \"The \\\\\"License\\\\\" refers to the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of the document.\",\\n      \"query\": \"What does \\\\\"License\\\\\" mean in the Apache License 2.0?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"\\\\\"Licensor\\\\\" shall mean the copyright owner or entity authorized by\",\\n        \"the copyright owner that is granting the License.\"\\n      ],\\n      \"expected_answer\": \"The \\\\\"Licensor\\\\\" is the copyright owner or entity authorized by the copyright owner that is granting the License.\",\\n      \"query\": \"Who is the \\\\\"Licensor\\\\\" according to the Apache License 2.0?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"\\\\\"Legal Entity\\\\\" shall mean the union of the acting entity and all\",\\n        \"other entities that control, are controlled by, or are under common\",\\n        \"control with that entity. For the purposes of this definition,\",\\n        \"\\\\\"control\\\\\" means (i) the power, direct or indirect, to cause the\",\\n        \"direction or management of such entity, whether by contract or\",\\n        \"otherwise, or (ii) ownership of fifty percent (50%) or more of the\",\\n        \"outstanding shares, or (iii) beneficial ownership of such entity.\"\\n      ],\\n      \"expected_answer\": \"A \\\\\"Legal Entity\\\\\" is the acting entity and all other entities under common control with it. \\\\\"Control\\\\\" means direct or indirect power to manage, or ownership of 50% or more of outstanding shares, or beneficial ownership.\",\\n      \"query\": \"Define \\\\\"Legal Entity\\\\\" and \\\\\"control\\\\\" as per the Apache License 2.0.\"\\n    },\\n    {\\n      \"citations\": [\\n        \"\\\\\"You\\\\\" (or \\\\\"Your\\\\\") shall mean an individual or Legal Entity\",\\n        \"exercising permissions granted by this License.\"\\n      ],\\n      \"expected_answer\": \"\\\\\"You\\\\\" or \\\\\"Your\\\\\" refers to an individual or Legal Entity exercising permissions granted by the License.\",\\n      \"query\": \"Who is referred to as \\\\\"You\\\\\" or \\\\\"Your\\\\\" in the Apache License 2.0?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"\\\\\"Source\\\\\" form shall mean the preferred form for making modifications,\",\\n        \"including but not limited to software source code, documentation\",\\n        \"source, and configuration files.\"\\n      ],\\n      \"expected_answer\": \"\\\\\"Source\\\\\" form is the preferred form for modifications, including software source code, documentation source, and configuration files.\",\\n      \"query\": \"What is \\\\\"Source\\\\\" form in the context of the Apache License 2.0?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"\\\\\"Object\\\\\" form shall mean any form resulting from mechanical\",\\n        \"transformation or translation of a Source form, including but\",\\n        \"not limited to compiled object code, generated documentation,\",\\n        \"and conversions to other media types.\"\\n      ],\\n      \"expected_answer\": \"\\\\\"Object\\\\\" form is any form resulting from mechanical transformation or translation of a Source form, such as compiled object code, generated documentation, and conversions to other media types.\",\\n      \"query\": \"What is \\\\\"Object\\\\\" form according to the Apache License 2.0?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"\\\\\"Work\\\\\" shall mean the work of authorship, whether in Source or\",\\n        \"Object form, made available under the License, as indicated by a\",\\n        \"copyright notice that is included in or attached to the work\",\\n        \"(an example is provided in the Appendix below).\"\\n      ],\\n      \"expected_answer\": \"\\\\\"Work\\\\\" refers to the work of authorship, in Source or Object form, made available under the License, as indicated by a copyright notice.\",\\n      \"query\": \"How is \\\\\"Work\\\\\" defined in the Apache License 2.0?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"\\\\\"Derivative Works\\\\\" shall mean any work, whether in Source or Object\",\\n        \"form, that is based on (or derived from) the Work and for which the\",\\n        \"editorial revisions, annotations, elaborations, or other modifications\",\\n        \"represent, as a whole, an original work of authorship. For the purposes\",\\n        \"of this License, Derivative Works shall not include works that remain\",\\n        \"separable from, or merely link (or bind by name) to the interfaces of,\",\\n        \"the Work and Derivative Works thereof.\"\\n      ],\\n      \"expected_answer\": \"\\\\\"Derivative Works\\\\\" are works in Source or Object form based on the Work, where modifications constitute an original work of authorship. It excludes works that remain separable from or merely link to the interfaces of the Work.\",\\n      \"query\": \"What constitutes \\\\\"Derivative Works\\\\\" under the Apache License 2.0?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"\\\\\"Contribution\\\\\" shall mean any work of authorship, including\",\\n        \"the original version of the Work and any modifications or additions\",\\n        \"to that Work or Derivative Works thereof, that is intentionally\",\\n        \"submitted to Licensor for inclusion in the Work by the copyright owner\",\\n        \"or by an individual or Legal Entity authorized to submit on behalf of\",\\n        \"the copyright owner. For the purposes of this definition, \\\\\"submitted\\\\\"\",\\n        \"means any form of electronic, verbal, or written communication sent\",\\n        \"to the Licensor or its representatives, including but not limited to\",\\n        \"communication on electronic mailing lists, source code control systems,\",\\n        \"and issue tracking systems that are managed by, or on behalf of, the\",\\n        \"Licensor for the purpose of discussing and improving the Work, but\",\\n        \"excluding communication that is conspicuously marked or otherwise\",\\n        \"designated in writing by the copyright owner as \\\\\"Not a Contribution.\\\\\"\"\\n      ],\\n      \"expected_answer\": \"A \\\\\"Contribution\\\\\" is any work of authorship, including original or modified Work, intentionally submitted to the Licensor for inclusion by the copyright owner or authorized entity. \\\\\"Submitted\\\\\" includes electronic, verbal, or written communication for discussing and improving the Work, but excludes anything explicitly marked \\\\\"Not a Contribution.\\\\\"\",\\n      \"query\": \"Define \\\\\"Contribution\\\\\" and \\\\\"submitted\\\\\" in the Apache License 2.0.\"\\n    },\\n    {\\n      \"citations\": [\\n        \"\\\\\"Contributor\\\\\" shall mean Licensor and any individual or Legal Entity\",\\n        \"on behalf of whom a Contribution has been received by Licensor and\",\\n        \"subsequently incorporated within the Work.\"\\n      ],\\n      \"expected_answer\": \"A \\\\\"Contributor\\\\\" is the Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received and incorporated into the Work.\",\\n      \"query\": \"Who is a \\\\\"Contributor\\\\\" according to the Apache License 2.0?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"2. Grant of Copyright License. Subject to the terms and conditions of\",\\n        \"this License, each Contributor hereby grants to You a perpetual,\",\\n        \"worldwide, non-exclusive, no-charge, royalty-free, irrevocable\",\\n        \"copyright license to reproduce, prepare Derivative Works of,\",\\n        \"publicly display, publicly perform, sublicense, and distribute the\",\\n        \"Work and such Derivative Works in Source or Object form.\"\\n      ],\\n      \"expected_answer\": \"Each Contributor grants a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and Derivative Works in Source or Object form.\",\\n      \"query\": \"What copyright license is granted by Contributors under the Apache License 2.0?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"3. Grant of Patent License. Subject to the terms and conditions of\",\\n        \"this License, each Contributor hereby grants to You a perpetual,\",\\n        \"worldwide, non-exclusive, no-charge, royalty-free, irrevocable\",\\n        \"(except as stated in this section) patent license to make, have made,\",\\n        \"use, offer to sell, sell, import, and otherwise transfer the Work,\",\\n        \"where such license applies only to those patent claims licensable\",\\n        \"by such Contributor that are necessarily infringed by their\",\\n        \"Contribution(s) alone or by combination of their Contribution(s)\",\\n        \"with the Work to which such Contribution(s) was submitted. If You\",\\n        \"institute patent litigation against any entity (including a\",\\n        \"cross-claim or counterclaim in a lawsuit) alleging that the Work\",\\n        \"or a Contribution incorporated within the Work constitutes direct\",\\n        \"or contributory patent infringement, then any patent licenses\",\\n        \"granted to You under this License for that Work shall terminate\",\\n        \"as of the date such litigation is filed.\"\\n      ],\\n      \"expected_answer\": \"Each Contributor grants a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable patent license to make, use, sell, import, and transfer the Work. This license applies only to patent claims necessarily infringed by their Contribution(s) alone or combined with the Work. The patent license terminates if the licensee institutes patent litigation alleging infringement by the Work or a Contribution.\",\\n      \"query\": \"What patent license is granted by Contributors under the Apache License 2.0, and when does it terminate?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"4. Redistribution. You may reproduce and distribute copies of the\",\\n        \"Work or Derivative Works thereof in any medium, with or without\",\\n        \"modifications, and in Source or Object form, provided that You\",\\n        \"meet the following conditions:\",\\n        \"(a) You must give any other recipients of the Work or\",\\n        \"Derivative Works a copy of this License; and\",\\n        \"(b) You must cause any modified files to carry prominent notices\",\\n        \"stating that You changed the files; and\",\\n        \"(c) You must retain, in the Source form of any Derivative Works\",\\n        \"that You distribute, all copyright, patent, trademark, and\",\\n        \"attribution notices from the Source form of the Work,\",\\n        \"excluding those notices that do not pertain to any part of\",\\n        \"the Derivative Works; and\",\\n        \"(d) If the Work includes a \\\\\"NOTICE\\\\\" text file as part of its\",\\n        \"distribution, then any Derivative Works that You distribute must\",\\n        \"include a readable copy of the attribution notices contained\",\\n        \"within such NOTICE file, excluding those notices that do not\",\\n        \"pertain to any part of the Derivative Works, in at least one\",\\n        \"of the following places: within a NOTICE text file distributed\",\\n        \"as part of the Derivative Works; within the Source form or\",\\n        \"documentation, if provided along with the Derivative Works; or,\",\\n        \"within a display generated by the Derivative Works, if and\",\\n        \"wherever such third-party notices normally appear. The contents\",\\n        \"of the NOTICE file are for informational purposes only and\",\\n        \"do not modify the License. You may add Your own attribution\",\\n        \"notices within Derivative Works that You distribute, alongside\",\\n        \"or as an addendum to the NOTICE text from the Work, provided\",\\n        \"that such additional attribution notices cannot be construed\",\\n        \"as modifying the License.\"\\n      ],\\n      \"expected_answer\": \"When redistributing the Work or Derivative Works, you must provide a copy of the License, include prominent notices for modified files, retain all original copyright, patent, trademark, and attribution notices in Source form, and include attribution notices from any \\\\\"NOTICE\\\\\" file in the distribution, Source form, documentation, or generated display. You may add your own attribution notices without modifying the License.\",\\n      \"query\": \"What are the conditions for redistribution of the Work or Derivative Works under the Apache License 2.0?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"5. Submission of Contributions. Unless You explicitly state otherwise,\",\\n        \"any Contribution intentionally submitted for inclusion in the Work\",\\n        \"by You to the Licensor shall be under the terms and conditions of\",\\n        \"this License, without any additional terms or conditions.\",\\n        \"Notwithstanding the above, nothing herein shall supersede or modify\",\\n        \"the terms of any separate license agreement you may have executed\",\\n        \"with Licensor regarding such Contributions.\"\\n      ],\\n      \"expected_answer\": \"Unless explicitly stated otherwise, any Contribution submitted for inclusion in the Work is under the terms of the Apache License 2.0, without additional terms. This does not supersede or modify any separate license agreement with the Licensor.\",\\n      \"query\": \"What are the terms for submitting Contributions under the Apache License 2.0?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"6. Trademarks. This License does not grant permission to use the trade\",\\n        \"names, trademarks, service marks, or product names of the Licensor,\",\\n        \"except as required for reasonable and customary use in describing the\",\\n        \"origin of the Work and reproducing the content of the NOTICE file.\"\\n      ],\\n      \"expected_answer\": \"The License does not grant permission to use the Licensor\\'s trade names, trademarks, service marks, or product names, except for describing the Work\\'s origin and reproducing the NOTICE file content.\",\\n      \"query\": \"Does the Apache License 2.0 grant permission to use Licensor\\'s trademarks?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"7. Disclaimer of Warranty. Unless required by applicable law or\",\\n        \"agreed to in writing, Licensor provides the Work (and each\",\\n        \"Contributor provides its Contributions) on an \\\\\"AS IS\\\\\" BASIS,\",\\n        \"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\",\\n        \"implied, including, without limitation, any warranties or conditions\",\\n        \"of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\",\\n        \"PARTICULAR PURPOSE. You are solely responsible for determining the\",\\n        \"appropriateness of using or redistributing the Work and assume any\",\\n        \"risks associated with Your exercise of permissions under this License.\"\\n      ],\\n      \"expected_answer\": \"The Work and Contributions are provided \\\\\"AS IS,\\\\\" without warranties of any kind, including title, non-infringement, merchantability, or fitness for a particular purpose, unless required by law or agreed in writing. Users are responsible for determining the appropriateness of use and assume all associated risks.\",\\n      \"query\": \"What is the disclaimer of warranty in the Apache License 2.0?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"8. Limitation of Liability. In no event and under no legal theory,\",\\n        \"whether in tort (including negligence), contract, or otherwise,\",\\n        \"unless required by applicable law (such as deliberate and grossly\",\\n        \"negligent acts) or agreed to in writing, shall any Contributor be\",\\n        \"liable to You for damages, including any direct, indirect, special,\",\\n        \"incidental, or consequential damages of any character arising as a\",\\n        \"result of this License or out of the use or inability to use the\",\\n        \"Work (including but not limited to damages for loss of goodwill,\",\\n        \"work stoppage, computer failure or malfunction, or any and all\",\\n        \"other commercial damages or losses), even if such Contributor\",\\n        \"has been advised of the possibility of such damages.\"\\n      ],\\n      \"expected_answer\": \"No Contributor is liable for damages (direct, indirect, special, incidental, or consequential), including loss of goodwill, work stoppage, computer failure, or commercial losses, arising from the License or use/inability to use the Work, unless required by law or agreed in writing, even if advised of such damages.\",\\n      \"query\": \"What is the limitation of liability for Contributors under the Apache License 2.0?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"9. Accepting Warranty or Additional Liability. While redistributing\",\\n        \"the Work or Derivative Works thereof, You may choose to offer,\",\\n        \"and charge a fee for, acceptance of support, warranty, indemnity,\",\\n        \"or other liability obligations and/or rights consistent with this\",\\n        \"License. However, in accepting such obligations, You may act only\",\\n        \"on Your own behalf and on Your sole responsibility, not on behalf\",\\n        \"of any other Contributor, and only if You agree to indemnify,\",\\n        \"defend, and hold each Contributor harmless for any liability\",\\n        \"incurred by, or claims asserted against, such Contributor by reason\",\\n        \"of your accepting any such warranty or additional liability.\"\\n      ],\\n      \"expected_answer\": \"When redistributing, you may offer and charge for support, warranty, or indemnity consistent with the License. However, you act solely on your own behalf and must indemnify, defend, and hold Contributors harmless for any liability or claims arising from your acceptance of such additional obligations.\",\\n      \"query\": \"Can a redistributor accept additional warranty or liability under the Apache License 2.0?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Agent Development Kit (ADK)\",\\n        \"An open-source, code-first Python toolkit for building, evaluating, and\",\\n        \"deploying sophisticated AI agents with flexibility and control.\"\\n      ],\\n      \"expected_answer\": \"ADK stands for Agent Development Kit, which is an open-source, code-first Python toolkit for building, evaluating, and deploying sophisticated AI agents.\",\\n      \"query\": \"What is ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Agent Development Kit (ADK) is a flexible and modular framework for **developing\",\\n        \"and deploying AI agents**. While optimized for Gemini and the Google ecosystem,\",\\n        \"ADK is **model-agnostic**, **deployment-agnostic**, and is built for\",\\n        \"**compatibility with other frameworks**.\"\\n      ],\\n      \"expected_answer\": \"ADK is a flexible and modular framework for developing and deploying AI agents. It is optimized for Gemini and the Google ecosystem but is model-agnostic, deployment-agnostic, and compatible with other frameworks.\",\\n      \"query\": \"What are the key characteristics of the Agent Development Kit (ADK)?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"ADK was designed to make agent development feel more like software development, to make it easier for\",\\n        \"developers to create, deploy, and orchestrate agentic architectures that range\",\\n        \"from simple tasks to complex workflows.\"\\n      ],\\n      \"expected_answer\": \"ADK was designed to make agent development feel more like software development, simplifying the creation, deployment, and orchestration of agentic architectures from simple to complex workflows.\",\\n      \"query\": \"What was the design goal of ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"**Rich Tool Ecosystem**: Utilize pre-built tools, custom functions,\",\\n        \"OpenAPI specs, or integrate existing tools to give agents diverse\",\\n        \"capabilities, all for tight integration with the Google ecosystem.\"\\n      ],\\n      \"expected_answer\": \"ADK offers a rich tool ecosystem, allowing the use of pre-built tools, custom functions, OpenAPI specs, or existing tools for diverse agent capabilities and tight integration with the Google ecosystem.\",\\n      \"query\": \"What kind of tool ecosystem does ADK provide?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"**Code-First Development**: Define agent logic, tools, and orchestration\",\\n        \"directly in Python for ultimate flexibility, testability, and versioning.\"\\n      ],\\n      \"expected_answer\": \"ADK supports code-first development, where agent logic, tools, and orchestration are defined directly in Python for flexibility, testability, and versioning.\",\\n      \"query\": \"What does \\'Code-First Development\\' mean in ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"**Modular Multi-Agent Systems**: Design scalable applications by composing\",\\n        \"multiple specialized agents into flexible hierarchies.\"\\n      ],\\n      \"expected_answer\": \"ADK enables modular multi-agent systems, allowing scalable applications to be designed by composing multiple specialized agents into flexible hierarchies.\",\\n      \"query\": \"How does ADK support modular multi-agent systems?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"**Deploy Anywhere**: Easily containerize and deploy agents on Cloud Run or\",\\n        \"scale seamlessly with Vertex AI Agent Engine.\"\\n      ],\\n      \"expected_answer\": \"ADK agents can be easily containerized and deployed on Cloud Run or scaled with Vertex AI Agent Engine.\",\\n      \"query\": \"Where can ADK agents be deployed?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"You can install ADK using `pip`:\",\\n        \"```bash\",\\n        \"pip install google-adk\",\\n        \"```\"\\n      ],\\n      \"expected_answer\": \"ADK can be installed using `pip` with the command `pip install google-adk`.\",\\n      \"query\": \"How do you install ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Explore the full documentation for detailed guides on building, evaluating, and\",\\n        \"deploying agents:\",\\n        \"* **[Documentation](https://google.github.io/adk-docs)**\"\\n      ],\\n      \"expected_answer\": \"The full documentation for ADK is available at `https://google.github.io/adk-docs`.\",\\n      \"query\": \"Where can I find the full documentation for ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"We welcome contributions from the community! Whether it\\'s bug reports, feature\",\\n        \"requests, documentation improvements, or code contributions, please see our\",\\n        \"[**Contributing Guidelines**](./CONTRIBUTING.md) to get started.\"\\n      ],\\n      \"expected_answer\": \"Contributions are welcome, including bug reports, feature requests, documentation improvements, and code contributions. Guidelines are in `CONTRIBUTING.md`.\",\\n      \"query\": \"What types of contributions are welcome for ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"This project is licensed under the Apache 2.0 License - see the\",\\n        \"[LICENSE](LICENSE) file for details.\"\\n      ],\\n      \"expected_answer\": \"The project is licensed under the Apache 2.0 License, with details in the `LICENSE` file.\",\\n      \"query\": \"What is the license for the ADK project?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"This feature is subject to the \\\\\"Pre-GA Offerings Terms\\\\\" in the General Service Terms section of the [Service Specific Terms](https://cloud.google.com/terms/service-terms#1). Pre-GA features are available \\\\\"as is\\\\\" and might have limited support. For more information, see the [launch stage descriptions](https://cloud.google.com/products?hl=en#product-launch-stages).\"\\n      ],\\n      \"expected_answer\": \"Pre-GA features are subject to the \\\\\"Pre-GA Offerings Terms\\\\\" in the General Service Terms section of the Service Specific Terms. They are provided \\\\\"as is\\\\\" with potentially limited support. More information is available in the launch stage descriptions.\",\\n      \"query\": \"What are the terms for Pre-GA features?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Building custom agents by directly implementing `_run_async_impl` provides powerful control but is more complex than using the predefined `LlmAgent` or standard `WorkflowAgent` types. We recommend understanding those foundational agent types first before tackling custom orchestration logic.\"\\n      ],\\n      \"expected_answer\": \"Building custom agents by implementing `_run_async_impl` offers powerful control but is more complex than using `LlmAgent` or `WorkflowAgent` types. It\\'s recommended to understand foundational agent types first.\",\\n      \"query\": \"What is the recommendation before building custom agents?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"A Custom Agent is essentially any class you create that inherits from `google.adk.agents.BaseAgent` and implements its core execution logic within the `_run_async_impl` asynchronous method.\"\\n      ],\\n      \"expected_answer\": \"A Custom Agent is a class that inherits from `google.adk.agents.BaseAgent` and implements its core execution logic within the `_run_async_impl` asynchronous method.\",\\n      \"query\": \"What defines a Custom Agent in ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"While the standard [Workflow Agents](workflow-agents/index.md) (`SequentialAgent`, `LoopAgent`, `ParallelAgent`) cover common orchestration patterns, you\\'ll need a Custom agent when your requirements include:\",\\n        \"* **Conditional Logic:** Executing different sub-agents or taking different paths based on runtime conditions or the results of previous steps.\",\\n        \"* **Complex State Management:** Implementing intricate logic for maintaining and updating state throughout the workflow beyond simple sequential passing.\",\\n        \"* **External Integrations:** Incorporating calls to external APIs, databases, or custom Python libraries directly within the orchestration flow control.\",\\n        \"* **Dynamic Agent Selection:** Choosing which sub-agent(s) to run next based on dynamic evaluation of the situation or input.\",\\n        \"* **Unique Workflow Patterns:** Implementing orchestration logic that doesn\\'t fit the standard sequential, parallel, or loop structures.\"\\n      ],\\n      \"expected_answer\": \"Custom agents are needed for conditional logic, complex state management, external integrations, dynamic agent selection, or unique workflow patterns that standard Workflow Agents cannot handle.\",\\n      \"query\": \"When should one use a Custom Agent instead of a Workflow Agent?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"The heart of any custom agent is the `_run_async_impl` method.\",\\n        \"* **Signature:** `async def _run_async_impl(self, ctx: InvocationContext) -> AsyncGenerator[Event, None]:`\",\\n        \"* **Asynchronous Generator:** It must be an `async def` function and return an `AsyncGenerator`. This allows it to `yield` events produced by sub-agents or its own logic back to the runner.\",\\n        \"* **`ctx` (InvocationContext):** Provides access to crucial runtime information, most importantly `ctx.session.state`, which is the primary way to share data between steps orchestrated by your custom agent.\"\\n      ],\\n      \"expected_answer\": \"The `_run_async_impl` method is central to custom agents. It must be an `async def` function returning an `AsyncGenerator` to yield events. The `ctx` (InvocationContext) provides access to runtime information, especially `ctx.session.state` for sharing data.\",\\n      \"query\": \"What is the `_run_async_impl` method in a custom agent?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"**Key Capabilities within `_run_async_impl`:**\",\\n        \"1. **Calling Sub-Agents:** You invoke sub-agents (which are typically stored as instance attributes like `self.my_llm_agent`) using their `run_async` method and yield their events:\",\\n        \"    ```python\",\\n        \"    async for event in self.some_sub_agent.run_async(ctx):\",\\n        \"        # Optionally inspect or log the event\",\\n        \"        yield event # Pass the event up\",\\n        \"    ```\",\\n        \"2. **Managing State:** Read from and write to the session state dictionary (`ctx.session.state`) to pass data between sub-agent calls or make decisions:\",\\n        \"    ```python\",\\n        \"    # Read data set by a previous agent\",\\n        \"    previous_result = ctx.session.state.get(\\\\\"some_key\\\\\")\",\\n        \"    # Make a decision based on state\",\\n        \"    if previous_result == \\\\\"some_value\\\\\":\",\\n        \"        # ... call a specific sub-agent ...\",\\n        \"    else:\",\\n        \"        # ... call another sub-agent ...\",\\n        \"    # Store a result for a later step (often done via a sub-agent\\'s output_key)\",\\n        \"    # ctx.session.state[\\\\\"my_custom_result\\\\\"] = \\\\\"calculated_value\\\\\"\",\\n        \"    ```\",\\n        \"3. **Implementing Control Flow:** Use standard Python constructs (`if`/`elif`/`else`, `for`/`while` loops, `try`/`except`) to create sophisticated, conditional, or iterative workflows involving your sub-agents.\"\\n      ],\\n      \"expected_answer\": \"Within `_run_async_impl`, you can call sub-agents using `run_async` and yield their events, manage state by reading from and writing to `ctx.session.state` to pass data and make decisions, and implement complex control flow using standard Python constructs like `if`/`else` and loops.\",\\n      \"query\": \"What are the key capabilities within the `_run_async_impl` method for custom agents?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Typically, a custom agent orchestrates other agents (like `LlmAgent`, `LoopAgent`, etc.).\",\\n        \"* **Initialization:** You usually pass instances of these sub-agents into your custom agent\\'s `__init__` method and store them as instance attributes (e.g., `self.story_generator = story_generator_instance`). This makes them accessible within `_run_async_impl`.\",\\n        \"* **`sub_agents` List:** When initializing the `BaseAgent` using `super().__init__(...)`, you should pass a `sub_agents` list. This list tells the ADK framework about the agents that are part of this custom agent\\'s immediate hierarchy. It\\'s important for framework features like lifecycle management, introspection, and potentially future routing capabilities, even if your `_run_async_impl` calls the agents directly via `self.xxx_agent`.\"\\n      ],\\n      \"expected_answer\": \"Custom agents orchestrate other agents by receiving them in their `__init__` method and storing them as instance attributes. Additionally, a `sub_agents` list should be passed to `super().__init__(...)` to inform the ADK framework about the immediate hierarchy for features like lifecycle management and introspection.\",\\n      \"query\": \"How are sub-agents managed and initialized within a custom agent?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"**Goal:** Create a system that generates a story, iteratively refines it through critique and revision, performs final checks, and crucially, *regenerates the story if the final tone check fails*.\",\\n        \"**Why Custom?** The core requirement driving the need for a custom agent here is the **conditional regeneration based on the tone check**. Standard workflow agents don\\'t have built-in conditional branching based on the outcome of a sub-agent\\'s task. We need custom Python logic (`if tone == \\\\\"negative\\\\\": ...`) within the orchestrator.\"\\n      ],\\n      \"expected_answer\": \"The `StoryFlowAgent` example uses a custom agent to achieve conditional regeneration of a story based on a tone check. This is necessary because standard workflow agents lack built-in conditional branching based on sub-agent outcomes, requiring custom Python logic for orchestration.\",\\n      \"query\": \"Why is a custom agent used in the `StoryFlowAgent` example?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"The initial `story_generator` runs. Its output is expected to be in `ctx.session.state[\\\\\"current_story\\\\\"]`.\",\\n        \"2. The `loop_agent` runs, which internally calls the `critic` and `reviser` sequentially for `max_iterations` times. They read/write `current_story` and `criticism` from/to the state.\",\\n        \"3. The `sequential_agent` runs, calling `grammar_check` then `tone_check`, reading `current_story` and writing `grammar_suggestions` and `tone_check_result` to the state.\",\\n        \"4. **Custom Part:** The `if` statement checks the `tone_check_result` from the state. If it\\'s \\\\\"negative\\\\\", the `story_generator` is called *again*, overwriting the `current_story` in the state. Otherwise, the flow ends.\"\\n      ],\\n      \"expected_answer\": \"The `StoryFlowAgent` first runs `story_generator`. Then, a `loop_agent` iteratively calls `critic` and `reviser`. A `sequential_agent` performs `grammar_check` and `tone_check`. Finally, a custom `if` statement checks `tone_check_result`; if \\\\\"negative\\\\\", `story_generator` is re-run.\",\\n      \"query\": \"Describe the execution logic of the `StoryFlowAgent`.\"\\n    },\\n    {\\n      \"citations\": [\\n        \"In the Agent Development Kit (ADK), an **Agent** is a self-contained execution unit designed to act autonomously to achieve specific goals. Agents can perform tasks, interact with users, utilize external tools, and coordinate with other agents.\"\\n      ],\\n      \"expected_answer\": \"In ADK, an Agent is a self-contained execution unit that acts autonomously to achieve goals, performing tasks, interacting with users, utilizing external tools, and coordinating with other agents.\",\\n      \"query\": \"What is an Agent in ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"The foundation for all agents in ADK is the `BaseAgent` class. It serves as the fundamental blueprint. To create functional agents, you typically extend `BaseAgent` in one of three main ways, catering to different needs – from intelligent reasoning to structured process control.\"\\n      ],\\n      \"expected_answer\": \"The `BaseAgent` class is the foundation for all agents in ADK, serving as a blueprint that is extended in three main ways to create functional agents for different needs.\",\\n      \"query\": \"What is the foundational class for all agents in ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"1. [**LLM Agents (`LlmAgent`, `Agent`)**](llm-agents.md): These agents utilize Large Language Models (LLMs) as their core engine to understand natural language, reason, plan, generate responses, and dynamically decide how to proceed or which tools to use, making them ideal for flexible, language-centric tasks.\",\\n        \"2. [**Workflow Agents (`SequentialAgent`, `ParallelAgent`, `LoopAgent`)**](workflow-agents/index.md): These specialized agents control the execution flow of other agents in predefined, deterministic patterns (sequence, parallel, or loop) without using an LLM for the flow control itself, perfect for structured processes needing predictable execution.\",\\n        \"3. [**Custom Agents**](custom-agents.md): Created by extending `BaseAgent` directly, these agents allow you to implement unique operational logic, specific control flows, or specialized integrations not covered by the standard types, catering to highly tailored application requirements.\"\\n      ],\\n      \"expected_answer\": \"ADK provides three core agent categories: LLM Agents (`LlmAgent`, `Agent`) for language-centric tasks using LLMs; Workflow Agents (`SequentialAgent`, `ParallelAgent`, `LoopAgent`) for deterministic control flow of other agents; and Custom Agents for unique operational logic or integrations by extending `BaseAgent` directly.\",\\n      \"query\": \"What are the three core agent categories in ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"| Feature              | LLM Agent (`LlmAgent`)              | Workflow Agent                              | Custom Agent (`BaseAgent` subclass)      |\",\\n        \"| :------------------- | :---------------------------------- | :------------------------------------------ | :--------------------------------------- |\",\\n        \"| **Primary Function** | Reasoning, Generation, Tool Use     | Controlling Agent Execution Flow            | Implementing Unique Logic/Integrations   |\",\\n        \"| **Core Engine**  | Large Language Model (LLM)          | Predefined Logic (Sequence, Parallel, Loop) | Custom Python Code                       |\",\\n        \"| **Determinism**  | Non-deterministic (Flexible)        | Deterministic (Predictable)                 | Can be either, based on implementation |\",\\n        \"| **Primary Use**  | Language tasks, Dynamic decisions   | Structured processes, Orchestration         | Tailored requirements, Specific workflows|\"\\n      ],\\n      \"expected_answer\": \"LLM Agents are primarily for reasoning, generation, and tool use, powered by an LLM, non-deterministic, and used for language tasks and dynamic decisions. Workflow Agents control agent execution flow using predefined logic, are deterministic, and are for structured processes and orchestration. Custom Agents implement unique logic/integrations using custom Python code, can be either deterministic or non-deterministic, and are for tailored requirements and specific workflows.\",\\n      \"query\": \"Compare LLM Agents, Workflow Agents, and Custom Agents based on their primary function, core engine, determinism, and primary use.\"\\n    },\\n    {\\n      \"citations\": [\\n        \"While each agent type serves a distinct purpose, the true power often comes from combining them. Complex applications frequently employ [multi-agent architectures](multi-agents.md) where:\",\\n        \"* **LLM Agents** handle intelligent, language-based task execution.\",\\n        \"* **Workflow Agents** manage the overall process flow using standard patterns.\",\\n        \"* **Custom Agents** provide specialized capabilities or rules needed for unique integrations.\"\\n      ],\\n      \"expected_answer\": \"Complex applications often combine different agent types in multi-agent architectures: LLM Agents for intelligent, language-based tasks; Workflow Agents for managing overall process flow; and Custom Agents for specialized capabilities or unique integrations.\",\\n      \"query\": \"How do different agent types work together in multi-agent systems?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"The `LlmAgent` (often aliased simply as `Agent`) is a core component in ADK, acting as the \\\\\"thinking\\\\\" part of your application. It leverages the power of a Large Language Model (LLM) for reasoning, understanding natural language, making decisions, generating responses, and interacting with tools.\"\\n      ],\\n      \"expected_answer\": \"The `LlmAgent` (or `Agent`) is a core ADK component that acts as the \\\\\"thinking\\\\\" part of an application, using an LLM for reasoning, natural language understanding, decision-making, response generation, and tool interaction.\",\\n      \"query\": \"What is the primary role of an `LlmAgent` in ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Unlike deterministic [Workflow Agents](workflow-agents/index.md) that follow predefined execution paths, `LlmAgent` behavior is non-deterministic. It uses the LLM to interpret instructions and context, deciding dynamically how to proceed, which tools to use (if any), or whether to transfer control to another agent.\"\\n      ],\\n      \"expected_answer\": \"`LlmAgent` behavior is non-deterministic, unlike Workflow Agents. It dynamically interprets instructions and context using an LLM to decide its actions, tool usage, or agent transfers.\",\\n      \"query\": \"How does `LlmAgent` behavior differ from Workflow Agents?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"* **`name` (Required):** Every agent needs a unique string identifier. This `name` is crucial for internal operations, especially in multi-agent systems where agents need to refer to or delegate tasks to each other. Choose a descriptive name that reflects the agent\\'s function (e.g., `customer_support_router`, `billing_inquiry_agent`). Avoid reserved names like `user`.\",\\n        \"* **`description` (Optional, Recommended for Multi-Agent):** Provide a concise summary of the agent\\'s capabilities. This description is primarily used by *other* LLM agents to determine if they should route a task to this agent. Make it specific enough to differentiate it from peers (e.g., \\\\\"Handles inquiries about current billing statements,\\\\\" not just \\\\\"Billing agent\\\\\").\",\\n        \"* **`model` (Required):** Specify the underlying LLM that will power this agent\\'s reasoning. This is a string identifier like `\\\\\"gemini-2.0-flash\\\\\"`. The choice of model impacts the agent\\'s capabilities, cost, and performance.\"\\n      ],\\n      \"expected_answer\": \"To define an `LlmAgent`\\'s identity and purpose, you must provide a unique `name` for internal operations and delegation, an optional but recommended `description` for other LLM agents to understand its capabilities, and a required `model` string identifier (e.g., `\\\\\"gemini-2.0-flash\\\\\"`) to specify the underlying LLM.\",\\n      \"query\": \"What are the required and recommended parameters for defining an `LlmAgent`\\'s identity and purpose?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"The `instruction` parameter is arguably the most critical for shaping an `LlmAgent`\\'s behavior. It\\'s a string (or a function returning a string) that tells the agent:\",\\n        \"* Its core task or goal.\",\\n        \"* Its personality or persona (e.g., \\\\\"You are a helpful assistant,\\\\\" \\\\\"You are a witty pirate\\\\\").\",\\n        \"* Constraints on its behavior (e.g., \\\\\"Only answer questions about X,\\\\\" \\\\\"Never reveal Y\\\\\").\",\\n        \"* How and when to use its `tools`. You should explain the purpose of each tool and the circumstances under which it should be called, supplementing any descriptions within the tool itself.\",\\n        \"* The desired format for its output (e.g., \\\\\"Respond in JSON,\\\\\" \\\\\"Provide a bulleted list\\\\\").\"\\n      ],\\n      \"expected_answer\": \"The `instruction` parameter, a string or function returning a string, is crucial for shaping an `LlmAgent`\\'s behavior. It defines the agent\\'s core task, personality, behavioral constraints, how and when to use its tools, and the desired output format.\",\\n      \"query\": \"What is the purpose of the `instruction` parameter for an `LlmAgent`?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"**Tips for Effective Instructions:**\",\\n        \"* **Be Clear and Specific:** Avoid ambiguity. Clearly state the desired actions and outcomes.\",\\n        \"* **Use Markdown:** Improve readability for complex instructions using headings, lists, etc.\",\\n        \"* **Provide Examples (Few-Shot):** For complex tasks or specific output formats, include examples directly in the instruction.\",\\n        \"* **Guide Tool Use:** Don\\'t just list tools; explain *when* and *why* the agent should use them.\"\\n      ],\\n      \"expected_answer\": \"For effective instructions, be clear and specific, use Markdown for readability, provide few-shot examples for complex tasks or formats, and guide tool use by explaining when and why tools should be used.\",\\n      \"query\": \"What are some tips for writing effective instructions for an `LlmAgent`?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"The instruction is a string template, you can use the `{var}` syntax to insert dynamic values into the instruction.\",\\n        \"* `{var}` is used to insert the value of the state variable named var.\",\\n        \"* `{artifact.var}` is used to insert the text content of the artifact named var.\",\\n        \"* If the state variable or artifact does not exist, the agent will raise an error. If you want to ignore the error, you can append a `?` to the variable name as in `{var?}`.\"\\n      ],\\n      \"expected_answer\": \"The `instruction` parameter supports string templating: `{var}` inserts a state variable\\'s value, `{artifact.var}` inserts an artifact\\'s text content. An error is raised if the variable/artifact doesn\\'t exist, unless `?` is appended (e.g., `{var?}`) to ignore the error.\",\\n      \"query\": \"How can dynamic values be inserted into an `LlmAgent`\\'s instruction?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Tools give your `LlmAgent` capabilities beyond the LLM\\'s built-in knowledge or reasoning. They allow the agent to interact with the outside world, perform calculations, fetch real-time data, or execute specific actions.\",\\n        \"* **`tools` (Optional):** Provide a list of tools the agent can use. Each item in the list can be:\",\\n        \"    * A Python function (automatically wrapped as a `FunctionTool`).\",\\n        \"    * An instance of a class inheriting from `BaseTool`.\",\\n        \"    * An instance of another agent (`AgentTool`, enabling agent-to-agent delegation - see [Multi-Agents](multi-agents.md)).\",\\n        \"The LLM uses the function/tool names, descriptions (from docstrings or the `description` field), and parameter schemas to decide which tool to call based on the conversation and its instructions.\"\\n      ],\\n      \"expected_answer\": \"Tools extend an `LlmAgent`\\'s capabilities beyond the LLM, enabling interaction with the outside world, calculations, data fetching, or specific actions. The `tools` parameter accepts Python functions, `BaseTool` instances, or other agents (`AgentTool`). The LLM uses tool names, descriptions, and parameter schemas to decide tool usage.\",\\n      \"query\": \"What is the role of `tools` for an `LlmAgent` and what types of tools can be provided?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"* **`generate_content_config` (Optional):** Pass an instance of `google.genai.types.GenerateContentConfig` to control parameters like `temperature` (randomness), `max_output_tokens` (response length), `top_p`, `top_k`, and safety settings.\"\\n      ],\\n      \"expected_answer\": \"The `generate_content_config` parameter (an instance of `google.genai.types.GenerateContentConfig`) allows fine-tuning LLM generation by controlling parameters such as `temperature`, `max_output_tokens`, `top_p`, `top_k`, and safety settings.\",\\n      \"query\": \"How can you fine-tune LLM generation for an `LlmAgent`?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"* **`input_schema` (Optional):** Define a Pydantic `BaseModel` class representing the expected input structure. If set, the user message content passed to this agent *must* be a JSON string conforming to this schema. Your instructions should guide the user or preceding agent accordingly.\",\\n        \"* **`output_schema` (Optional):** Define a Pydantic `BaseModel` class representing the desired output structure. If set, the agent\\'s final response *must* be a JSON string conforming to this schema.\",\\n        \"    * **Constraint:** Using `output_schema` enables controlled generation within the LLM but **disables the agent\\'s ability to use tools or transfer control to other agents**. Your instructions must guide the LLM to produce JSON matching the schema directly.\",\\n        \"* **`output_key` (Optional):** Provide a string key. If set, the text content of the agent\\'s *final* response will be automatically saved to the session\\'s state dictionary under this key (e.g., `session.state[output_key] = agent_response_text`). This is useful for passing results between agents or steps in a workflow.\"\\n      ],\\n      \"expected_answer\": \"`input_schema` defines the expected input JSON structure. `output_schema` defines the desired output JSON structure, but disables tool use and agent transfer. `output_key` saves the agent\\'s final response text to the session\\'s state dictionary under the specified key.\",\\n      \"query\": \"Explain the use of `input_schema`, `output_schema`, and `output_key` for `LlmAgent`.\"\\n    },\\n    {\\n      \"citations\": [\\n        \"* **`include_contents` (Optional, Default: `\\'default\\'`):** Determines if the `contents` (history) are sent to the LLM.\",\\n        \"    * `\\'default\\'`: The agent receives the relevant conversation history.\",\\n        \"    * `\\'none\\'`: The agent receives no prior `contents`. It operates based solely on its current instruction and any input provided in the *current* turn (useful for stateless tasks or enforcing specific contexts).\"\\n      ],\\n      \"expected_answer\": \"The `include_contents` parameter controls whether conversation history is sent to the LLM. `\\'default\\'` sends relevant history, while `\\'none\\'` sends no prior contents, making the agent operate solely on current instruction and input, useful for stateless tasks.\",\\n      \"query\": \"What does the `include_contents` parameter do for an `LlmAgent`?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"* **`planner` (Optional):** Assign a `BasePlanner` instance to enable multi-step reasoning and planning before execution. (See [Multi-Agents](multi-agents.md) patterns).\",\\n        \"* **`code_executor` (Optional):** Provide a `BaseCodeExecutor` instance to allow the agent to execute code blocks (e.g., Python) found in the LLM\\'s response. ([See Tools/Built-in tools](../tools/built-in-tools.md)).\"\\n      ],\\n      \"expected_answer\": \"For complex reasoning, an `LlmAgent` can use a `planner` (a `BasePlanner` instance) for multi-step reasoning and planning. A `code_executor` (a `BaseCodeExecutor` instance) allows the agent to execute code blocks from the LLM\\'s response.\",\\n      \"query\": \"How can an `LlmAgent` be configured for planning and code execution?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"The Agent Development Kit (ADK) is designed for flexibility, allowing you to integrate various Large Language Models (LLMs) into your agents. While the setup for Google Gemini models is covered in the [Setup Foundation Models](../get-started/installation.md) guide, this page details how to leverage Gemini effectively and integrate other popular models, including those hosted externally or running locally.\"\\n      ],\\n      \"expected_answer\": \"ADK is designed for flexibility in integrating various LLMs, including Google Gemini models and other popular models hosted externally or running locally.\",\\n      \"query\": \"What is ADK\\'s approach to integrating Large Language Models (LLMs)?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"ADK primarily uses two mechanisms for model integration:\",\\n        \"1. **Direct String / Registry:** For models tightly integrated with Google Cloud (like Gemini models accessed via Google AI Studio or Vertex AI) or models hosted on Vertex AI endpoints. You typically provide the model name or endpoint resource string directly to the `LlmAgent`. ADK\\'s internal registry resolves this string to the appropriate backend client, often utilizing the `google-genai` library.\",\\n        \"2. **Wrapper Classes:** For broader compatibility, especially with models outside the Google ecosystem or those requiring specific client configurations (like models accessed via LiteLLM). You instantiate a specific wrapper class (e.g., `LiteLlm`) and pass this object as the `model` parameter to your `LlmAgent`.\"\\n      ],\\n      \"expected_answer\": \"ADK integrates models using two primary mechanisms: direct string/registry for Google Cloud-integrated models (like Gemini via Google AI Studio or Vertex AI), where the model name or endpoint string is passed directly to `LlmAgent`; and wrapper classes (e.g., `LiteLlm`) for broader compatibility with external or locally hosted models, where an instantiated wrapper object is passed as the `model` parameter.\",\\n      \"query\": \"What are the two primary mechanisms ADK uses for model integration?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"This is the most direct way to use Google\\'s flagship models within ADK.\",\\n        \"**Integration Method:** Pass the model\\'s identifier string directly to the `model` parameter of `LlmAgent` (or its alias, `Agent`).\"\\n      ],\\n      \"expected_answer\": \"To use Google Gemini models directly in ADK, pass the model\\'s identifier string to the `model` parameter of `LlmAgent` (or `Agent`).\",\\n      \"query\": \"How do you integrate Google Gemini models into ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"The `google-genai` library, used internally by ADK for Gemini, can connect through either Google AI Studio or Vertex AI.\"\\n      ],\\n      \"expected_answer\": \"The `google-genai` library, used by ADK for Gemini, can connect via Google AI Studio or Vertex AI.\",\\n      \"query\": \"What are the backend options for `google-genai` when using Gemini models with ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"In order to use voice/video streaming in ADK, you will need to use Gemini models that support the Live API. You can find the **model ID(s)** that support the Gemini Live API in the documentation:\",\\n        \"- [Google AI Studio: Gemini Live API](https://ai.google.dev/gemini-api/docs/models#live-api)\",\\n        \"- [Vertex AI: Gemini Live API](https://cloud.google.com/vertex-ai/generative-ai/docs/live-api)\"\\n      ],\\n      \"expected_answer\": \"To use voice/video streaming in ADK, you need Gemini models that support the Live API. Model IDs can be found in the Google AI Studio or Vertex AI documentation for the Gemini Live API.\",\\n      \"query\": \"What is required for voice/video streaming with Gemini models in ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"**Use Case:** Google AI Studio is the easiest way to get started with Gemini. All you need is the [API key](https://aistudio.google.com/app/apikey). Best for rapid prototyping and development.\",\\n        \"**Setup:** Typically requires an API key set as an environment variable:\",\\n        \"```shell\",\\n        \"export GOOGLE_API_KEY=\\\\\"YOUR_GOOGLE_API_KEY\\\\\"\",\\n        \"export GOOGLE_GENAI_USE_VERTEXAI=FALSE\",\\n        \"```\",\\n        \"**Models:** Find all available models on the [Google AI for Developers site](https://ai.google.dev/gemini-api/docs/models).\"\\n      ],\\n      \"expected_answer\": \"Google AI Studio is ideal for rapid prototyping with Gemini. Setup requires setting `GOOGLE_API_KEY` and `GOOGLE_GENAI_USE_VERTEXAI=FALSE` environment variables. Available models are listed on the Google AI for Developers site.\",\\n      \"query\": \"How do you set up and use Google AI Studio with Gemini models in ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"**Use Case:** Recommended for production applications, leveraging Google Cloud infrastructure. Gemini on Vertex AI supports enterprise-grade features, security, and compliance controls.\",\\n        \"**Setup:**\",\\n        \"* Authenticate using Application Default Credentials (ADC):\",\\n        \"    ```shell\",\\n        \"    gcloud auth application-default login\",\\n        \"    ```\",\\n        \"* Set your Google Cloud project and location:\",\\n        \"    ```shell\",\\n        \"    export GOOGLE_CLOUD_PROJECT=\\\\\"YOUR_PROJECT_ID\\\\\"\",\\n        \"    export GOOGLE_CLOUD_LOCATION=\\\\\"YOUR_VERTEX_AI_LOCATION\\\\\" # e.g., us-central1\",\\n        \"    ```\",\\n        \"* Explicitly tell the library to use Vertex AI:\",\\n        \"    ```shell\",\\n        \"    export GOOGLE_GENAI_USE_VERTEXAI=TRUE\",\\n        \"    ```\",\\n        \"**Models:** Find available model IDs in the [Vertex AI documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models).\"\\n      ],\\n      \"expected_answer\": \"Vertex AI is recommended for production applications with Gemini, offering enterprise features. Setup involves authenticating with ADC (`gcloud auth application-default login`), setting `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` environment variables, and explicitly setting `GOOGLE_GENAI_USE_VERTEXAI=TRUE`. Available model IDs are in the Vertex AI documentation.\",\\n      \"query\": \"What is the recommended setup for using Gemini models with Vertex AI in ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"To access a vast range of LLMs from providers like OpenAI, Anthropic (non-Vertex AI), Cohere, and many others, ADK offers integration through the LiteLLM library.\",\\n        \"**Integration Method:** Instantiate the `LiteLlm` wrapper class and pass it to the `model` parameter of `LlmAgent`.\"\\n      ],\\n      \"expected_answer\": \"ADK integrates with various LLMs from providers like OpenAI, Anthropic (non-Vertex AI), and Cohere via the LiteLLM library. To use them, instantiate the `LiteLlm` wrapper class and pass it to the `model` parameter of `LlmAgent`.\",\\n      \"query\": \"How does ADK integrate with cloud and proprietary models like OpenAI or Anthropic (non-Vertex AI)?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"[LiteLLM](https://docs.litellm.ai/) acts as a translation layer, providing a standardized, OpenAI-compatible interface to over 100+ LLMs.\"\\n      ],\\n      \"expected_answer\": \"LiteLLM serves as a translation layer, offering a standardized, OpenAI-compatible interface to over 100+ LLMs.\",\\n      \"query\": \"What is LiteLLM and its primary function?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"**Setup:**\",\\n        \"1. **Install LiteLLM:**\",\\n        \"        ```shell\",\\n        \"        pip install litellm\",\\n        \"        ```\",\\n        \"2. **Set Provider API Keys:** Configure API keys as environment variables for the specific providers you intend to use.\",\\n        \"    * *Example for OpenAI:*\",\\n        \"        ```shell\",\\n        \"        export OPENAI_API_KEY=\\\\\"YOUR_OPENAI_API_KEY\\\\\"\",\\n        \"        ```\",\\n        \"    * *Example for Anthropic (non-Vertex AI):*\",\\n        \"        ```shell\",\\n        \"        export ANTHROPIC_API_KEY=\\\\\"YOUR_ANTHROPIC_API_KEY\\\\\"\",\\n        \"        ```\",\\n        \"    * *Consult the [LiteLLM Providers Documentation](https://docs.litellm.ai/docs/providers) for the correct environment variable names for other providers.*\"\\n      ],\\n      \"expected_answer\": \"To set up LiteLLM, first install it using `pip install litellm`. Then, configure API keys for specific providers as environment variables, such as `OPENAI_API_KEY` for OpenAI or `ANTHROPIC_API_KEY` for Anthropic (non-Vertex AI).\",\\n      \"query\": \"What are the setup steps for using LiteLLM with ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"For maximum control, cost savings, privacy, or offline use cases, you can run open-source models locally or self-host them and integrate them using LiteLLM.\"\\n      ],\\n      \"expected_answer\": \"Open-source models can be run locally or self-hosted and integrated via LiteLLM for maximum control, cost savings, privacy, or offline use cases.\",\\n      \"query\": \"Why would someone use open and local models via LiteLLM?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"[Ollama](https://ollama.com/) allows you to easily run open-source models locally.\"\\n      ],\\n      \"expected_answer\": \"Ollama enables easy local execution of open-source models.\",\\n      \"query\": \"What is Ollama used for?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"If your agent is relying on tools, please make sure that you select a model with tool support from [Ollama website](https://ollama.com/search?c=tools).\",\\n        \"For reliable results, we recommend using a decent-sized model with tool support.\",\\n        \"The tool support for the model can be checked with the following command:\",\\n        \"```bash\",\\n        \"ollama show mistral-small3.1\",\\n        \"  Model\",\\n        \"    architecture        mistral3\",\\n        \"    parameters          24.0B\",\\n        \"    context length      131072\",\\n        \"    embedding length    5120\",\\n        \"    quantization        Q4_K_M\",\\n        \"  Capabilities\",\\n        \"    completion\",\\n        \"    vision\",\\n        \"    tools\",\\n        \"```\",\\n        \"You are supposed to see `tools` listed under capabilities.\"\\n      ],\\n      \"expected_answer\": \"When using Ollama models with agents that rely on tools, select a model with tool support from the Ollama website. Verify tool support by running `ollama show <model_name>` and checking for `tools` under \\\\\"Capabilities\\\\\". A decent-sized model with tool support is recommended for reliable results.\",\\n      \"query\": \"How do you ensure an Ollama model supports tools for an ADK agent?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"You can also look at the template the model is using and tweak it based on your needs.\",\\n        \"```bash\",\\n        \"ollama show --modelfile llama3.2 > model_file_to_modify\",\\n        \"```\",\\n        \"For instance, the default template for the above model inherently suggests that the model shall call a function all the time. This may result in an infinite loop of function calls.\",\\n        \"```\",\\n        \"Given the following functions, please respond with a JSON for a function call\",\\n        \"with its proper arguments that best answers the given prompt.\",\\n        \"Respond in the format {\\\\\"name\\\\\": function name, \\\\\"parameters\\\\\": dictionary of\",\\n        \"argument name and its value}. Do not use variables.\",\\n        \"```\",\\n        \"You can swap such prompts with a more descriptive one to prevent infinite tool call loops.\",\\n        \"For instance:\",\\n        \"```\",\\n        \"Review the user\\'s prompt and the available functions listed below.\",\\n        \"First, determine if calling one of these functions is the most appropriate way to respond. A function call is likely needed if the prompt asks for a specific action, requires external data lookup, or involves calculations handled by the functions. If the prompt is a general question or can be answered directly, a function call is likely NOT needed.\",\\n        \"If you determine a function call IS required: Respond ONLY with a JSON object in the format {\\\\\"name\\\\\": \\\\\"function_name\\\\\", \\\\\"parameters\\\\\": {\\\\\"argument_name\\\\\": \\\\\"value\\\\\"}}. Ensure parameter values are concrete, not variables.\",\\n        \"If you determine a function call IS NOT required: Respond directly to the user\\'s prompt in plain text, providing the answer or information requested. Do not output any JSON.\",\\n        \"```\",\\n        \"Then you can create a new model with the following command:\",\\n        \"```bash\",\\n        \"ollama create llama3.2-modified -f model_file_to_modify\",\\n        \"```\"\\n      ],\\n      \"expected_answer\": \"To prevent infinite tool call loops with Ollama models, you can modify the model\\'s template. First, export the model\\'s modelfile using `ollama show --modelfile <model_name> > model_file_to_modify`. Then, edit the prompt to include conditional logic for tool calls, such as determining if a function call is appropriate or if a direct plain text response is needed. Finally, create a new model with the modified modelfile using `ollama create <new_model_name> -f model_file_to_modify`.\",\\n      \"query\": \"How can you prevent infinite tool call loops when using Ollama models with ADK agents?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Our LiteLLM wrapper can be used to create agents with Ollama models.\",\\n        \"```py\",\\n        \"root_agent = Agent(\",\\n        \"    model=LiteLlm(model=\\\\\"ollama_chat/mistral-small3.1\\\\\"),\",\\n        \"    name=\\\\\"dice_agent\\\\\",\",\\n        \"    description=(\",\\n        \"        \\\\\"hello world agent that can roll a dice of 8 sides and check prime\\\\\"\",\\n        \"        \\\\\" numbers.\\\\\"\",\\n        \"    ),\",\\n        \"    instruction=\\\\\"\\\\\"\\\\\"\",\\n        \"      You roll dice and answer questions about the outcome of the dice rolls.\",\\n        \"    \\\\\"\\\\\"\\\\\",\",\\n        \"    tools=[\",\\n        \"        roll_die,\",\\n        \"        check_prime,\",\\n        \"    ],\",\\n        \")\",\\n        \"```\",\\n        \"**It is important to set the provider `ollama_chat` instead of `ollama`. Using `ollama` will result in unexpected behaviors such as infinite tool call loops and ignoring previous context.**\"\\n      ],\\n      \"expected_answer\": \"To create agents with Ollama models using the LiteLLM wrapper, set the `model` parameter to `LiteLlm(model=\\\\\"ollama_chat/<model_name>\\\\\")`. It is crucial to use `ollama_chat` as the provider instead of `ollama` to avoid issues like infinite tool call loops and ignoring previous context.\",\\n      \"query\": \"How do you configure an ADK agent to use an Ollama model via LiteLLM, and what is an important consideration?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"While `api_base` can be provided inside LiteLLM for generation, LiteLLM library is calling other APIs relying on the env variable instead as of v1.65.5 after completion. So at this time, we recommend setting the env variable `OLLAMA_API_BASE` to point to the ollama server.\",\\n        \"```bash\",\\n        \"export OLLAMA_API_BASE=\\\\\"http://localhost:11434\\\\\"\",\\n        \"adk web\",\\n        \"```\"\\n      ],\\n      \"expected_answer\": \"It is recommended to set the `OLLAMA_API_BASE` environment variable to point to the Ollama server (e.g., `http://localhost:11434`) because the LiteLLM library relies on this environment variable for API calls after completion, as of v1.65.5.\",\\n      \"query\": \"Why is it recommended to set the `OLLAMA_API_BASE` environment variable when using Ollama with ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Alternatively, `openai` can be used as the provider name. But this will also require setting the `OPENAI_API_BASE=http://localhost:11434/v1` and `OPENAI_API_KEY=anything` env variables instead of `OLLAMA_API_BASE`. **Please note that api base now has `/v1` at the end.**\",\\n        \"```py\",\\n        \"root_agent = Agent(\",\\n        \"    model=LiteLlm(model=\\\\\"openai/mistral-small3.1\\\\\"),\",\\n        \"    name=\\\\\"dice_agent\\\\\",\",\\n        \"    description=(\",\\n        \"        \\\\\"hello world agent that can roll a dice of 8 sides and check prime\\\\\"\",\\n        \"        \\\\\" numbers.\\\\\"\",\\n        \"    ),\",\\n        \"    instruction=\\\\\"\\\\\"\\\\\"\",\\n        \"      You roll dice and answer questions about the outcome of the dice rolls.\",\\n        \"    \\\\\"\\\\\"\\\\\",\",\\n        \"    tools=[\",\\n        \"        roll_die,\",\\n        \"        check_prime,\",\\n        \"    ],\",\\n        \")\",\\n        \"```\",\\n        \"```bash\",\\n        \"export OPENAI_API_BASE=http://localhost:11434/v1\",\\n        \"export OPENAI_API_KEY=anything\",\\n        \"adk web\",\\n        \"```\"\\n      ],\\n      \"expected_answer\": \"Alternatively, you can use `openai` as the provider name for Ollama models with LiteLLM. This requires setting the environment variables `OPENAI_API_BASE` to `http://localhost:11434/v1` (note the `/v1` suffix) and `OPENAI_API_KEY` to any value.\",\\n      \"query\": \"Can Ollama models be used with the `openai` provider name in LiteLLM, and if so, what environment variables are needed?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"You can see the request sent to the Ollama server by adding the following in your agent code just after imports.\",\\n        \"```py\",\\n        \"import litellm\",\\n        \"litellm._turn_on_debug()\",\\n        \"```\"\\n      ],\\n      \"expected_answer\": \"To debug requests sent to the Ollama server, add `import litellm` and `litellm._turn_on_debug()` in your agent code after imports.\",\\n      \"query\": \"How can you debug requests sent to the Ollama server from an ADK agent?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Tools such as [vLLM](https://github.com/vllm-project/vllm) allow you to host models efficiently and often expose an OpenAI-compatible API endpoint.\"\\n      ],\\n      \"expected_answer\": \"Tools like vLLM enable efficient hosting of models and often expose an OpenAI-compatible API endpoint.\",\\n      \"query\": \"What is vLLM used for in the context of model hosting?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"**Setup:**\",\\n        \"1. **Deploy Model:** Deploy your chosen model using vLLM (or a similar tool). Note the API base URL (e.g., `https://your-vllm-endpoint.run.app/v1`).\",\\n        \"    * *Important for ADK Tools:* When deploying, ensure the serving tool supports and enables OpenAI-compatible tool/function calling. For vLLM, this might involve flags like `--enable-auto-tool-choice` and potentially a specific `--tool-call-parser`, depending on the model.\",\\n        \"2. **Authentication:** Determine how your endpoint handles authentication (e.g., API key, bearer token).\"\\n      ],\\n      \"expected_answer\": \"To set up a self-hosted endpoint like vLLM, first deploy your model and note the API base URL. Crucially, ensure the serving tool supports OpenAI-compatible tool/function calling (e.g., with `--enable-auto-tool-choice` for vLLM). Then, determine the authentication method for your endpoint.\",\\n      \"query\": \"What are the setup steps for using a self-hosted endpoint (e.g., vLLM) with ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"For enterprise-grade scalability, reliability, and integration with Google Cloud\\'s MLOps ecosystem, you can use models deployed to Vertex AI Endpoints. This includes models from Model Garden or your own fine-tuned models.\"\\n      ],\\n      \"expected_answer\": \"Models deployed to Vertex AI Endpoints, including those from Model Garden or fine-tuned models, can be used for enterprise-grade scalability, reliability, and integration with Google Cloud\\'s MLOps ecosystem.\",\\n      \"query\": \"What are the benefits of using hosted and tuned models on Vertex AI?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"**Vertex AI Setup (Consolidated):**\",\\n        \"Ensure your environment is configured for Vertex AI:\",\\n        \"1. **Authentication:** Use Application Default Credentials (ADC):\",\\n        \"    ```shell\",\\n        \"    gcloud auth application-default login\",\\n        \"    ```\",\\n        \"2. **Environment Variables:** Set your project and location:\",\\n        \"    ```shell\",\\n        \"    export GOOGLE_CLOUD_PROJECT=\\\\\"YOUR_PROJECT_ID\\\\\"\",\\n        \"    export GOOGLE_CLOUD_LOCATION=\\\\\"YOUR_VERTEX_AI_LOCATION\\\\\" # e.g., us-central1\",\\n        \"    ```\",\\n        \"3. **Enable Vertex Backend:** Crucially, ensure the `google-genai` library targets Vertex AI:\",\\n        \"    ```shell\",\\n        \"    export GOOGLE_GENAI_USE_VERTEXAI=TRUE\",\\n        \"    ```\"\\n      ],\\n      \"expected_answer\": \"To configure your environment for Vertex AI, authenticate using Application Default Credentials (`gcloud auth application-default login`), set `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` environment variables, and enable the Vertex backend by setting `GOOGLE_GENAI_USE_VERTEXAI=TRUE`.\",\\n      \"query\": \"What is the consolidated setup for Vertex AI when using ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"You can deploy various open and proprietary models from the [Vertex AI Model Garden](https://console.cloud.google.com/vertex-ai/model-garden) to an endpoint.\"\\n      ],\\n      \"expected_answer\": \"Various open and proprietary models from the Vertex AI Model Garden can be deployed to an endpoint.\",\\n      \"query\": \"What types of models can be deployed from Vertex AI Model Garden?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Deploying your fine-tuned models (whether based on Gemini or other architectures supported by Vertex AI) results in an endpoint that can be used directly.\"\\n      ],\\n      \"expected_answer\": \"Fine-tuned models, including those based on Gemini or other Vertex AI-supported architectures, can be deployed to a directly usable endpoint.\",\\n      \"query\": \"Can fine-tuned models be used directly via Vertex AI endpoints?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Some providers, like Anthropic, make their models available directly through Vertex AI.\",\\n        \"**Integration Method:** Uses the direct model string (e.g., `\\\\\"claude-3-sonnet@20240229\\\\\"`), *but requires manual registration* within ADK.\",\\n        \"**Why Registration?** ADK\\'s registry automatically recognizes `gemini-*` strings and standard Vertex AI endpoint strings (`projects/.../endpoints/...`) and routes them via the `google-genai` library. For other model types used directly via Vertex AI (like Claude), you must explicitly tell the ADK registry which specific wrapper class (`Claude` in this case) knows how to handle that model identifier string with the Vertex AI backend.\"\\n      ],\\n      \"expected_answer\": \"Some third-party models, like Anthropic\\'s Claude, are available directly through Vertex AI. Their integration uses a direct model string (e.g., `\\\\\"claude-3-sonnet@20240229\\\\\"`) but requires manual registration within ADK. This is because ADK\\'s registry needs to be explicitly told which wrapper class (e.g., `Claude`) handles that model identifier with the Vertex AI backend, unlike `gemini-*` or standard Vertex AI endpoint strings which are automatically recognized.\",\\n      \"query\": \"How are third-party models like Anthropic Claude integrated with ADK via Vertex AI, and why is manual registration needed?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"**Setup:**\",\\n        \"1. **Vertex AI Environment:** Ensure the consolidated Vertex AI setup (ADC, Env Vars, `GOOGLE_GENAI_USE_VERTEXAI=TRUE`) is complete.\",\\n        \"2. **Install Provider Library:** Install the necessary client library configured for Vertex AI.\",\\n        \"    ```shell\",\\n        \"    pip install \\\\\"anthropic[vertex]\\\\\"\",\\n        \"    ```\",\\n        \"3. **Register Model Class:** Add this code near the start of your application, *before* creating an agent using the Claude model string:\",\\n        \"    ```python\",\\n        \"    # Required for using Claude model strings directly via Vertex AI with LlmAgent\",\\n        \"    from google.adk.models.anthropic_llm import Claude\",\\n        \"    from google.adk.models.registry import LLMRegistry\",\\n        \"    LLMRegistry.register(Claude)\",\\n        \"    ```\"\\n      ],\\n      \"expected_answer\": \"To set up third-party models like Anthropic Claude on Vertex AI with ADK, first ensure the consolidated Vertex AI environment setup is complete. Then, install the necessary provider client library (e.g., `pip install \\\\\"anthropic[vertex]\\\\\"`). Finally, register the model class (e.g., `Claude`) with `LLMRegistry.register(Claude)` at the start of your application, before creating any agents using that model string.\",\\n      \"query\": \"What are the setup steps for integrating third-party models (e.g., Anthropic Claude) on Vertex AI with ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"The Agent Development Kit (ADK) supports building sophisticated applications by composing multiple, distinct `BaseAgent` instances into a **Multi-Agent System (MAS)**.\"\\n      ],\\n      \"expected_answer\": \"The Agent Development Kit (ADK) supports building sophisticated applications by composing multiple, distinct `BaseAgent` instances into a Multi-Agent System (MAS).\",\\n      \"query\": \"What is a Multi-Agent System (MAS) in ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"In ADK, a multi-agent system is an application where different agents, often forming a hierarchy, collaborate or coordinate to achieve a larger goal. Structuring your application this way offers significant advantages, including enhanced modularity, specialization, reusability, maintainability, and the ability to define structured control flows using dedicated workflow agents.\"\\n      ],\\n      \"expected_answer\": \"In ADK, a multi-agent system involves different agents, often in a hierarchy, collaborating to achieve a larger goal. This structure enhances modularity, specialization, reusability, maintainability, and allows for structured control flows using workflow agents.\",\\n      \"query\": \"What are the advantages of structuring an application as a Multi-Agent System in ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"You can compose various types of agents derived from `BaseAgent` to build these systems:\",\\n        \"* **LLM Agents:** Agents powered by large language models. (See [LLM Agents](llm-agents.md))\",\\n        \"* **Workflow Agents:** Specialized agents (`SequentialAgent`, `ParallelAgent`, `LoopAgent`) designed to manage the execution flow of their sub-agents. (See [Workflow Agents](workflow-agents/index.md))\",\\n        \"* **Custom agents:** Your own agents inheriting from `BaseAgent` with specialized, non-LLM logic. (See [Custom Agents](custom-agents.md))\"\\n      ],\\n      \"expected_answer\": \"Multi-agent systems in ADK can be composed of LLM Agents (powered by LLMs), Workflow Agents (`SequentialAgent`, `ParallelAgent`, `LoopAgent`) for managing execution flow, and Custom Agents (inheriting from `BaseAgent`) for specialized, non-LLM logic.\",\\n      \"query\": \"What types of agents can be composed to build Multi-Agent Systems in ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"The foundation for structuring multi-agent systems is the parent-child relationship defined in `BaseAgent`.\",\\n        \"* **Establishing Hierarchy:** You create a tree structure by passing a list of agent instances to the `sub_agents` argument when initializing a parent agent. ADK automatically sets the `parent_agent` attribute on each child agent during initialization (`google.adk.agents.base_agent.py` - `model_post_init`).\",\\n        \"* **Single Parent Rule:** An agent instance can only be added as a sub-agent once. Attempting to assign a second parent will result in a `ValueError`.\",\\n        \"* **Importance:** This hierarchy defines the scope for [Workflow Agents](#22-workflow-agents-as-orchestrators) and influences the potential targets for LLM-Driven Delegation. You can navigate the hierarchy using `agent.parent_agent` or find descendants using `agent.find_agent(name)`.\"\\n      ],\\n      \"expected_answer\": \"The parent-child relationship in `BaseAgent` forms the hierarchy of multi-agent systems. This is established by passing a list of agent instances to the `sub_agents` argument of a parent agent, which automatically sets the `parent_agent` attribute on children. An agent can only have one parent. This hierarchy is crucial for Workflow Agents and LLM-Driven Delegation, and can be navigated using `agent.parent_agent` or `agent.find_agent(name)`.\",\\n      \"query\": \"How is agent hierarchy established and managed in ADK, and why is it important?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"ADK includes specialized agents derived from `BaseAgent` that don\\'t perform tasks themselves but orchestrate the execution flow of their `sub_agents`.\"\\n      ],\\n      \"expected_answer\": \"ADK includes specialized agents derived from `BaseAgent` that orchestrate the execution flow of their `sub_agents` rather than performing tasks themselves.\",\\n      \"query\": \"What is the primary role of Workflow Agents in ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"* **[`SequentialAgent`](workflow-agents/sequential-agents.md):** Executes its `sub_agents` one after another in the order they are listed.\",\\n        \"    * **Context:** Passes the *same* [`InvocationContext`](../runtime/index.md) sequentially, allowing agents to easily pass results via shared state.\"\\n      ],\\n      \"expected_answer\": \"A `SequentialAgent` executes its `sub_agents` one after another in order, passing the same `InvocationContext` sequentially to allow results to be shared via a shared state.\",\\n      \"query\": \"How does `SequentialAgent` work and manage context?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"* **[`ParallelAgent`](workflow-agents/parallel-agents.md):** Executes its `sub_agents` in parallel. Events from sub-agents may be interleaved.\",\\n        \"    * **Context:** Modifies the `InvocationContext.branch` for each child agent (e.g., `ParentBranch.ChildName`), providing a distinct contextual path which can be useful for isolating history in some memory implementations.\",\\n        \"    * **State:** Despite different branches, all parallel children access the *same shared* `session.state`, enabling them to read initial state and write results (use distinct keys to avoid race conditions).\"\\n      ],\\n      \"expected_answer\": \"A `ParallelAgent` executes its `sub_agents` in parallel, potentially interleaving events. It modifies `InvocationContext.branch` for each child for distinct contextual paths, but all parallel children access the same shared `session.state` for reading initial state and writing results, requiring distinct keys to avoid race conditions.\",\\n      \"query\": \"How does `ParallelAgent` execute sub-agents and manage context and state?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"* **[`LoopAgent`](workflow-agents/loop-agents.md):** Executes its `sub_agents` sequentially in a loop.\",\\n        \"    * **Termination:** The loop stops if the optional `max_iterations` is reached, or if any sub-agent yields an [`Event`](../events/index.md) with `actions.escalate=True`.\",\\n        \"    * **Context & State:** Passes the *same* `InvocationContext` in each iteration, allowing state changes (e.g., counters, flags) to persist across loops.\"\\n      ],\\n      \"expected_answer\": \"A `LoopAgent` executes its `sub_agents` sequentially in a loop. The loop terminates if `max_iterations` is reached or if a sub-agent yields an `Event` with `actions.escalate=True`. It passes the same `InvocationContext` in each iteration, allowing state changes to persist.\",\\n      \"query\": \"How does `LoopAgent` function, and what are its termination conditions?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"The `instruction` parameter is arguably the most critical for shaping an `LlmAgent`\\'s behavior. It\\'s a string (or a function returning a string) that tells the agent:\",\\n        \"* Its core task or goal.\",\\n        \"* Its personality or persona (e.g., \\\\\"You are a helpful assistant,\\\\\" \\\\\"You are a witty pirate\\\\\").\",\\n        \"* Constraints on its behavior (e.g., \\\\\"Only answer questions about X,\\\\\" \\\\\"Never reveal Y\\\\\").\",\\n        \"* How and when to use its `tools`. You should explain the purpose of each tool and the circumstances under which it should be called, supplementing any descriptions within the tool itself.\",\\n        \"* The desired format for its output (e.g., \\\\\"Respond in JSON,\\\\\" \\\\\"Provide a bulleted list\\\\\").\"\\n      ],\\n      \"expected_answer\": \"The `instruction` parameter is crucial for defining an `LlmAgent`\\'s core task, personality, behavioral constraints, tool usage guidelines, and desired output format.\",\\n      \"query\": \"What aspects of an `LlmAgent`\\'s behavior are defined by the `instruction` parameter?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"**Tips for Effective Instructions:**\",\\n        \"* **Be Clear and Specific:** Avoid ambiguity. Clearly state the desired actions and outcomes.\",\\n        \"* **Use Markdown:** Improve readability for complex instructions using headings, lists, etc.\",\\n        \"* **Provide Examples (Few-Shot):** For complex tasks or specific output formats, include examples directly in the instruction.\",\\n        \"* **Guide Tool Use:** Don\\'t just list tools; explain *when* and *why* the agent should use them.\"\\n      ],\\n      \"expected_answer\": \"To write effective instructions, be clear and specific, use Markdown for readability, provide few-shot examples for complex tasks, and guide tool use by explaining when and why tools should be used.\",\\n      \"query\": \"What are the best practices for crafting effective instructions for an `LlmAgent`?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"The instruction is a string template, you can use the `{var}` syntax to insert dynamic values into the instruction.\",\\n        \"* `{var}` is used to insert the value of the state variable named var.\",\\n        \"* `{artifact.var}` is used to insert the text content of the artifact named var.\",\\n        \"* If the state variable or artifact does not exist, the agent will raise an error. If you want to ignore the error, you can append a `?` to the variable name as in `{var?}`.\"\\n      ],\\n      \"expected_answer\": \"Dynamic values can be inserted into instructions using `{var}` for state variables and `{artifact.var}` for artifact content. Appending `?` to the variable name (e.g., `{var?}`) prevents errors if the variable or artifact does not exist.\",\\n      \"query\": \"How does string templating work within `LlmAgent` instructions?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Tools give your `LlmAgent` capabilities beyond the LLM\\'s built-in knowledge or reasoning. They allow the agent to interact with the outside world, perform calculations, fetch real-time data, or execute specific actions.\"\\n      ],\\n      \"expected_answer\": \"Tools extend an `LlmAgent`\\'s capabilities by enabling interaction with external systems, performing calculations, fetching real-time data, and executing specific actions.\",\\n      \"query\": \"What capabilities do tools provide to an `LlmAgent`?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"* **`tools` (Optional):** Provide a list of tools the agent can use. Each item in the list can be:\",\\n        \"    * A Python function (automatically wrapped as a `FunctionTool`).\",\\n        \"    * An instance of a class inheriting from `BaseTool`.\",\\n        \"    * An instance of another agent (`AgentTool`, enabling agent-to-agent delegation - see [Multi-Agents](multi-agents.md)).\"\\n      ],\\n      \"expected_answer\": \"The `tools` parameter accepts a list of Python functions (wrapped as `FunctionTool`), instances of `BaseTool` subclasses, or instances of other agents (`AgentTool`) for agent-to-agent delegation.\",\\n      \"query\": \"What are the different types of items that can be included in the `tools` list for an `LlmAgent`?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"The LLM uses the function/tool names, descriptions (from docstrings or the `description` field), and parameter schemas to decide which tool to call based on the conversation and its instructions.\"\\n      ],\\n      \"expected_answer\": \"The LLM decides which tool to call based on the tool\\'s name, description (from docstrings or the `description` field), and parameter schemas, in conjunction with the conversation context and its instructions.\",\\n      \"query\": \"How does the LLM decide which tool to call?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"* **`generate_content_config` (Optional):** Pass an instance of `google.genai.types.GenerateContentConfig` to control parameters like `temperature` (randomness), `max_output_tokens` (response length), `top_p`, `top_k`, and safety settings.\"\\n      ],\\n      \"expected_answer\": \"The `generate_content_config` parameter, an instance of `google.genai.types.GenerateContentConfig`, allows control over LLM generation parameters such as `temperature`, `max_output_tokens`, `top_p`, `top_k`, and safety settings.\",\\n      \"query\": \"Which parameters can be controlled using `generate_content_config`?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"* **`input_schema` (Optional):** Define a Pydantic `BaseModel` class representing the expected input structure. If set, the user message content passed to this agent *must* be a JSON string conforming to this schema. Your instructions should guide the user or preceding agent accordingly.\"\\n      ],\\n      \"expected_answer\": \"The `input_schema` parameter defines a Pydantic `BaseModel` for the expected input structure. If set, the user message content must be a JSON string conforming to this schema, and instructions should guide users or preceding agents accordingly.\",\\n      \"query\": \"What is the purpose of `input_schema` in `LlmAgent` configuration?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"* **`output_schema` (Optional):** Define a Pydantic `BaseModel` class representing the desired output structure. If set, the agent\\'s final response *must* be a JSON string conforming to this schema.\",\\n        \"    * **Constraint:** Using `output_schema` enables controlled generation within the LLM but **disables the agent\\'s ability to use tools or transfer control to other agents**. Your instructions must guide the LLM to produce JSON matching the schema directly.\"\\n      ],\\n      \"expected_answer\": \"The `output_schema` parameter defines a Pydantic `BaseModel` for the desired output JSON structure. When used, the agent\\'s final response must conform to this schema, but it disables the agent\\'s ability to use tools or transfer control to other agents, requiring the LLM to directly produce the JSON.\",\\n      \"query\": \"What is the effect of using `output_schema` on an `LlmAgent`\\'s behavior?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"* **`output_key` (Optional):** Provide a string key. If set, the text content of the agent\\'s *final* response will be automatically saved to the session\\'s state dictionary under this key (e.g., `session.state[output_key] = agent_response_text`). This is useful for passing results between agents or steps in a workflow.\"\\n      ],\\n      \"expected_answer\": \"The `output_key` parameter, if set, automatically saves the `LlmAgent`\\'s final response text to the session\\'s state dictionary under the specified key, facilitating result passing between agents or workflow steps.\",\\n      \"query\": \"How does `output_key` facilitate data transfer between agents?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"* **`include_contents` (Optional, Default: `\\'default\\'`):** Determines if the `contents` (history) are sent to the LLM.\",\\n        \"    * `\\'default\\'`: The agent receives the relevant conversation history.\",\\n        \"    * `\\'none\\'`: The agent receives no prior `contents`. It operates based solely on its current instruction and any input provided in the *current* turn (useful for stateless tasks or enforcing specific contexts).\"\\n      ],\\n      \"expected_answer\": \"The `include_contents` parameter controls whether the LLM receives conversation history. `\\'default\\'` includes history, while `\\'none\\'` excludes it, making the agent stateless and reliant only on current input and instructions.\",\\n      \"query\": \"Explain the difference between `include_contents=\\'default\\'` and `include_contents=\\'none\\'`.\"\\n    },\\n    {\\n      \"citations\": [\\n        \"* **`planner` (Optional):** Assign a `BasePlanner` instance to enable multi-step reasoning and planning before execution. (See [Multi-Agents](multi-agents.md) patterns).\",\\n        \"* **`code_executor` (Optional):** Provide a `BaseCodeExecutor` instance to allow the agent to execute code blocks (e.g., Python) found in the LLM\\'s response. ([See Tools/Built-in tools](../tools/built-in-tools.md)).\"\\n      ],\\n      \"expected_answer\": \"An `LlmAgent` can be configured with a `planner` (a `BasePlanner` instance) for multi-step reasoning and planning, and a `code_executor` (a `BaseCodeExecutor` instance) to execute code blocks from the LLM\\'s response.\",\\n      \"query\": \"What are `planner` and `code_executor` used for in `LlmAgent`?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"The `google-genai` library, used internally by ADK for Gemini, can connect through either Google AI Studio or Vertex AI.\"\\n      ],\\n      \"expected_answer\": \"The `google-genai` library, which ADK uses for Gemini, can connect via Google AI Studio or Vertex AI.\",\\n      \"query\": \"What are the two connection options for the `google-genai` library with Gemini in ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"In order to use voice/video streaming in ADK, you will need to use Gemini models that support the Live API.\"\\n      ],\\n      \"expected_answer\": \"To use voice/video streaming in ADK, Gemini models that support the Live API are required.\",\\n      \"query\": \"What is a prerequisite for using voice/video streaming with Gemini models in ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Google AI Studio is the easiest way to get started with Gemini. All you need is the [API key](https://aistudio.google.com/app/apikey). Best for rapid prototyping and development.\"\\n      ],\\n      \"expected_answer\": \"Google AI Studio is the easiest way to start with Gemini, requiring only an API key, and is best for rapid prototyping and development.\",\\n      \"query\": \"What is the primary benefit of using Google AI Studio for Gemini with ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Vertex AI is recommended for production applications, leveraging Google Cloud infrastructure. Gemini on Vertex AI supports enterprise-grade features, security, and compliance controls.\"\\n      ],\\n      \"expected_answer\": \"Vertex AI is recommended for production applications due to its enterprise-grade features, security, and compliance controls, leveraging Google Cloud infrastructure.\",\\n      \"query\": \"Why is Vertex AI recommended for production applications with Gemini in ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"To access a vast range of LLMs from providers like OpenAI, Anthropic (non-Vertex AI), Cohere, and many others, ADK offers integration through the LiteLLM library.\"\\n      ],\\n      \"expected_answer\": \"ADK integrates with a wide range of LLMs from providers like OpenAI, Anthropic (non-Vertex AI), and Cohere through the LiteLLM library.\",\\n      \"query\": \"Which library does ADK use to integrate with various LLM providers like OpenAI and Anthropic (non-Vertex AI)?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"[LiteLLM](https://docs.litellm.ai/) acts as a translation layer, providing a standardized, OpenAI-compatible interface to over 100+ LLMs.\"\\n      ],\\n      \"expected_answer\": \"LiteLLM functions as a translation layer, offering a standardized, OpenAI-compatible interface to more than 100 LLMs.\",\\n      \"query\": \"How many LLMs does LiteLLM provide an interface to?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"If your agent is relying on tools, please make sure that you select a model with tool support from [Ollama website](https://ollama.com/search?c=tools).\"\\n      ],\\n      \"expected_answer\": \"If an ADK agent relies on tools, the Ollama model selected must have tool support.\",\\n      \"query\": \"What is a key consideration when selecting an Ollama model for an ADK agent that uses tools?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"It is important to set the provider `ollama_chat` instead of `ollama`. Using `ollama` will result in unexpected behaviors such as infinite tool call loops and ignoring previous context.\"\\n      ],\\n      \"expected_answer\": \"When using Ollama models with LiteLLM, it is important to set the provider to `ollama_chat` instead of `ollama` to avoid issues like infinite tool call loops and ignoring previous context.\",\\n      \"query\": \"What is the correct provider name for Ollama models in LiteLLM, and why is it important?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"While `api_base` can be provided inside LiteLLM for generation, LiteLLM library is calling other APIs relying on the env variable instead as of v1.65.5 after completion. So at this time, we recommend setting the env variable `OLLAMA_API_BASE` to point to the ollama server.\"\\n      ],\\n      \"expected_answer\": \"It is recommended to set the `OLLAMA_API_BASE` environment variable to point to the Ollama server because the LiteLLM library, as of v1.65.5, relies on this environment variable for API calls after completion.\",\\n      \"query\": \"Why is `OLLAMA_API_BASE` environment variable recommended for Ollama integration?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Alternatively, `openai` can be used as the provider name. But this will also require setting the `OPENAI_API_BASE=http://localhost:11434/v1` and `OPENAI_API_KEY=anything` env variables instead of `OLLAMA_API_BASE`. **Please note that api base now has `/v1` at the end.**\"\\n      ],\\n      \"expected_answer\": \"The `openai` provider name can be used for Ollama models, but it requires setting `OPENAI_API_BASE` to `http://localhost:11434/v1` and `OPENAI_API_KEY` to any value, instead of `OLLAMA_API_BASE`.\",\\n      \"query\": \"What environment variables are needed if using `openai` as the provider name for Ollama models?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Tools such as [vLLM](https://github.com/vllm-project/vllm) allow you to host models efficiently and often expose an OpenAI-compatible API endpoint.\"\\n      ],\\n      \"expected_answer\": \"vLLM allows efficient hosting of models and often provides an OpenAI-compatible API endpoint.\",\\n      \"query\": \"What kind of API endpoint does vLLM typically expose?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"* *Important for ADK Tools:* When deploying, ensure the serving tool supports and enables OpenAI-compatible tool/function calling. For vLLM, this might involve flags like `--enable-auto-tool-choice` and potentially a specific `--tool-call-parser`, depending on the model.\"\\n      ],\\n      \"expected_answer\": \"When deploying a model with vLLM for ADK tools, it\\'s crucial to ensure the serving tool supports and enables OpenAI-compatible tool/function calling, potentially using flags like `--enable-auto-tool-choice` and a specific `--tool-call-parser`.\",\\n      \"query\": \"What is important for ADK Tools when deploying a model with vLLM?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"For enterprise-grade scalability, reliability, and integration with Google Cloud\\'s MLOps ecosystem, you can use models deployed to Vertex AI Endpoints. This includes models from Model Garden or your own fine-tuned models.\"\\n      ],\\n      \"expected_answer\": \"Models deployed to Vertex AI Endpoints, including those from Model Garden or fine-tuned models, offer enterprise-grade scalability, reliability, and integration with Google Cloud\\'s MLOps ecosystem.\",\\n      \"query\": \"What are the advantages of using Vertex AI Endpoints for models?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Some providers, like Anthropic, make their models available directly through Vertex AI.\"\\n      ],\\n      \"expected_answer\": \"Providers such as Anthropic offer their models directly through Vertex AI.\",\\n      \"query\": \"Are third-party models available directly through Vertex AI?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"ADK\\'s registry automatically recognizes `gemini-*` strings and standard Vertex AI endpoint strings (`projects/.../endpoints/...`) and routes them via the `google-genai` library. For other model types used directly via Vertex AI (like Claude), you must explicitly tell the ADK registry which specific wrapper class (`Claude` in this case) knows how to handle that model identifier string with the Vertex AI backend.\"\\n      ],\\n      \"expected_answer\": \"ADK\\'s registry automatically routes `gemini-*` and standard Vertex AI endpoint strings via `google-genai`. However, for other Vertex AI model types like Claude, you must explicitly register the corresponding wrapper class (e.g., `Claude`) with the ADK registry.\",\\n      \"query\": \"Why is explicit registration needed for some Vertex AI models in ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"The Agent Development Kit (ADK) supports building sophisticated applications by composing multiple, distinct `BaseAgent` instances into a **Multi-Agent System (MAS)**.\"\\n      ],\\n      \"expected_answer\": \"ADK facilitates the creation of sophisticated applications by allowing the composition of multiple, distinct `BaseAgent` instances into a Multi-Agent System (MAS).\",\\n      \"query\": \"What is the primary capability of ADK regarding complex applications?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"In ADK, a multi-agent system is an application where different agents, often forming a hierarchy, collaborate or coordinate to achieve a larger goal.\"\\n      ],\\n      \"expected_answer\": \"In ADK, a multi-agent system is an application where various agents, often organized hierarchically, collaborate or coordinate to achieve a broader objective.\",\\n      \"query\": \"What is the fundamental structure of a multi-agent system in ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Structuring your application this way offers significant advantages, including enhanced modularity, specialization, reusability, maintainability, and the ability to define structured control flows using dedicated workflow agents.\"\\n      ],\\n      \"expected_answer\": \"Structuring an application as a multi-agent system offers enhanced modularity, specialization, reusability, maintainability, and the ability to define structured control flows using dedicated workflow agents.\",\\n      \"query\": \"What are the key benefits of adopting a multi-agent system architecture?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"The foundation for structuring multi-agent systems is the parent-child relationship defined in `BaseAgent`.\"\\n      ],\\n      \"expected_answer\": \"The parent-child relationship defined in `BaseAgent` forms the foundation for structuring multi-agent systems.\",\\n      \"query\": \"What is the foundational concept for structuring multi-agent systems in ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"You create a tree structure by passing a list of agent instances to the `sub_agents` argument when initializing a parent agent. ADK automatically sets the `parent_agent` attribute on each child agent during initialization (`google.adk.agents.base_agent.py` - `model_post_init`).\"\\n      ],\\n      \"expected_answer\": \"A tree structure is created by passing a list of agent instances to the `sub_agents` argument during parent agent initialization, and ADK automatically sets the `parent_agent` attribute on each child.\",\\n      \"query\": \"How is a tree structure created in ADK\\'s agent hierarchy?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"An agent instance can only be added as a sub-agent once. Attempting to assign a second parent will result in a `ValueError`.\"\\n      ],\\n      \"expected_answer\": \"An agent instance can only be a sub-agent to one parent; assigning a second parent will cause a `ValueError`.\",\\n      \"query\": \"What happens if an agent instance is assigned to multiple parents?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"This hierarchy defines the scope for [Workflow Agents](#22-workflow-agents-as-orchestrators) and influences the potential targets for LLM-Driven Delegation. You can navigate the hierarchy using `agent.parent_agent` or find descendants using `agent.find_agent(name)`.\"\\n      ],\\n      \"expected_answer\": \"The agent hierarchy defines the scope for Workflow Agents and influences LLM-Driven Delegation targets. It can be navigated using `agent.parent_agent` or by finding descendants with `agent.find_agent(name)`.\",\\n      \"query\": \"How can one navigate the agent hierarchy in ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"ADK includes specialized agents derived from `BaseAgent` that don\\'t perform tasks themselves but orchestrate the execution flow of their `sub_agents`.\"\\n      ],\\n      \"expected_answer\": \"ADK\\'s specialized agents, derived from `BaseAgent`, orchestrate the execution flow of their `sub_agents` instead of performing tasks directly.\",\\n      \"query\": \"What is the primary function of specialized agents in ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Executes its `sub_agents` one after another in the order they are listed.\"\\n      ],\\n      \"expected_answer\": \"A `SequentialAgent` executes its `sub_agents` sequentially in the order they are listed.\",\\n      \"query\": \"How does a `SequentialAgent` execute its sub-agents?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Passes the *same* [`InvocationContext`](../runtime/index.md) sequentially, allowing agents to easily pass results via shared state.\"\\n      ],\\n      \"expected_answer\": \"A `SequentialAgent` passes the same `InvocationContext` sequentially, enabling agents to share results through a shared state.\",\\n      \"query\": \"How does `SequentialAgent` manage `InvocationContext` for its sub-agents?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Executes its `sub_agents` in parallel. Events from sub-agents may be interleaved.\"\\n      ],\\n      \"expected_answer\": \"A `ParallelAgent` executes its `sub_agents` concurrently, and events from these sub-agents may be interleaved.\",\\n      \"query\": \"How does a `ParallelAgent` execute its sub-agents?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Modifies the `InvocationContext.branch` for each child agent (e.g., `ParentBranch.ChildName`), providing a distinct contextual path which can be useful for isolating history in some memory implementations.\"\\n      ],\\n      \"expected_answer\": \"A `ParallelAgent` modifies the `InvocationContext.branch` for each child agent, creating a distinct contextual path useful for isolating history in memory implementations.\",\\n      \"query\": \"What is the purpose of modifying `InvocationContext.branch` in a `ParallelAgent`?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Despite different branches, all parallel children access the *same shared* `session.state`, enabling them to read initial state and write results (use distinct keys to avoid race conditions).\"\\n      ],\\n      \"expected_answer\": \"All parallel children in a `ParallelAgent` access the same shared `session.state`, allowing them to read initial state and write results. However, distinct keys should be used to prevent race conditions.\",\\n      \"query\": \"How do parallel children agents manage shared state, and what precaution should be taken?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Executes its `sub_agents` sequentially in a loop.\"\\n      ],\\n      \"expected_answer\": \"A `LoopAgent` executes its `sub_agents` sequentially within a loop.\",\\n      \"query\": \"What is the execution pattern of a `LoopAgent`?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"The loop stops if the optional `max_iterations` is reached, or if any sub-agent yields an [`Event`](../events/index.md) with `actions.escalate=True`.\"\\n      ],\\n      \"expected_answer\": \"A `LoopAgent` terminates if it reaches its `max_iterations` or if any sub-agent yields an `Event` with `actions.escalate=True`.\",\\n      \"query\": \"What are the conditions for a `LoopAgent` to terminate?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Passes the *same* `InvocationContext` in each iteration, allowing state changes (e.g., counters, flags) to persist across loops.\"\\n      ],\\n      \"expected_answer\": \"A `LoopAgent` passes the same `InvocationContext` in each iteration, ensuring that state changes, such as counters or flags, persist across loops.\",\\n      \"query\": \"How does `LoopAgent` ensure state changes persist across iterations?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"The `instruction` parameter is arguably the most critical for shaping an `LlmAgent`\\'s behavior. It\\'s a string (or a function returning a string) that tells the agent:\",\\n        \"* Its core task or goal.\",\\n        \"* Its personality or persona (e.g., \\\\\"You are a helpful assistant,\\\\\" \\\\\"You are a witty pirate\\\\\").\",\\n        \"* Constraints on its behavior (e.g., \\\\\"Only answer questions about X,\\\\\" \\\\\"Never reveal Y\\\\\").\",\\n        \"* How and when to use its `tools`. You should explain the purpose of each tool and the circumstances under which it should be called, supplementing any descriptions within the tool itself.\",\\n        \"* The desired format for its output (e.g., \\\\\"Respond in JSON,\\\\\" \\\\\"Provide a bulleted list\\\\\").\"\\n      ],\\n      \"expected_answer\": \"The `instruction` parameter defines an `LlmAgent`\\'s core task, personality, behavioral constraints, tool usage, and desired output format.\",\\n      \"query\": \"What are the key elements defined by the `instruction` parameter for an `LlmAgent`?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"**Tips for Effective Instructions:**\",\\n        \"* **Be Clear and Specific:** Avoid ambiguity. Clearly state the desired actions and outcomes.\",\\n        \"* **Use Markdown:** Improve readability for complex instructions using headings, lists, etc.\",\\n        \"* **Provide Examples (Few-Shot):** For complex tasks or specific output formats, include examples directly in the instruction.\",\\n        \"* **Guide Tool Use:** Don\\'t just list tools; explain *when* and *why* the agent should use them.\"\\n      ],\\n      \"expected_answer\": \"Effective instructions should be clear and specific, use Markdown for readability, include few-shot examples for complex tasks, and guide tool use by explaining when and why tools should be utilized.\",\\n      \"query\": \"What are the best practices for writing `LlmAgent` instructions?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"The instruction is a string template, you can use the `{var}` syntax to insert dynamic values into the instruction.\",\\n        \"* `{var}` is used to insert the value of the state variable named var.\",\\n        \"* `{artifact.var}` is used to insert the text content of the artifact named var.\",\\n        \"* If the state variable or artifact does not exist, the agent will raise an error. If you want to ignore the error, you can append a `?` to the variable name as in `{var?}`.\"\\n      ],\\n      \"expected_answer\": \"Dynamic values can be inserted into `LlmAgent` instructions using `{var}` for state variables and `{artifact.var}` for artifact content. Adding `?` (e.g., `{var?}`) prevents errors if the variable or artifact is missing.\",\\n      \"query\": \"How can dynamic data be incorporated into `LlmAgent` instructions?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Tools give your `LlmAgent` capabilities beyond the LLM\\'s built-in knowledge or reasoning. They allow the agent to interact with the outside world, perform calculations, fetch real-time data, or execute specific actions.\"\\n      ],\\n      \"expected_answer\": \"Tools provide `LlmAgent` with capabilities to interact with external systems, perform calculations, retrieve real-time data, and execute specific actions, extending beyond the LLM\\'s inherent knowledge.\",\\n      \"query\": \"What is the primary function of tools in an `LlmAgent`?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"* **`tools` (Optional):** Provide a list of tools the agent can use. Each item in the list can be:\",\\n        \"    * A Python function (automatically wrapped as a `FunctionTool`).\",\\n        \"    * An instance of a class inheriting from `BaseTool`.\",\\n        \"    * An instance of another agent (`AgentTool`, enabling agent-to-agent delegation - see [Multi-Agents](multi-agents.md)).\"\\n      ],\\n      \"expected_answer\": \"The `tools` parameter accepts Python functions (wrapped as `FunctionTool`), instances of `BaseTool` subclasses, or instances of other agents (`AgentTool`) for delegation.\",\\n      \"query\": \"What are the acceptable types for items in the `tools` list of an `LlmAgent`?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"The LLM uses the function/tool names, descriptions (from docstrings or the `description` field), and parameter schemas to decide which tool to call based on the conversation and its instructions.\"\\n      ],\\n      \"expected_answer\": \"The LLM determines which tool to call by evaluating the tool\\'s name, description, and parameter schemas in conjunction with the conversation context and its own instructions.\",\\n      \"query\": \"What factors does the LLM consider when deciding which tool to invoke?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"* **`generate_content_config` (Optional):** Pass an instance of `google.genai.types.GenerateContentConfig` to control parameters like `temperature` (randomness), `max_output_tokens` (response length), `top_p`, `top_k`, and safety settings.\"\\n      ],\\n      \"expected_answer\": \"The `generate_content_config` parameter, an instance of `google.genai.types.GenerateContentConfig`, allows control over LLM generation attributes such as `temperature`, `max_output_tokens`, `top_p`, `top_k`, and safety settings.\",\\n      \"query\": \"Which configuration object is used to control LLM generation parameters?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"* **`input_schema` (Optional):** Define a Pydantic `BaseModel` class representing the expected input structure. If set, the user message content passed to this agent *must* be a JSON string conforming to this schema. Your instructions should guide the user or preceding agent accordingly.\"\\n      ],\\n      \"expected_answer\": \"The `input_schema` parameter defines a Pydantic `BaseModel` for the expected input structure, requiring user message content to be a JSON string conforming to it. Instructions should guide users or preceding agents to adhere to this schema.\",\\n      \"query\": \"What is the role of `input_schema` in ensuring structured input for an `LlmAgent`?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"* **`output_schema` (Optional):** Define a Pydantic `BaseModel` class representing the desired output structure. If set, the agent\\'s final response *must* be a JSON string conforming to this schema.\",\\n        \"    * **Constraint:** Using `output_schema` enables controlled generation within the LLM but **disables the agent\\'s ability to use tools or transfer control to other agents**. Your instructions must guide the LLM to produce JSON matching the schema directly.\"\\n      ],\\n      \"expected_answer\": \"The `output_schema` parameter defines a Pydantic `BaseModel` for the desired output JSON structure. While it enables controlled LLM generation, it disables the agent\\'s ability to use tools or transfer control, requiring the LLM to directly produce JSON matching the schema.\",\\n      \"query\": \"What are the implications of using `output_schema` for an `LlmAgent`?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"* **`output_key` (Optional):** Provide a string key. If set, the text content of the agent\\'s *final* response will be automatically saved to the session\\'s state dictionary under this key (e.g., `session.state[output_key] = agent_response_text`). This is useful for passing results between agents or steps in a workflow.\"\\n      ],\\n      \"expected_answer\": \"The `output_key` parameter saves the `LlmAgent`\\'s final response text to the session\\'s state dictionary under the specified key, facilitating result sharing between agents or workflow steps.\",\\n      \"query\": \"How does `output_key` contribute to inter-agent communication?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"* **`include_contents` (Optional, Default: `\\'default\\'`):** Determines if the `contents` (history) are sent to the LLM.\",\\n        \"    * `\\'default\\'`: The agent receives the relevant conversation history.\",\\n        \"    * `\\'none\\'`: The agent receives no prior `contents`. It operates based solely on its current instruction and any input provided in the *current* turn (useful for stateless tasks or enforcing specific contexts).\"\\n      ],\\n      \"expected_answer\": \"The `include_contents` parameter controls whether conversation history is provided to the LLM. `\\'default\\'` includes relevant history, while `\\'none\\'` ensures the agent operates solely on current input and instructions, useful for stateless operations.\",\\n      \"query\": \"When would you set `include_contents` to `\\'none\\'` for an `LlmAgent`?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"* **`planner` (Optional):** Assign a `BasePlanner` instance to enable multi-step reasoning and planning before execution. (See [Multi-Agents](multi-agents.md) patterns).\",\\n        \"* **`code_executor` (Optional):** Provide a `BaseCodeExecutor` instance to allow the agent to execute code blocks (e.g., Python) found in the LLM\\'s response. ([See Tools/Built-in tools](../tools/built-in-tools.md)).\"\\n      ],\\n      \"expected_answer\": \"An `LlmAgent` can use a `BasePlanner` instance for multi-step reasoning and planning, and a `BaseCodeExecutor` instance to execute code blocks from the LLM\\'s response.\",\\n      \"query\": \"What are the roles of `planner` and `code_executor` in enhancing `LlmAgent` capabilities?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"The `google-genai` library, used internally by ADK for Gemini, can connect through either Google AI Studio or Vertex AI.\"\\n      ],\\n      \"expected_answer\": \"The `google-genai` library, which ADK uses for Gemini, supports connections via Google AI Studio or Vertex AI.\",\\n      \"query\": \"What are the two primary connection methods for `google-genai` with Gemini in ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"In order to use voice/video streaming in ADK, you will need to use Gemini models that support the Live API.\"\\n      ],\\n      \"expected_answer\": \"To enable voice/video streaming in ADK, you must use Gemini models that support the Live API.\",\\n      \"query\": \"What specific Gemini model feature is required for voice/video streaming in ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Google AI Studio is the easiest way to get started with Gemini. All you need is the [API key](https://aistudio.google.com/app/apikey). Best for rapid prototyping and development.\"\\n      ],\\n      \"expected_answer\": \"Google AI Studio is the simplest way to begin with Gemini, requiring only an API key, and is ideal for rapid prototyping and development.\",\\n      \"query\": \"What makes Google AI Studio suitable for rapid prototyping with Gemini?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Vertex AI is recommended for production applications, leveraging Google Cloud infrastructure. Gemini on Vertex AI supports enterprise-grade features, security, and compliance controls.\"\\n      ],\\n      \"expected_answer\": \"Vertex AI is recommended for production applications because it offers enterprise-grade features, security, and compliance controls, leveraging Google Cloud infrastructure.\",\\n      \"query\": \"Why is Vertex AI the preferred choice for production applications using Gemini?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"To access a vast range of LLMs from providers like OpenAI, Anthropic (non-Vertex AI), Cohere, and many others, ADK offers integration through the LiteLLM library.\"\\n      ],\\n      \"expected_answer\": \"ADK integrates with a wide array of LLMs from providers such as OpenAI, Anthropic (non-Vertex AI), and Cohere via the LiteLLM library.\",\\n      \"query\": \"Which library facilitates ADK\\'s integration with diverse LLM providers?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"[LiteLLM](https://docs.litellm.ai/) acts as a translation layer, providing a standardized, OpenAI-compatible interface to over 100+ LLMs.\"\\n      ],\\n      \"expected_answer\": \"LiteLLM serves as a translation layer, offering a standardized, OpenAI-compatible interface for over 100 LLMs.\",\\n      \"query\": \"What is the primary function of LiteLLM?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"If your agent is relying on tools, please make sure that you select a model with tool support from [Ollama website](https://ollama.com/search?c=tools).\"\\n      ],\\n      \"expected_answer\": \"For agents relying on tools, ensure the selected Ollama model supports tools, which can be verified on the Ollama website.\",\\n      \"query\": \"How can you verify tool support for an Ollama model?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"It is important to set the provider `ollama_chat` instead of `ollama`. Using `ollama` will result in unexpected behaviors such as infinite tool call loops and ignoring previous context.\"\\n      ],\\n      \"expected_answer\": \"It is crucial to use `ollama_chat` as the provider instead of `ollama` to prevent issues like infinite tool call loops and context loss.\",\\n      \"query\": \"What are the potential issues if `ollama` is used as the provider instead of `ollama_chat`?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"While `api_base` can be provided inside LiteLLM for generation, LiteLLM library is calling other APIs relying on the env variable instead as of v1.65.5 after completion. So at this time, we recommend setting the env variable `OLLAMA_API_BASE` to point to the ollama server.\"\\n      ],\\n      \"expected_answer\": \"Setting the `OLLAMA_API_BASE` environment variable to the Ollama server is recommended because the LiteLLM library, as of v1.65.5, uses this variable for API calls after completion, even if `api_base` is provided internally.\",\\n      \"query\": \"Why is `OLLAMA_API_BASE` environment variable preferred over `api_base` within LiteLLM for Ollama integration?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Alternatively, `openai` can be used as the provider name. But this will also require setting the `OPENAI_API_BASE=http://localhost:11434/v1` and `OPENAI_API_KEY=anything` env variables instead of `OLLAMA_API_BASE`. **Please note that api base now has `/v1` at the end.**\"\\n      ],\\n      \"expected_answer\": \"Using `openai` as the provider name for Ollama models requires setting `OPENAI_API_BASE` to `http://localhost:11434/v1` and `OPENAI_API_KEY` to any value, replacing `OLLAMA_API_BASE`.\",\\n      \"query\": \"What specific changes are needed to use the `openai` provider name for Ollama models?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Tools such as [vLLM](https://github.com/vllm-project/vllm) allow you to host models efficiently and often expose an OpenAI-compatible API endpoint.\"\\n      ],\\n      \"expected_answer\": \"vLLM enables efficient model hosting and typically provides an OpenAI-compatible API endpoint.\",\\n      \"query\": \"What is the primary advantage of using vLLM for model hosting?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"* *Important for ADK Tools:* When deploying, ensure the serving tool supports and enables OpenAI-compatible tool/function calling. For vLLM, this might involve flags like `--enable-auto-tool-choice` and potentially a specific `--tool-call-parser`, depending on the model.\"\\n      ],\\n      \"expected_answer\": \"When deploying models with vLLM for ADK Tools, it\\'s crucial to ensure the serving tool supports OpenAI-compatible tool/function calling, potentially requiring specific flags like `--enable-auto-tool-choice` and a `--tool-call-parser`.\",\\n      \"query\": \"What specific configurations are important for vLLM deployments to support ADK Tools?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"For enterprise-grade scalability, reliability, and integration with Google Cloud\\'s MLOps ecosystem, you can use models deployed to Vertex AI Endpoints. This includes models from Model Garden or your own fine-tuned models.\"\\n      ],\\n      \"expected_answer\": \"Models deployed to Vertex AI Endpoints, including those from Model Garden or fine-tuned models, offer enterprise-grade scalability, reliability, and integration with Google Cloud\\'s MLOps ecosystem.\",\\n      \"query\": \"What are the benefits of deploying models to Vertex AI Endpoints?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Some providers, like Anthropic, make their models available directly through Vertex AI.\"\\n      ],\\n      \"expected_answer\": \"Yes, some providers, such as Anthropic, offer their models directly through Vertex AI.\",\\n      \"query\": \"Are there third-party LLM providers whose models are directly accessible via Vertex AI?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"ADK\\'s registry automatically recognizes `gemini-*` strings and standard Vertex AI endpoint strings (`projects/.../endpoints/...`) and routes them via the `google-genai` library. For other model types used directly via Vertex AI (like Claude), you must explicitly tell the ADK registry which specific wrapper class (`Claude` in this case) knows how to handle that model identifier string with the Vertex AI backend.\"\\n      ],\\n      \"expected_answer\": \"ADK\\'s registry automatically handles `gemini-*` and standard Vertex AI endpoint strings via `google-genai`. However, for other Vertex AI model types (e.g., Claude), explicit registration of the corresponding wrapper class is required to inform the ADK registry how to handle that model identifier.\",\\n      \"query\": \"Explain the difference in model registration for Gemini vs. other Vertex AI models in ADK.\"\\n    },\\n    {\\n      \"citations\": [\\n        \"The Agent Development Kit (ADK) supports building sophisticated applications by composing multiple, distinct `BaseAgent` instances into a **Multi-Agent System (MAS)**.\"\\n      ],\\n      \"expected_answer\": \"ADK supports building sophisticated applications by composing multiple, distinct `BaseAgent` instances into a Multi-Agent System (MAS).\",\\n      \"query\": \"What is the core capability of ADK for complex application development?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"In ADK, a multi-agent system is an application where different agents, often forming a hierarchy, collaborate or coordinate to achieve a larger goal.\"\\n      ],\\n      \"expected_answer\": \"A multi-agent system in ADK is an application where various agents, often organized hierarchically, collaborate or coordinate to achieve a common goal.\",\\n      \"query\": \"Describe the nature of a multi-agent system within ADK.\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Structuring your application this way offers significant advantages, including enhanced modularity, specialization, reusability, maintainability, and the ability to define structured control flows using dedicated workflow agents.\"\\n      ],\\n      \"expected_answer\": \"Structuring an application as a multi-agent system provides benefits such as enhanced modularity, specialization, reusability, maintainability, and the ability to define structured control flows using workflow agents.\",\\n      \"query\": \"What are the primary advantages of using a multi-agent system architecture in ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"The foundation for structuring multi-agent systems is the parent-child relationship defined in `BaseAgent`.\"\\n      ],\\n      \"expected_answer\": \"The parent-child relationship, defined in `BaseAgent`, serves as the foundation for structuring multi-agent systems.\",\\n      \"query\": \"What is the fundamental principle for organizing multi-agent systems in ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"You create a tree structure by passing a list of agent instances to the `sub_agents` argument when initializing a parent agent. ADK automatically sets the `parent_agent` attribute on each child agent during initialization (`google.adk.agents.base_agent.py` - `model_post_init`).\"\\n      ],\\n      \"expected_answer\": \"A tree structure is formed by providing a list of agent instances to the `sub_agents` argument during parent agent initialization, and ADK automatically assigns the `parent_agent` attribute to each child.\",\\n      \"query\": \"How is the hierarchical structure of agents established in ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"An agent instance can only be added as a sub-agent once. Attempting to assign a second parent will result in a `ValueError`.\"\\n      ],\\n      \"expected_answer\": \"An agent instance can only be a sub-agent to a single parent; attempting to assign it to another parent will raise a `ValueError`.\",\\n      \"query\": \"What is the constraint on an agent instance having multiple parents?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"This hierarchy defines the scope for [Workflow Agents](#22-workflow-agents-as-orchestrators) and influences the potential targets for LLM-Driven Delegation. You can navigate the hierarchy using `agent.parent_agent` or find descendants using `agent.find_agent(name)`.\"\\n      ],\\n      \"expected_answer\": \"The agent hierarchy defines the scope for Workflow Agents and influences LLM-Driven Delegation targets. Navigation is possible via `agent.parent_agent` or by finding descendants using `agent.find_agent(name)`.\",\\n      \"query\": \"What are the practical implications of the agent hierarchy in ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"ADK includes specialized agents derived from `BaseAgent` that don\\'t perform tasks themselves but orchestrate the execution flow of their `sub_agents`.\"\\n      ],\\n      \"expected_answer\": \"ADK provides specialized agents, derived from `BaseAgent`, that orchestrate the execution flow of their `sub_agents` rather than performing tasks directly.\",\\n      \"query\": \"What is the primary characteristic of specialized agents in ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Executes its `sub_agents` one after another in the order they are listed.\"\\n      ],\\n      \"expected_answer\": \"A `SequentialAgent` executes its `sub_agents` sequentially, following the order in which they are listed.\",\\n      \"query\": \"Describe the execution order of sub-agents in a `SequentialAgent`.\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Passes the *same* [`InvocationContext`](../runtime/index.md) sequentially, allowing agents to easily pass results via shared state.\"\\n      ],\\n      \"expected_answer\": \"A `SequentialAgent` passes the same `InvocationContext` to its sub-agents sequentially, enabling them to share results through a common state.\",\\n      \"query\": \"How does `SequentialAgent` facilitate data sharing among its sub-agents?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Executes its `sub_agents` in parallel. Events from sub-agents may be interleaved.\"\\n      ],\\n      \"expected_answer\": \"A `ParallelAgent` executes its `sub_agents` concurrently, and events generated by these sub-agents may occur in an interleaved fashion.\",\\n      \"query\": \"What is the concurrency behavior of a `ParallelAgent`?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Modifies the `InvocationContext.branch` for each child agent (e.g., `ParentBranch.ChildName`), providing a distinct contextual path which can be useful for isolating history in some memory implementations.\"\\n      ],\\n      \"expected_answer\": \"The `InvocationContext.branch` is modified for each child agent in a `ParallelAgent` to provide a distinct contextual path, which can help in isolating history within certain memory implementations.\",\\n      \"query\": \"Why is `InvocationContext.branch` modified for child agents in a `ParallelAgent`?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Despite different branches, all parallel children access the *same shared* `session.state`, enabling them to read initial state and write results (use distinct keys to avoid race conditions).\"\\n      ],\\n      \"expected_answer\": \"All parallel children agents share the same `session.state`, allowing them to read initial state and write results. However, it\\'s important to use distinct keys when writing to avoid race conditions.\",\\n      \"query\": \"How do parallel children agents interact with the `session.state`?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Executes its `sub_agents` sequentially in a loop.\"\\n      ],\\n      \"expected_answer\": \"A `LoopAgent` executes its `sub_agents` sequentially within a loop.\",\\n      \"query\": \"Describe the looping mechanism of a `LoopAgent`.\"\\n    },\\n    {\\n      \"citations\": [\\n        \"The loop stops if the optional `max_iterations` is reached, or if any sub-agent yields an [`Event`](../events/index.md) with `actions.escalate=True`.\"\\n      ],\\n      \"expected_answer\": \"A `LoopAgent` terminates either when it reaches the specified `max_iterations` or when any of its sub-agents yields an `Event` with `actions.escalate=True`.\",\\n      \"query\": \"Under what conditions does a `LoopAgent` cease execution?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Passes the *same* `InvocationContext` in each iteration, allowing state changes (e.g., counters, flags) to persist across loops.\"\\n      ],\\n      \"expected_answer\": \"A `LoopAgent` maintains state changes across iterations by passing the same `InvocationContext` in each loop, allowing elements like counters or flags to persist.\",\\n      \"query\": \"How does a `LoopAgent` ensure persistence of state changes?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"The `instruction` parameter is arguably the most critical for shaping an `LlmAgent`\\'s behavior. It\\'s a string (or a function returning a string) that tells the agent:\",\\n        \"* Its core task or goal.\",\\n        \"* Its personality or persona (e.g., \\\\\"You are a helpful assistant,\\\\\" \\\\\"You are a witty pirate\\\\\").\",\\n        \"* Constraints on its behavior (e.g., \\\\\"Only answer questions about X,\\\\\" \\\\\"Never reveal Y\\\\\").\",\\n        \"* How and when to use its `tools`. You should explain the purpose of each tool and the circumstances under which it should be called, supplementing any descriptions within the tool itself.\",\\n        \"* The desired format for its output (e.g., \\\\\"Respond in JSON,\\\\\" \\\\\"Provide a bulleted list\\\\\").\"\\n      ],\\n      \"expected_answer\": \"The `instruction` parameter is crucial for defining an `LlmAgent`\\'s core task, personality, behavioral constraints, tool usage, and desired output format.\",\\n      \"query\": \"What are the key aspects controlled by the `instruction` parameter in an `LlmAgent`?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"**Tips for Effective Instructions:**\",\\n        \"* **Be Clear and Specific:** Avoid ambiguity. Clearly state the desired actions and outcomes.\",\\n        \"* **Use Markdown:** Improve readability for complex instructions using headings, lists, etc.\",\\n        \"* **Provide Examples (Few-Shot):** For complex tasks or specific output formats, include examples directly in the instruction.\",\\n        \"* **Guide Tool Use:** Don\\'t just list tools; explain *when* and *why* the agent should use them.\"\\n      ],\\n      \"expected_answer\": \"To write effective instructions, be clear and specific, use Markdown for readability, provide few-shot examples for complex tasks, and guide tool use by explaining when and why tools should be utilized.\",\\n      \"query\": \"What are the recommended practices for crafting effective `LlmAgent` instructions?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"The instruction is a string template, you can use the `{var}` syntax to insert dynamic values into the instruction.\",\\n        \"* `{var}` is used to insert the value of the state variable named var.\",\\n        \"* `{artifact.var}` is used to insert the text content of the artifact named var.\",\\n        \"* If the state variable or artifact does not exist, the agent will raise an error. If you want to ignore the error, you can append a `?` to the variable name as in `{var?}`.\"\\n      ],\\n      \"expected_answer\": \"Dynamic values can be embedded in `LlmAgent` instructions using `{var}` for state variables and `{artifact.var}` for artifact content. Appending `?` to the variable name (e.g., `{var?}`) allows ignoring errors if the variable or artifact is absent.\",\\n      \"query\": \"How can dynamic content be embedded within `LlmAgent` instructions?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Tools give your `LlmAgent` capabilities beyond the LLM\\'s built-in knowledge or reasoning. They allow the agent to interact with the outside world, perform calculations, fetch real-time data, or execute specific actions.\"\\n      ],\\n      \"expected_answer\": \"Tools extend an `LlmAgent`\\'s capabilities by enabling interaction with external systems, performing calculations, retrieving real-time data, and executing specific actions, thereby going beyond the LLM\\'s inherent knowledge.\",\\n      \"query\": \"What is the primary benefit of providing tools to an `LlmAgent`?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"* **`tools` (Optional):** Provide a list of tools the agent can use. Each item in the list can be:\",\\n        \"    * A Python function (automatically wrapped as a `FunctionTool`).\",\\n        \"    * An instance of a class inheriting from `BaseTool`.\",\\n        \"    * An instance of another agent (`AgentTool`, enabling agent-to-agent delegation - see [Multi-Agents](multi-agents.md)).\"\\n      ],\\n      \"expected_answer\": \"The `tools` parameter accepts a list of Python functions (automatically wrapped as `FunctionTool`), instances of `BaseTool` subclasses, or instances of other agents (`AgentTool`) for agent-to-agent delegation.\",\\n      \"query\": \"What are the permissible types of entries in the `tools` list for an `LlmAgent`?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"The LLM uses the function/tool names, descriptions (from docstrings or the `description` field), and parameter schemas to decide which tool to call based on the conversation and its instructions.\"\\n      ],\\n      \"expected_answer\": \"The LLM determines which tool to invoke by considering the tool\\'s name, description, and parameter schemas, in conjunction with the ongoing conversation and its own instructions.\",\\n      \"query\": \"What criteria does the LLM use to select a tool?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"* **`generate_content_config` (Optional):** Pass an instance of `google.genai.types.GenerateContentConfig` to control parameters like `temperature` (randomness), `max_output_tokens` (response length), `top_p`, `top_k`, and safety settings.\"\\n      ],\\n      \"expected_answer\": \"The `generate_content_config` parameter, an instance of `google.genai.types.GenerateContentConfig`, allows fine-grained control over LLM generation attributes such as `temperature`, `max_output_tokens`, `top_p`, `top_k`, and safety settings.\",\\n      \"query\": \"Which configuration object is used to fine-tune LLM generation parameters?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"* **`input_schema` (Optional):** Define a Pydantic `BaseModel` class representing the expected input structure. If set, the user message content passed to this agent *must* be a JSON string conforming to this schema. Your instructions should guide the user or preceding agent accordingly.\"\\n      ],\\n      \"expected_answer\": \"The `input_schema` parameter defines a Pydantic `BaseModel` for the expected input structure. If set, the user message content must be a JSON string adhering to this schema, and instructions should guide users or preceding agents accordingly.\",\\n      \"query\": \"What is the role of `input_schema` in enforcing structured input for an `LlmAgent`?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"* **`output_schema` (Optional):** Define a Pydantic `BaseModel` class representing the desired output structure. If set, the agent\\'s final response *must* be a JSON string conforming to this schema.\",\\n        \"    * **Constraint:** Using `output_schema` enables controlled generation within the LLM but **disables the agent\\'s ability to use tools or transfer control to other agents**. Your instructions must guide the LLM to produce JSON matching the schema directly.\"\\n      ],\\n      \"expected_answer\": \"The `output_schema` parameter defines a Pydantic `BaseModel` for the desired output JSON structure. While it enables controlled LLM generation, it restricts the agent from using tools or transferring control, requiring the LLM to directly produce JSON that matches the schema.\",\\n      \"query\": \"What are the limitations imposed by using `output_schema` on an `LlmAgent`?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"* **`output_key` (Optional):** Provide a string key. If set, the text content of the agent\\'s *final* response will be automatically saved to the session\\'s state dictionary under this key (e.g., `session.state[output_key] = agent_response_text`). This is useful for passing results between agents or steps in a workflow.\"\\n      ],\\n      \"expected_answer\": \"The `output_key` parameter automatically saves the `LlmAgent`\\'s final response text to the session\\'s state dictionary under the specified key, facilitating the transfer of results between agents or workflow steps.\",\\n      \"query\": \"How does `output_key` enable result sharing in workflows?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"* **`include_contents` (Optional, Default: `\\'default\\'`):** Determines if the `contents` (history) are sent to the LLM.\",\\n        \"    * `\\'default\\'`: The agent receives the relevant conversation history.\",\\n        \"    * `\\'none\\'`: The agent receives no prior `contents`. It operates based solely on its current instruction and any input provided in the *current* turn (useful for stateless tasks or enforcing specific contexts).\"\\n      ],\\n      \"expected_answer\": \"The `include_contents` parameter controls whether conversation history is sent to the LLM. `\\'default\\'` includes relevant history, while `\\'none\\'` makes the agent operate solely on current input and instructions, suitable for stateless tasks.\",\\n      \"query\": \"What is the impact of `include_contents=\\'none\\'` on an `LlmAgent`\\'s operation?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"* **`planner` (Optional):** Assign a `BasePlanner` instance to enable multi-step reasoning and planning before execution. (See [Multi-Agents](multi-agents.md) patterns).\",\\n        \"* **`code_executor` (Optional):** Provide a `BaseCodeExecutor` instance to allow the agent to execute code blocks (e.g., Python) found in the LLM\\'s response. ([See Tools/Built-in tools](../tools/built-in-tools.md)).\"\\n      ],\\n      \"expected_answer\": \"An `LlmAgent` can be configured with a `BasePlanner` instance for multi-step reasoning and planning, and a `BaseCodeExecutor` instance to enable execution of code blocks from the LLM\\'s response.\",\\n      \"query\": \"What are the functionalities provided by `planner` and `code_executor` in an `LlmAgent`?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"The `google-genai` library, used internally by ADK for Gemini, can connect through either Google AI Studio or Vertex AI.\"\\n      ],\\n      \"expected_answer\": \"The `google-genai` library, used by ADK for Gemini, supports connections via Google AI Studio or Vertex AI.\",\\n      \"query\": \"What are the two connection options for the `google-genai` library when integrating Gemini with ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"In order to use voice/video streaming in ADK, you will need to use Gemini models that support the Live API.\"\\n      ],\\n      \"expected_answer\": \"To use voice/video streaming in ADK, Gemini models must support the Live API.\",\\n      \"query\": \"What is a prerequisite for enabling voice/video streaming with Gemini models in ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Google AI Studio is the easiest way to get started with Gemini. All you need is the [API key](https://aistudio.google.com/app/apikey). Best for rapid prototyping and development.\"\\n      ],\\n      \"expected_answer\": \"Google AI Studio is the simplest way to begin with Gemini, requiring only an API key, and is ideal for rapid prototyping and development.\",\\n      \"query\": \"What makes Google AI Studio a good choice for rapid prototyping with Gemini?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Vertex AI is recommended for production applications, leveraging Google Cloud infrastructure. Gemini on Vertex AI supports enterprise-grade features, security, and compliance controls.\"\\n      ],\\n      \"expected_answer\": \"Vertex AI is recommended for production applications because it offers enterprise-grade features, security, and compliance controls, leveraging Google Cloud infrastructure.\",\\n      \"query\": \"Why is Vertex AI the recommended platform for production applications using Gemini?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"To access a vast range of LLMs from providers like OpenAI, Anthropic (non-Vertex AI), Cohere, and many others, ADK offers integration through the LiteLLM library.\"\\n      ],\\n      \"expected_answer\": \"ADK integrates with a wide range of LLMs from providers such as OpenAI, Anthropic (non-Vertex AI), and Cohere via the LiteLLM library.\",\\n      \"query\": \"Which library does ADK use to integrate with various LLM providers?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"[LiteLLM](https://docs.litellm.ai/) acts as a translation layer, providing a standardized, OpenAI-compatible interface to over 100+ LLMs.\"\\n      ],\\n      \"expected_answer\": \"LiteLLM serves as a translation layer, offering a standardized, OpenAI-compatible interface to over 100 LLMs.\",\\n      \"query\": \"What is the primary role of LiteLLM in ADK\\'s model integration?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"If your agent is relying on tools, please make sure that you select a model with tool support from [Ollama website](https://ollama.com/search?c=tools).\"\\n      ],\\n      \"expected_answer\": \"For agents relying on tools, ensure the selected Ollama model has tool support, which can be verified on the Ollama website.\",\\n      \"query\": \"What is a crucial requirement for Ollama models when used with ADK agents that utilize tools?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"It is important to set the provider `ollama_chat` instead of `ollama`. Using `ollama` will result in unexpected behaviors such as infinite tool call loops and ignoring previous context.\"\\n      ],\\n      \"expected_answer\": \"It is important to use `ollama_chat` as the provider instead of `ollama` to prevent issues like infinite tool call loops and context loss.\",\\n      \"query\": \"What are the consequences of using `ollama` instead of `ollama_chat` as the provider?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"While `api_base` can be provided inside LiteLLM for generation, LiteLLM library is calling other APIs relying on the env variable instead as of v1.65.5 after completion. So at this time, we recommend setting the env variable `OLLAMA_API_BASE` to point to the ollama server.\"\\n      ],\\n      \"expected_answer\": \"Setting the `OLLAMA_API_BASE` environment variable to the Ollama server is recommended because the LiteLLM library, as of v1.65.5, relies on this variable for API calls after completion, even if `api_base` is provided internally.\",\\n      \"query\": \"Why is it advised to set the `OLLAMA_API_BASE` environment variable for Ollama integration?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Alternatively, `openai` can be used as the provider name. But this will also require setting the `OPENAI_API_BASE=http://localhost:11434/v1` and `OPENAI_API_KEY=anything` env variables instead of `OLLAMA_API_BASE`. **Please note that api base now has `/v1` at the end.**\"\\n      ],\\n      \"expected_answer\": \"Using `openai` as the provider name for Ollama models requires setting `OPENAI_API_BASE` to `http://localhost:11434/v1` and `OPENAI_API_KEY` to any value, replacing `OLLAMA_API_BASE`.\",\\n      \"query\": \"What environment variables are necessary when using `openai` as the provider for Ollama models?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Tools such as [vLLM](https://github.com/vllm-project/vllm) allow you to host models efficiently and often expose an OpenAI-compatible API endpoint.\"\\n      ],\\n      \"expected_answer\": \"vLLM enables efficient model hosting and typically provides an OpenAI-compatible API endpoint.\",\\n      \"query\": \"What is the main advantage of using vLLM for model hosting?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"* *Important for ADK Tools:* When deploying, ensure the serving tool supports and enables OpenAI-compatible tool/function calling. For vLLM, this might involve flags like `--enable-auto-tool-choice` and potentially a specific `--tool-call-parser`, depending on the model.\"\\n      ],\\n      \"expected_answer\": \"When deploying models with vLLM for ADK Tools, it\\'s crucial to ensure the serving tool supports OpenAI-compatible tool/function calling, potentially requiring specific flags like `--enable-auto-tool-choice` and a `--tool-call-parser`.\",\\n      \"query\": \"What are the critical considerations for vLLM deployments to ensure compatibility with ADK Tools?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"For enterprise-grade scalability, reliability, and integration with Google Cloud\\'s MLOps ecosystem, you can use models deployed to Vertex AI Endpoints. This includes models from Model Garden or your own fine-tuned models.\"\\n      ],\\n      \"expected_answer\": \"Models deployed to Vertex AI Endpoints, including those from Model Garden or fine-tuned models, offer enterprise-grade scalability, reliability, and integration with Google Cloud\\'s MLOps ecosystem.\",\\n      \"query\": \"What are the key benefits of using Vertex AI Endpoints for model deployment?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Some providers, like Anthropic, make their models available directly through Vertex AI.\"\\n      ],\\n      \"expected_answer\": \"Yes, some providers, such as Anthropic, offer their models directly through Vertex AI.\",\\n      \"query\": \"Are there any third-party LLM providers whose models are directly available through Vertex AI?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"ADK\\'s registry automatically recognizes `gemini-*` strings and standard Vertex AI endpoint strings (`projects/.../endpoints/...`) and routes them via the `google-genai` library. For other model types used directly via Vertex AI (like Claude), you must explicitly tell the ADK registry which specific wrapper class (`Claude` in this case) knows how to handle that model identifier string with the Vertex AI backend.\"\\n      ],\\n      \"expected_answer\": \"ADK\\'s registry automatically handles `gemini-*` and standard Vertex AI endpoint strings via `google-genai`. However, for other Vertex AI model types (e.g., Claude), explicit registration of the corresponding wrapper class is required to inform the ADK registry how to handle that model identifier.\",\\n      \"query\": \"What is the process for registering non-Gemini Vertex AI models with ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"The Agent Development Kit (ADK) supports building sophisticated applications by composing multiple, distinct `BaseAgent` instances into a **Multi-Agent System (MAS)**.\"\\n      ],\\n      \"expected_answer\": \"ADK supports building sophisticated applications by composing multiple, distinct `BaseAgent` instances into a Multi-Agent System (MAS).\",\\n      \"query\": \"What is the primary architectural approach ADK supports for complex applications?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"In ADK, a multi-agent system is an application where different agents, often forming a hierarchy, collaborate or coordinate to achieve a larger goal.\"\\n      ],\\n      \"expected_answer\": \"A multi-agent system in ADK is an application where various agents, often organized hierarchically, collaborate or coordinate to achieve a common goal.\",\\n      \"query\": \"How are agents organized and what is their purpose in a multi-agent system?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Structuring your application this way offers significant advantages, including enhanced modularity, specialization, reusability, maintainability, and the ability to define structured control flows using dedicated workflow agents.\"\\n      ],\\n      \"expected_answer\": \"Structuring an application as a multi-agent system offers enhanced modularity, specialization, reusability, maintainability, and the ability to define structured control flows using workflow agents.\",\\n      \"query\": \"What are the main advantages of using a multi-agent system architecture in ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"The foundation for structuring multi-agent systems is the parent-child relationship defined in `BaseAgent`.\"\\n      ],\\n      \"expected_answer\": \"The parent-child relationship defined in `BaseAgent` forms the foundation for structuring multi-agent systems.\",\\n      \"query\": \"What is the foundational concept for organizing multi-agent systems in ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"You create a tree structure by passing a list of agent instances to the `sub_agents` argument when initializing a parent agent. ADK automatically sets the `parent_agent` attribute on each child agent during initialization (`google.adk.agents.base_agent.py` - `model_post_init`).\"\\n      ],\\n      \"expected_answer\": \"A tree structure is created by providing a list of agent instances to the `sub_agents` argument during parent agent initialization, and ADK automatically assigns the `parent_agent` attribute to each child.\",\\n      \"query\": \"How is the hierarchical structure of agents established in ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"An agent instance can only be added as a sub-agent once. Attempting to assign a second parent will result in a `ValueError`.\"\\n      ],\\n      \"expected_answer\": \"An agent instance can only be a sub-agent to a single parent; attempting to assign it to another parent will raise a `ValueError`.\",\\n      \"query\": \"What is the rule regarding an agent instance having multiple parents?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"This hierarchy defines the scope for [Workflow Agents](#22-workflow-agents-as-orchestrators) and influences the potential targets for LLM-Driven Delegation. You can navigate the hierarchy using `agent.parent_agent` or find descendants using `agent.find_agent(name)`.\"\\n      ],\\n      \"expected_answer\": \"The agent hierarchy defines the scope for Workflow Agents and influences LLM-Driven Delegation targets. It can be navigated using `agent.parent_agent` or by finding descendants with `agent.find_agent(name)`.\",\\n      \"query\": \"What are the practical implications of the agent hierarchy in ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"ADK includes specialized agents derived from `BaseAgent` that don\\'t perform tasks themselves but orchestrate the execution flow of their `sub_agents`.\"\\n      ],\\n      \"expected_answer\": \"ADK\\'s specialized agents, derived from `BaseAgent`, orchestrate the execution flow of their `sub_agents` rather than performing tasks directly.\",\\n      \"query\": \"What is the primary characteristic of specialized agents in ADK?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Executes its `sub_agents` one after another in the order they are listed.\"\\n      ],\\n      \"expected_answer\": \"A `SequentialAgent` executes its `sub_agents` sequentially, following the order in which they are listed.\",\\n      \"query\": \"Describe the execution order of sub-agents in a `SequentialAgent`.\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Passes the *same* [`InvocationContext`](../runtime/index.md) sequentially, allowing agents to easily pass results via shared state.\"\\n      ],\\n      \"expected_answer\": \"A `SequentialAgent` passes the same `InvocationContext` to its sub-agents sequentially, enabling them to share results through a common state.\",\\n      \"query\": \"How does `SequentialAgent` facilitate data sharing among its sub-agents?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Executes its `sub_agents` in parallel. Events from sub-agents may be interleaved.\"\\n      ],\\n      \"expected_answer\": \"A `ParallelAgent` executes its `sub_agents` concurrently, and events generated by these sub-agents may occur in an interleaved fashion.\",\\n      \"query\": \"What is the concurrency behavior of a `ParallelAgent`?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Modifies the `InvocationContext.branch` for each child agent (e.g., `ParentBranch.ChildName`), providing a distinct contextual path which can be useful for isolating history in some memory implementations.\"\\n      ],\\n      \"expected_answer\": \"The `InvocationContext.branch` is modified for each child agent in a `ParallelAgent` to provide a distinct contextual path, which can help in isolating history within certain memory implementations.\",\\n      \"query\": \"Why is `InvocationContext.branch` modified for child agents in a `ParallelAgent`?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Despite different branches, all parallel children access the *same shared* `session.state`, enabling them to read initial state and write results (use distinct keys to avoid race conditions).\"\\n      ],\\n      \"expected_answer\": \"All parallel children agents share the same `session.state`, allowing them to read initial state and write results. However, it\\'s important to use distinct keys when writing to avoid race conditions.\",\\n      \"query\": \"How do parallel children agents interact with the `session.state`?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Executes its `sub_agents` sequentially in a loop.\"\\n      ],\\n      \"expected_answer\": \"A `LoopAgent` executes its `sub_agents` sequentially within a loop.\",\\n      \"query\": \"Describe the looping mechanism of a `LoopAgent`.\"\\n    },\\n    {\\n      \"citations\": [\\n        \"The loop stops if the optional `max_iterations` is reached, or if any sub-agent yields an [`Event`](../events/index.md) with `actions.escalate=True`.\"\\n      ],\\n      \"expected_answer\": \"A `LoopAgent` terminates either when it reaches the specified `max_iterations` or when any of its sub-agents yields an `Event` with `actions.escalate=True`.\",\\n      \"query\": \"Under what conditions does a `LoopAgent` cease execution?\"\\n    },\\n    {\\n      \"citations\": [\\n        \"Passes the *same* `InvocationContext` in each iteration, allowing state changes (e.g., counters, flags) to persist across loops.\"\\n      ],\\n      \"expected_answer\": \"A `LoopAgent` maintains state changes across iterations by passing the same `InvocationContext` in each loop, allowing elements like counters or flags to persist.\",\\n      \"query\": \"How does a `LoopAgent` ensure persistence of state changes?\"\\n    }\\n  ]\\n}'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "class TestCase(BaseModel):\n",
    "    query: str\n",
    "    expected_answer: str\n",
    "    citations: List[str]\n",
    "\n",
    "class OutputSchema(BaseModel):\n",
    "    test_cases: List[TestCase]\n",
    "\n",
    "# Load document context\n",
    "with open(\"../texts/adk-docs.txt\", \"r\") as f:\n",
    "    document_context = f.read()\n",
    "\n",
    "# Prepare system prompt\n",
    "system_prompt = \"\"\"\n",
    "You are a world-class assistant specializing in generating comprehensive test cases for Retrieval-Augmented Generation (RAG) systems.\n",
    "\n",
    "Given the full context of a document, your task is to create a diverse set of at least 100 queries to rigorously evaluate the RAG system. Each query should target specific facts, details, or concepts from the context, and once specific queries are exhausted, include broader or inferential questions that still relate to the content.\n",
    "\n",
    "For each test case, provide:\n",
    "- `query`: A clear, concise question that could be asked of the RAG system.\n",
    "- `expected_answer`: The precise answer that should be returned, based strictly on the context.\n",
    "- `citations`: A list of references (quotes) from the context that support the answer.\n",
    "\n",
    "Ensure queries cover a wide range of topics, including factual recall, reasoning, chronology, character analysis, and thematic understanding. Avoid duplication and strive for variety in question types.\n",
    "\n",
    "Return the output as a JSON object with a list of test cases in the following format:\n",
    "{{\n",
    "    \"test_cases\": [\n",
    "        {{\n",
    "            \"query\": \"...\",\n",
    "            \"expected_answer\": \"...\",\n",
    "            \"citations\": [\"...\"]\n",
    "        }},\n",
    "        ...\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "# Set up Gemini LLM with LangChain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0,\n",
    "    thinking_budget=0,\n",
    "    response_mime_type=\"application/json\",\n",
    "    response_schema=OutputSchema.schema()\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\"],\n",
    "    template=system_prompt\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Generate the test set\n",
    "result = chain.run(context=document_context)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1379d64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'citations': ['# Code owners file.', 'This file controls who is tagged for review for any given pull request.', '*                                      @GoogleCloudPlatform/teams/generative-ai-devrel'], 'expected_answer': 'The code owner for all files is the GitHub team `@GoogleCloudPlatform/teams/generative-ai-devrel`.', 'query': 'Who is the code owner for this repository?'}, {'citations': ['name: Bug report', 'about: Create a report to help us improve', \"title: ''\", \"labels: ''\", \"assignees: ''\", '**Describe the bug**', 'A clear and concise description of what the bug is.', '**To Reproduce**', 'Steps to reproduce the behavior:', \"1. Go to '...'\", \"2. Click on '....'\", '3. See error', '**Expected behavior**', 'A clear and concise description of what you expected to happen.', '**Screenshots**', 'If applicable, add screenshots to help explain your problem.', '**Versions**', ' - OS: [e.g. Windows, Mac, Linux]', ' - ADK version:', ' - Python version:', '**Additional context**', 'Add any other context about the problem here.'], 'expected_answer': 'The bug report template includes sections for describing the bug, steps to reproduce, expected behavior, screenshots, versions (OS, ADK, Python), and additional context.', 'query': 'What information should be included in a bug report?'}, {'citations': ['name: Feature request', 'about: Suggest an idea for this project', \"title: ''\", \"labels: ''\", \"assignees: ''\", '**Is your feature request related to a problem? Please describe.**', 'A clear and concise description of what the problem is.', \"**Describe the solution you'd like**\", 'A clear and concise description of what you want to happen.', \"**Describe alternatives you've considered**\", \"A clear and concise description of any alternative solutions or features you've considered.\", '**Additional context**', 'Add any other context or screenshots about the feature request here.'], 'expected_answer': \"A feature request should describe the problem it's related to, the desired solution, any alternative solutions considered, and additional context or screenshots.\", 'query': 'What are the key sections of a feature request?'}, {'citations': ['allowedCopyrightHolders:', '  - \"Google LLC\"', 'allowedLicenses:', '  - \"Apache-2.0\"', 'ignoreFiles:', '  - \"**/requirements*.txt\"', '  - \"**/__init__.py\"', '  - \"**/constraints*.txt\"', 'ignoreLicenseYear: true', 'sourceFileExtensions:', '  - \"py\"'], 'expected_answer': 'The header checker lint configuration allows \"Google LLC\" as a copyright holder and \"Apache-2.0\" as a license. It ignores license year and processes files with the \".py\" extension. Files matching \"**/requirements*.txt\", \"**/__init__.py\", and \"**/constraints*.txt\" are ignored.', 'query': 'What are the configurations for the header checker lint?'}, {'citations': ['  \"extends\": [', '    \"config:recommended\"', '  ],', '  \"prConcurrentLimit\": 0,', '  \"rebaseWhen\": \"never\",', '  \"dependencyDashboard\": true'], 'expected_answer': 'The Renovate configuration extends `config:recommended`, sets `prConcurrentLimit` to 0, `rebaseWhen` to \"never\", and enables `dependencyDashboard`.', 'query': 'What are the general settings in the Renovate configuration file?'}, {'citations': ['  \"pip_requirements\": {', '    \"fileMatch\": [', '      \"requirements.txt\",', '      \"requirements-test.txt\",', '      \"requirements-composer.txt\",', '      \"constraints.txt\",', '      \"constraints-test.txt\"', '    ]', '  }'], 'expected_answer': 'For pip requirements, Renovate matches files named \"requirements.txt\", \"requirements-test.txt\", \"requirements-composer.txt\", \"constraints.txt\", and \"constraints-test.txt\".', 'query': 'Which files are matched for pip requirements in Renovate?'}, {'citations': ['  \"ignorePaths\": [', '    \"**/target/**\"', '  ]'], 'expected_answer': 'Renovate is configured to ignore paths under \"**/target/**\".', 'query': 'What paths does Renovate ignore?'}, {'citations': ['    {', '      \"matchDatasources\": [\"maven\"],', '      \"matchFilePatterns\": [\"pom.xml\"],', '      \"groupName\": \"Java Maven Dependencies\"', '    }'], 'expected_answer': 'Maven dependencies matching `pom.xml` files are grouped under \"Java Maven Dependencies\".', 'query': 'How are Maven dependencies grouped in Renovate?'}, {'citations': ['    {', '      \"matchDatasources\": [\"pypi\"],', '      \"matchFilePatterns\": [\"requirements.txt\"],', '      \"groupName\": \"Python pip Dependencies\"', '    }'], 'expected_answer': 'PyPI dependencies matching `requirements.txt` files are grouped under \"Python pip Dependencies\".', 'query': 'How are Python pip dependencies grouped in Renovate?'}, {'citations': ['    {', '      \"separateMinorPatch\": true,', '      \"matchPackageNames\": [', '        \"/pytest/\"', '      ]', '    }'], 'expected_answer': 'For packages matching \"/pytest/\", minor and patch updates are separated.', 'query': 'What is the rule for pytest package updates in Renovate?'}, {'citations': ['    {', '      \"matchUpdateTypes\": [', '        \"minor\"', '      ],', '      \"extends\": [', '        \"schedule:monthly\"', '      ]', '    }'], 'expected_answer': 'Minor updates are scheduled monthly.', 'query': 'What is the schedule for minor updates in Renovate?'}, {'citations': ['    {', '      \"matchUpdateTypes\": [', '        \"patch\"', '      ],', '      \"extends\": [', '        \"schedule:quarterly\"', '      ]', '    }'], 'expected_answer': 'Patch updates are scheduled quarterly.', 'query': 'What is the schedule for patch updates in Renovate?'}, {'citations': ['    {', '      \"matchDatasources\": [\"maven\"],', '      \"matchUpdateTypes\": [\"minor\"],', '      \"groupName\": \"Java Minor Updates\",', '      \"extends\": [\"schedule:monthly\"]', '    }'], 'expected_answer': 'Maven minor updates are grouped as \"Java Minor Updates\" and scheduled monthly.', 'query': 'How are Java minor updates handled by Renovate?'}, {'citations': ['    {', '      \"matchDatasources\": [\"maven\"],', '      \"matchUpdateTypes\": [\"patch\"],', '      \"groupName\": \"Java Patch Updates\",', '      \"extends\": [\"schedule:quarterly\"]', '    }'], 'expected_answer': 'Maven patch updates are grouped as \"Java Patch Updates\" and scheduled quarterly.', 'query': 'How are Java patch updates handled by Renovate?'}, {'citations': ['  \"vulnerabilityAlerts\": {', '    \"schedule\": [', '      \"at any time\"', '    ]', '  }'], 'expected_answer': 'Vulnerability alerts are scheduled to run \"at any time\".', 'query': 'When are vulnerability alerts scheduled to run according to Renovate config?'}, {'citations': ['  \"platformAutomerge\": true,', '  \"automergeType\": \"branch\"'], 'expected_answer': 'Platform automerge is enabled, and the automerge type is \"branch\".', 'query': 'What are the automerge settings in Renovate?'}, {'citations': ['name: Publish Docs', 'on:', '  push:', '    branches:', '      - main', 'permissions:', '  contents: write'], 'expected_answer': 'The \"Publish Docs\" workflow is triggered on pushes to the `main` branch and has `write` permissions for `contents`.', 'query': \"What triggers the 'Publish Docs' workflow and what permissions does it have?\"}, {'citations': ['jobs:', '  deploy:', '    runs-on: ubuntu-latest', '    steps:', '      - uses: actions/checkout@v4', '      - name: Configure Git Credentials', '        run: |', '          git config user.name github-actions[bot]', '          git config user.email 41898282+github-actions[bot]@users.noreply.github.com', '      - uses: actions/setup-python@v5', '        with:', '          python-version: 3.x', '      - run: echo \"cache_id=$(date --utc \\'+%V\\')\" >> $GITHUB_ENV', '      - uses: actions/cache@v4', '        with:', '          key: mkdocs-material-${{ env.cache_id }}', '          path: .cache', '          restore-keys: |', '            mkdocs-material-', '      - run: pip install -r requirements.txt', '      - run: mkdocs gh-deploy --force'], 'expected_answer': \"The 'deploy' job in the 'Publish Docs' workflow runs on `ubuntu-latest`. It checks out the repository, configures Git credentials for `github-actions[bot]`, sets up Python 3.x, caches `mkdocs-material`, installs dependencies from `requirements.txt`, and then deploys the documentation using `mkdocs gh-deploy --force`.\", 'query': \"Describe the steps involved in the 'deploy' job of the 'Publish Docs' workflow.\"}, {'citations': ['name: Python Lint', 'on:', '  push:', '    branches:', '      - main', '    paths:', '      - \"samples/python/**\"', '      - \".github/workflows/python-lint.yaml\"', '  pull_request:', '    types:', '      - opened', '      - reopened', '      - synchronize', '    paths:', '      - \"samples/python/**\"', '      - \".github/workflows/python-lint.yaml\"', '  schedule:', '    - cron: \"0 0 * * 0\"'], 'expected_answer': 'The \"Python Lint\" workflow is triggered on pushes to the `main` branch affecting `samples/python/**` or `.github/workflows/python-lint.yaml`, on pull request `opened`, `reopened`, or `synchronize` events affecting the same paths, and on a weekly schedule at midnight UTC on Sundays (`cron: \"0 0 * * 0\"`).', 'query': \"What are the triggers for the 'Python Lint' workflow?\"}, {'citations': ['jobs:', '  get_sample_dirs:', '    runs-on: ubuntu-latest', '    outputs:', '      sample_dirs: ${{ steps.get_dirs.outputs.sample_dirs }}', '    steps:', '      - uses: actions/checkout@v4', '        with:', '          ref: ${{ github.event.pull_request.head.sha }}', '      - name: Get Sample Directories', '        id: get_dirs', '        run: |', '          SAMPLE_DIRS=$(find samples/python/ -type d -maxdepth 5 ! -path \"samples/python\" | jq -R -s \\'split(\"\\\\n\")[:-1]\\' )', '          echo \"sample_dirs=$SAMPLE_DIRS\" >> $GITHUB_OUTPUT', '          echo \"SAMPLE_DIRS: $SAMPLE_DIRS\"  # For debugging'], 'expected_answer': 'The `get_sample_dirs` job in the Python Lint workflow runs on `ubuntu-latest`. It checks out the repository and then uses `find` and `jq` to identify Python sample directories up to a depth of 5, excluding the root `samples/python` directory. The output `sample_dirs` is then made available for other jobs.', 'query': 'Explain the purpose and steps of the `get_sample_dirs` job in the Python Lint workflow.'}, {'citations': ['  lint:', '    runs-on: ubuntu-latest', '    strategy:', '      matrix:', '        python-version: [\"3.9\", \"3.10\", \"3.11\", \"3.12\"]', '        sample_dir: ${{ fromJson(needs.get_sample_dirs.outputs.sample_dirs) }}', '      fail-fast: false', '    needs:', '      - get_sample_dirs', '    steps:', '      - uses: actions/checkout@v4', '        with:', '          ref: ${{ github.event.pull_request.head.sha }}', '      - name: Set up Python ${{ matrix.python-version }}', '        uses: actions/setup-python@v4', '        with:', '          python-version: ${{ matrix.python-version }}', '          cache: \"pip\"', '      - name: Install dependencies', '        run: |', '          python -m pip install --upgrade pip', '          pip install black flake8', '      - name: Run Black and Flake8 Linting in ${{ matrix.sample_dir }}', '        run: |', '          cd ${{ matrix.sample_dir }}', '          black .', '          flake8 .'], 'expected_answer': 'The `lint` job in the Python Lint workflow runs on `ubuntu-latest` with a matrix strategy for Python versions 3.9, 3.10, 3.11, 3.12, and each sample directory. It depends on `get_sample_dirs` and does not fail fast. Steps include checking out the code, setting up Python with pip cache, installing `black` and `flake8`, and then running `black` and `flake8` in each sample directory.', 'query': 'Describe the `lint` job in the Python Lint workflow, including its strategy and steps.'}, {'citations': ['name: Python Tests', 'on:', '  push:', '    branches:', '      - main', '    paths:', '      - \"samples/python/**\"', '      - \".github/workflows/python-tests.yaml\"', '  pull_request:', '    types:', '      - opened', '      - reopened', '      - synchronize', '    paths:', '      - \"samples/python/**\"', '      - \".github/workflows/python-tests.yaml\"', '  schedule:', '    - cron: \"0 0 * * 0\"'], 'expected_answer': 'The \"Python Tests\" workflow is triggered on pushes to the `main` branch affecting `samples/python/**` or `.github/workflows/python-tests.yaml`, on pull request `opened`, `reopened`, or `synchronize` events affecting the same paths, and on a weekly schedule at midnight UTC on Sundays (`cron: \"0 0 * * 0\"`).', 'query': \"What events trigger the 'Python Tests' workflow?\"}, {'citations': ['jobs:', '  build:', '    runs-on: ubuntu-latest', '    strategy:', '      matrix:', '        python-version: [\"3.9\", \"3.10\", \"3.11\", \"3.12\"]', '        sample_dir: ${{ fromJson(needs.get_sample_dirs.outputs.sample_dirs) }}', \"      fail-fast: false # Important: Don't stop if one matrix configuration fails\", '    needs:', '      - get_sample_dirs'], 'expected_answer': 'The `build` job in the Python Tests workflow runs on `ubuntu-latest` with a matrix strategy for Python versions 3.9, 3.10, 3.11, 3.12, and each sample directory. It depends on `get_sample_dirs` and is configured not to fail fast.', 'query': \"Describe the `build` job's configuration in the Python Tests workflow.\"}, {'citations': ['      - name: Get Sample Directories', '        id: get_sample_dirs', '        run: |', '          SAMPLE_DIRS=$( find $PYTHON_DIR -type d -not -path \"*/.venv*\" -exec test -e \\'{}\\'/requirements.txt \\\\; -print )', '          echo \"sample_dirs=$SAMPLE_DIRS\" >> $GITHUB_OUTPUT', '          echo \"SAMPLE_DIRS: $SAMPLE_DIRS\"  # For debugging'], 'expected_answer': 'The `get_sample_dirs` step in the Python Tests workflow finds directories under `$PYTHON_DIR` that are not `.venv` directories and contain a `requirements.txt` file. These directories are then outputted as `sample_dirs`.', 'query': 'How does the `get_sample_dirs` step identify sample directories in the Python Tests workflow?'}, {'citations': ['      - name: Install dependencies', '        run: |', '          python -m pip install --upgrade pip', '          pip install pytest coverage', '          pip install -r ${{ matrix.sample_dir }}/requirements.txt', '          pip install -r ${{ matrix.sample_dir }}/requirements-test.txt'], 'expected_answer': \"The 'Install dependencies' step upgrades pip, installs `pytest` and `coverage`, and then installs dependencies from `requirements.txt` and `requirements-test.txt` within the current sample directory.\", 'query': 'What dependencies are installed in the Python Tests workflow?'}, {'citations': ['      - name: Run pytest in ${{ matrix.sample_dir }}', '        run: |', '          cd ${{ matrix.sample_dir }}', '          pytest | tee pytest.txt'], 'expected_answer': \"The 'Run pytest' step navigates to the sample directory and executes `pytest`, piping the output to `pytest.txt`.\", 'query': 'How are pytest tests executed in the Python Tests workflow?'}, {'citations': ['      - name: Show failed tests and overall summary', '        run: |', '          grep -E \"FAILED tests|ERROR tests|[0-9]+ passed,\" pytest.txt', '          failed_count=$(grep -E \"FAILED tests|ERROR tests\" pytest.txt | wc -l | tr -d \\'[:space:]\\')', '          if [[ $failed_count -gt 0 ]]; then', '            echo \"$failed_count failed lines found! Task failed.\"', '            exit 1', '          fi'], 'expected_answer': 'The \\'Show failed tests and overall summary\\' step greps `pytest.txt` for \"FAILED tests\", \"ERROR tests\", or \"[0-9]+ passed,\". It then counts lines with \"FAILED tests\" or \"ERROR tests\" and exits with an error if `failed_count` is greater than 0.', 'query': \"What is the purpose of the 'Show failed tests and overall summary' step in the Python Tests workflow?\"}, {'citations': ['      - name: Upload pytest test results', '        uses: actions/upload-artifact@v3', '        with:', '          name: pytest-results-${{ matrix.python-version }}-${{ matrix.sample_dir }}', '          path: |', '            pytest.txt', '            ./htmlcov/', '          retention-days: 30', '        if: ${{ always() }}'], 'expected_answer': \"The 'Upload pytest test results' step uploads `pytest.txt` and the `./htmlcov/` directory as an artifact named `pytest-results-${{ matrix.python-version }}-${{ matrix.sample_dir }}`. The artifact is retained for 30 days and is uploaded even if previous steps fail.\", 'query': 'How are pytest test results uploaded and retained in the Python Tests workflow?'}, {'citations': ['__pycache__/', '*.py[cod]', '*$py.class', '*.so', '.Python', 'develop-eggs/', 'dist/', 'downloads/', 'eggs/', '.eggs/', 'lib/', 'lib64/', 'parts/', 'sdist/', 'var/', 'wheels/', 'share/python-wheels/', '*.egg-info/', '.installed.cfg', '*.egg', 'MANIFEST'], 'expected_answer': 'The `.gitignore` file includes common Python-related files and directories such as `__pycache__/`, `*.py[cod]`, `*$py.class`, `*.so`, `.Python`, `develop-eggs/`, `dist/`, `downloads/`, `eggs/`, `.eggs/`, `lib/`, `lib64/`, `parts/`, `sdist/`, `var/`, `wheels/`, `share/python-wheels/`, `*.egg-info/`, `.installed.cfg`, `*.egg`, and `MANIFEST`.', 'query': 'What are some of the Python-specific files and directories ignored by Git?'}, {'citations': ['htmlcov/', '.tox/', '.nox/', '.coverage', '.coverage.*', '.cache', 'nosetests.xml', 'coverage.xml', '*.cover', '*.py,cover', '.hypothesis/', '.pytest_cache/', 'cover/'], 'expected_answer': 'The `.gitignore` file ignores unit test and coverage reports including `htmlcov/`, `.tox/`, `.nox/`, `.coverage`, `.coverage.*`, `.cache`, `nosetests.xml`, `coverage.xml`, `*.cover`, `*.py,cover`, `.hypothesis/`, `.pytest_cache/`, and `cover/`.', 'query': 'Which files related to unit tests and coverage reports are ignored by Git?'}, {'citations': ['*.manifest', '*.spec'], 'expected_answer': 'PyInstaller-related files such as `*.manifest` and `*.spec` are ignored.', 'query': 'What PyInstaller files are ignored?'}, {'citations': ['pip-log.txt', 'pip-delete-this-directory.txt'], 'expected_answer': 'Installer logs like `pip-log.txt` and `pip-delete-this-directory.txt` are ignored.', 'query': 'Which installer log files are ignored?'}, {'citations': ['*.mo', '*.pot'], 'expected_answer': 'Translation files with extensions `*.mo` and `*.pot` are ignored.', 'query': 'What translation files are ignored?'}, {'citations': ['*.log', 'local_settings.py', 'db.sqlite3', 'db.sqlite3-journal'], 'expected_answer': 'Django-related files such as `*.log`, `local_settings.py`, `db.sqlite3`, and `db.sqlite3-journal` are ignored.', 'query': 'What Django-specific files are ignored?'}, {'citations': ['instance/', '.webassets-cache'], 'expected_answer': 'Flask-related files and directories like `instance/` and `.webassets-cache` are ignored.', 'query': 'Which Flask-related files are ignored?'}, {'citations': ['.scrapy'], 'expected_answer': 'The `.scrapy` directory is ignored for Scrapy projects.', 'query': 'What Scrapy-specific directory is ignored?'}, {'citations': ['docs/_build/'], 'expected_answer': 'The `docs/_build/` directory is ignored for Sphinx documentation.', 'query': 'What Sphinx documentation directory is ignored?'}, {'citations': ['.pybuilder/', 'target/'], 'expected_answer': 'PyBuilder-related directories like `.pybuilder/` and `target/` are ignored.', 'query': 'What PyBuilder directories are ignored?'}, {'citations': ['.ipynb_checkpoints'], 'expected_answer': 'Jupyter Notebook checkpoints (`.ipynb_checkpoints`) are ignored.', 'query': 'Are Jupyter Notebook checkpoints ignored?'}, {'citations': ['profile_default/', 'ipython_config.py'], 'expected_answer': 'IPython-related files and directories such as `profile_default/` and `ipython_config.py` are ignored.', 'query': 'What IPython files are ignored?'}, {'citations': ['celerybeat-schedule', 'celerybeat.pid'], 'expected_answer': 'Celery-related files like `celerybeat-schedule` and `celerybeat.pid` are ignored.', 'query': 'What Celery files are ignored?'}, {'citations': ['*.sage.py'], 'expected_answer': 'SageMath parsed files with the `*.sage.py` extension are ignored.', 'query': 'What SageMath files are ignored?'}, {'citations': ['.venv', 'env/', 'venv/', 'ENV/', 'env.bak/', 'venv.bak/'], 'expected_answer': 'Various environment directories such as `.venv`, `env/`, `venv/`, `ENV/`, `env.bak/`, and `venv.bak/` are ignored.', 'query': 'Which environment directories are ignored?'}, {'citations': ['.spyderproject', '.spyproject'], 'expected_answer': 'Spyder project settings files like `.spyderproject` and `.spyproject` are ignored.', 'query': 'What Spyder project settings files are ignored?'}, {'citations': ['.ropeproject'], 'expected_answer': 'Rope project settings (`.ropeproject`) are ignored.', 'query': 'Is the Rope project settings file ignored?'}, {'citations': ['.mypy_cache/', '.dmypy.json', 'dmypy.json'], 'expected_answer': 'mypy-related files and directories such as `.mypy_cache/`, `.dmypy.json`, and `dmypy.json` are ignored.', 'query': 'What mypy files are ignored?'}, {'citations': ['.pyre/'], 'expected_answer': 'The `.pyre/` directory for Pyre type checker is ignored.', 'query': 'Is the Pyre type checker directory ignored?'}, {'citations': ['.pytype/'], 'expected_answer': 'The `.pytype/` directory for pytype static type analyzer is ignored.', 'query': 'Is the pytype static type analyzer directory ignored?'}, {'citations': ['cython_debug/'], 'expected_answer': 'Cython debug symbols in `cython_debug/` are ignored.', 'query': 'Are Cython debug symbols ignored?'}, {'citations': ['.DS_Store'], 'expected_answer': 'Mac Finder files like `.DS_Store` are ignored.', 'query': 'Are Mac Finder files ignored?'}, {'citations': ['docs/site'], 'expected_answer': 'The `docs/site` directory for mkdocs documentation is ignored.', 'query': 'What mkdocs documentation directory is ignored?'}, {'citations': ['/site/public', '.hugo_build.lock'], 'expected_answer': 'Hugo-related files and directories like `/site/public` and `.hugo_build.lock` are ignored.', 'query': 'What Hugo files are ignored?'}, {'citations': ['.vscode/settings.json'], 'expected_answer': 'VS Code personal settings in `.vscode/settings.json` are ignored.', 'query': 'Are VS Code personal settings ignored?'}, {'citations': ['Contributions to this project must be accompanied by a', '[Contributor License Agreement](https://cla.developers.google.com/about) (CLA).', 'You (or your employer) retain the copyright to your contribution; this simply', 'gives us permission to use and redistribute your contributions as part of the', 'project.', 'If you or your current employer have already signed the Google CLA (even if it', \"was for a different project), you probably don't need to do it again.\", 'Visit <https://cla.developers.google.com/> to see your current agreements or to', 'sign a new one.'], 'expected_answer': 'Contributions require a Contributor License Agreement (CLA). The contributor or their employer retains copyright, but the CLA grants permission to use and redistribute contributions. If a Google CLA has been signed previously, it may not be necessary to sign again. The CLA can be managed at `https://cla.developers.google.com/`.', 'query': 'What is the requirement for contributions regarding the Contributor License Agreement (CLA)?'}, {'citations': [\"We adhere to [Google's Open Source Community Guidelines](https://opensource.google/conduct/).\", 'Please familiarize yourself with these guidelines to ensure a positive and', 'collaborative environment for everyone.'], 'expected_answer': \"Contributors must adhere to Google's Open Source Community Guidelines, available at `https://opensource.google/conduct/`.\", 'query': 'What community guidelines should contributors review?'}, {'citations': ['Check the [GitHub Issues](https://github.com/google/adk-docs/issues) for bug', 'reports or feature requests. Feel free to pick up an existing issue or open', 'a new one if you have an idea or find a bug.'], 'expected_answer': 'Contributors can find something to work on by checking GitHub Issues for bug reports or feature requests, or by opening a new issue.', 'query': 'How can a contributor find something to work on?'}, {'citations': ['1.  **Clone the repository:**', '    ```shell', '    git clone git@github.com:google/adk-docs.git', '    cd adk-docs', '    ```', '2.  **Create and activate a virtual environment:**', '    ```shell', '    python -m venv venv', '    source venv/bin/activate', '    ```', '3.  **Install dependencies:**', '    ```shell', '    pip install -r requirements.txt', '    ```', '4.  **Run the local development server:**', '    ```shell', '    mkdocs serve', '    ```', '    This command starts a local server, typically at `http://127.0.0.1:8000/`.', '    The site will automatically reload when you save changes to the documentation files.', '    For more details on the site configuration, see the mkdocs.yml file.'], 'expected_answer': 'To set up for development, clone the repository, create and activate a virtual environment, install dependencies using `pip install -r requirements.txt`, and then run the local development server with `mkdocs serve`.', 'query': 'What are the steps for setting up the development environment?'}, {'citations': ['All contributions, including those from project members, undergo a review process.', '1.  **Create a Pull Request:** We use GitHub Pull Requests (PRs) for code review.', \"    Please refer to GitHub Help if you're unfamiliar with PRs.\", '2.  **Review Process:** Project maintainers will review your PR, providing feedback', '    or requesting changes if necessary.', '3.  **Merging:** Once the PR is approved and passes any required checks, it will be', '    merged into the main branch.', 'Consult [GitHub Help](https://help.github.com/articles/about-pull-requests/) for', 'more information on using pull requests. We look forward to your contributions!'], 'expected_answer': 'All contributions go through a review process. Contributors must create a Pull Request (PR) on GitHub. Project maintainers will review the PR and provide feedback. Once approved and all checks pass, the PR will be merged into the main branch.', 'query': 'Describe the code review process for contributions.'}, {'citations': ['Apache License', 'Version 2.0, January 2004', 'http://www.apache.org/licenses/'], 'expected_answer': 'The project is licensed under the Apache License, Version 2.0, dated January 2004.', 'query': 'What is the license of this project?'}, {'citations': ['\"License\" shall mean the terms and conditions for use, reproduction,', 'and distribution as defined by Sections 1 through 9 of this document.'], 'expected_answer': 'The \"License\" refers to the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of the document.', 'query': 'What does \"License\" mean in the Apache License 2.0?'}, {'citations': ['\"Licensor\" shall mean the copyright owner or entity authorized by', 'the copyright owner that is granting the License.'], 'expected_answer': 'The \"Licensor\" is the copyright owner or entity authorized by the copyright owner that is granting the License.', 'query': 'Who is the \"Licensor\" according to the Apache License 2.0?'}, {'citations': ['\"Legal Entity\" shall mean the union of the acting entity and all', 'other entities that control, are controlled by, or are under common', 'control with that entity. For the purposes of this definition,', '\"control\" means (i) the power, direct or indirect, to cause the', 'direction or management of such entity, whether by contract or', 'otherwise, or (ii) ownership of fifty percent (50%) or more of the', 'outstanding shares, or (iii) beneficial ownership of such entity.'], 'expected_answer': 'A \"Legal Entity\" is the acting entity and all other entities under common control with it. \"Control\" means direct or indirect power to manage, or ownership of 50% or more of outstanding shares, or beneficial ownership.', 'query': 'Define \"Legal Entity\" and \"control\" as per the Apache License 2.0.'}, {'citations': ['\"You\" (or \"Your\") shall mean an individual or Legal Entity', 'exercising permissions granted by this License.'], 'expected_answer': '\"You\" or \"Your\" refers to an individual or Legal Entity exercising permissions granted by the License.', 'query': 'Who is referred to as \"You\" or \"Your\" in the Apache License 2.0?'}, {'citations': ['\"Source\" form shall mean the preferred form for making modifications,', 'including but not limited to software source code, documentation', 'source, and configuration files.'], 'expected_answer': '\"Source\" form is the preferred form for modifications, including software source code, documentation source, and configuration files.', 'query': 'What is \"Source\" form in the context of the Apache License 2.0?'}, {'citations': ['\"Object\" form shall mean any form resulting from mechanical', 'transformation or translation of a Source form, including but', 'not limited to compiled object code, generated documentation,', 'and conversions to other media types.'], 'expected_answer': '\"Object\" form is any form resulting from mechanical transformation or translation of a Source form, such as compiled object code, generated documentation, and conversions to other media types.', 'query': 'What is \"Object\" form according to the Apache License 2.0?'}, {'citations': ['\"Work\" shall mean the work of authorship, whether in Source or', 'Object form, made available under the License, as indicated by a', 'copyright notice that is included in or attached to the work', '(an example is provided in the Appendix below).'], 'expected_answer': '\"Work\" refers to the work of authorship, in Source or Object form, made available under the License, as indicated by a copyright notice.', 'query': 'How is \"Work\" defined in the Apache License 2.0?'}, {'citations': ['\"Derivative Works\" shall mean any work, whether in Source or Object', 'form, that is based on (or derived from) the Work and for which the', 'editorial revisions, annotations, elaborations, or other modifications', 'represent, as a whole, an original work of authorship. For the purposes', 'of this License, Derivative Works shall not include works that remain', 'separable from, or merely link (or bind by name) to the interfaces of,', 'the Work and Derivative Works thereof.'], 'expected_answer': '\"Derivative Works\" are works in Source or Object form based on the Work, where modifications constitute an original work of authorship. It excludes works that remain separable from or merely link to the interfaces of the Work.', 'query': 'What constitutes \"Derivative Works\" under the Apache License 2.0?'}, {'citations': ['\"Contribution\" shall mean any work of authorship, including', 'the original version of the Work and any modifications or additions', 'to that Work or Derivative Works thereof, that is intentionally', 'submitted to Licensor for inclusion in the Work by the copyright owner', 'or by an individual or Legal Entity authorized to submit on behalf of', 'the copyright owner. For the purposes of this definition, \"submitted\"', 'means any form of electronic, verbal, or written communication sent', 'to the Licensor or its representatives, including but not limited to', 'communication on electronic mailing lists, source code control systems,', 'and issue tracking systems that are managed by, or on behalf of, the', 'Licensor for the purpose of discussing and improving the Work, but', 'excluding communication that is conspicuously marked or otherwise', 'designated in writing by the copyright owner as \"Not a Contribution.\"'], 'expected_answer': 'A \"Contribution\" is any work of authorship, including original or modified Work, intentionally submitted to the Licensor for inclusion by the copyright owner or authorized entity. \"Submitted\" includes electronic, verbal, or written communication for discussing and improving the Work, but excludes anything explicitly marked \"Not a Contribution.\"', 'query': 'Define \"Contribution\" and \"submitted\" in the Apache License 2.0.'}, {'citations': ['\"Contributor\" shall mean Licensor and any individual or Legal Entity', 'on behalf of whom a Contribution has been received by Licensor and', 'subsequently incorporated within the Work.'], 'expected_answer': 'A \"Contributor\" is the Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received and incorporated into the Work.', 'query': 'Who is a \"Contributor\" according to the Apache License 2.0?'}, {'citations': ['2. Grant of Copyright License. Subject to the terms and conditions of', 'this License, each Contributor hereby grants to You a perpetual,', 'worldwide, non-exclusive, no-charge, royalty-free, irrevocable', 'copyright license to reproduce, prepare Derivative Works of,', 'publicly display, publicly perform, sublicense, and distribute the', 'Work and such Derivative Works in Source or Object form.'], 'expected_answer': 'Each Contributor grants a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and Derivative Works in Source or Object form.', 'query': 'What copyright license is granted by Contributors under the Apache License 2.0?'}, {'citations': ['3. Grant of Patent License. Subject to the terms and conditions of', 'this License, each Contributor hereby grants to You a perpetual,', 'worldwide, non-exclusive, no-charge, royalty-free, irrevocable', '(except as stated in this section) patent license to make, have made,', 'use, offer to sell, sell, import, and otherwise transfer the Work,', 'where such license applies only to those patent claims licensable', 'by such Contributor that are necessarily infringed by their', 'Contribution(s) alone or by combination of their Contribution(s)', 'with the Work to which such Contribution(s) was submitted. If You', 'institute patent litigation against any entity (including a', 'cross-claim or counterclaim in a lawsuit) alleging that the Work', 'or a Contribution incorporated within the Work constitutes direct', 'or contributory patent infringement, then any patent licenses', 'granted to You under this License for that Work shall terminate', 'as of the date such litigation is filed.'], 'expected_answer': 'Each Contributor grants a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable patent license to make, use, sell, import, and transfer the Work. This license applies only to patent claims necessarily infringed by their Contribution(s) alone or combined with the Work. The patent license terminates if the licensee institutes patent litigation alleging infringement by the Work or a Contribution.', 'query': 'What patent license is granted by Contributors under the Apache License 2.0, and when does it terminate?'}, {'citations': ['4. Redistribution. You may reproduce and distribute copies of the', 'Work or Derivative Works thereof in any medium, with or without', 'modifications, and in Source or Object form, provided that You', 'meet the following conditions:', '(a) You must give any other recipients of the Work or', 'Derivative Works a copy of this License; and', '(b) You must cause any modified files to carry prominent notices', 'stating that You changed the files; and', '(c) You must retain, in the Source form of any Derivative Works', 'that You distribute, all copyright, patent, trademark, and', 'attribution notices from the Source form of the Work,', 'excluding those notices that do not pertain to any part of', 'the Derivative Works; and', '(d) If the Work includes a \"NOTICE\" text file as part of its', 'distribution, then any Derivative Works that You distribute must', 'include a readable copy of the attribution notices contained', 'within such NOTICE file, excluding those notices that do not', 'pertain to any part of the Derivative Works, in at least one', 'of the following places: within a NOTICE text file distributed', 'as part of the Derivative Works; within the Source form or', 'documentation, if provided along with the Derivative Works; or,', 'within a display generated by the Derivative Works, if and', 'wherever such third-party notices normally appear. The contents', 'of the NOTICE file are for informational purposes only and', 'do not modify the License. You may add Your own attribution', 'notices within Derivative Works that You distribute, alongside', 'or as an addendum to the NOTICE text from the Work, provided', 'that such additional attribution notices cannot be construed', 'as modifying the License.'], 'expected_answer': 'When redistributing the Work or Derivative Works, you must provide a copy of the License, include prominent notices for modified files, retain all original copyright, patent, trademark, and attribution notices in Source form, and include attribution notices from any \"NOTICE\" file in the distribution, Source form, documentation, or generated display. You may add your own attribution notices without modifying the License.', 'query': 'What are the conditions for redistribution of the Work or Derivative Works under the Apache License 2.0?'}, {'citations': ['5. Submission of Contributions. Unless You explicitly state otherwise,', 'any Contribution intentionally submitted for inclusion in the Work', 'by You to the Licensor shall be under the terms and conditions of', 'this License, without any additional terms or conditions.', 'Notwithstanding the above, nothing herein shall supersede or modify', 'the terms of any separate license agreement you may have executed', 'with Licensor regarding such Contributions.'], 'expected_answer': 'Unless explicitly stated otherwise, any Contribution submitted for inclusion in the Work is under the terms of the Apache License 2.0, without additional terms. This does not supersede or modify any separate license agreement with the Licensor.', 'query': 'What are the terms for submitting Contributions under the Apache License 2.0?'}, {'citations': ['6. Trademarks. This License does not grant permission to use the trade', 'names, trademarks, service marks, or product names of the Licensor,', 'except as required for reasonable and customary use in describing the', 'origin of the Work and reproducing the content of the NOTICE file.'], 'expected_answer': \"The License does not grant permission to use the Licensor's trade names, trademarks, service marks, or product names, except for describing the Work's origin and reproducing the NOTICE file content.\", 'query': \"Does the Apache License 2.0 grant permission to use Licensor's trademarks?\"}, {'citations': ['7. Disclaimer of Warranty. Unless required by applicable law or', 'agreed to in writing, Licensor provides the Work (and each', 'Contributor provides its Contributions) on an \"AS IS\" BASIS,', 'WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or', 'implied, including, without limitation, any warranties or conditions', 'of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A', 'PARTICULAR PURPOSE. You are solely responsible for determining the', 'appropriateness of using or redistributing the Work and assume any', 'risks associated with Your exercise of permissions under this License.'], 'expected_answer': 'The Work and Contributions are provided \"AS IS,\" without warranties of any kind, including title, non-infringement, merchantability, or fitness for a particular purpose, unless required by law or agreed in writing. Users are responsible for determining the appropriateness of use and assume all associated risks.', 'query': 'What is the disclaimer of warranty in the Apache License 2.0?'}, {'citations': ['8. Limitation of Liability. In no event and under no legal theory,', 'whether in tort (including negligence), contract, or otherwise,', 'unless required by applicable law (such as deliberate and grossly', 'negligent acts) or agreed to in writing, shall any Contributor be', 'liable to You for damages, including any direct, indirect, special,', 'incidental, or consequential damages of any character arising as a', 'result of this License or out of the use or inability to use the', 'Work (including but not limited to damages for loss of goodwill,', 'work stoppage, computer failure or malfunction, or any and all', 'other commercial damages or losses), even if such Contributor', 'has been advised of the possibility of such damages.'], 'expected_answer': 'No Contributor is liable for damages (direct, indirect, special, incidental, or consequential), including loss of goodwill, work stoppage, computer failure, or commercial losses, arising from the License or use/inability to use the Work, unless required by law or agreed in writing, even if advised of such damages.', 'query': 'What is the limitation of liability for Contributors under the Apache License 2.0?'}, {'citations': ['9. Accepting Warranty or Additional Liability. While redistributing', 'the Work or Derivative Works thereof, You may choose to offer,', 'and charge a fee for, acceptance of support, warranty, indemnity,', 'or other liability obligations and/or rights consistent with this', 'License. However, in accepting such obligations, You may act only', 'on Your own behalf and on Your sole responsibility, not on behalf', 'of any other Contributor, and only if You agree to indemnify,', 'defend, and hold each Contributor harmless for any liability', 'incurred by, or claims asserted against, such Contributor by reason', 'of your accepting any such warranty or additional liability.'], 'expected_answer': 'When redistributing, you may offer and charge for support, warranty, or indemnity consistent with the License. However, you act solely on your own behalf and must indemnify, defend, and hold Contributors harmless for any liability or claims arising from your acceptance of such additional obligations.', 'query': 'Can a redistributor accept additional warranty or liability under the Apache License 2.0?'}, {'citations': ['Agent Development Kit (ADK)', 'An open-source, code-first Python toolkit for building, evaluating, and', 'deploying sophisticated AI agents with flexibility and control.'], 'expected_answer': 'ADK stands for Agent Development Kit, which is an open-source, code-first Python toolkit for building, evaluating, and deploying sophisticated AI agents.', 'query': 'What is ADK?'}, {'citations': ['Agent Development Kit (ADK) is a flexible and modular framework for **developing', 'and deploying AI agents**. While optimized for Gemini and the Google ecosystem,', 'ADK is **model-agnostic**, **deployment-agnostic**, and is built for', '**compatibility with other frameworks**.'], 'expected_answer': 'ADK is a flexible and modular framework for developing and deploying AI agents. It is optimized for Gemini and the Google ecosystem but is model-agnostic, deployment-agnostic, and compatible with other frameworks.', 'query': 'What are the key characteristics of the Agent Development Kit (ADK)?'}, {'citations': ['ADK was designed to make agent development feel more like software development, to make it easier for', 'developers to create, deploy, and orchestrate agentic architectures that range', 'from simple tasks to complex workflows.'], 'expected_answer': 'ADK was designed to make agent development feel more like software development, simplifying the creation, deployment, and orchestration of agentic architectures from simple to complex workflows.', 'query': 'What was the design goal of ADK?'}, {'citations': ['**Rich Tool Ecosystem**: Utilize pre-built tools, custom functions,', 'OpenAPI specs, or integrate existing tools to give agents diverse', 'capabilities, all for tight integration with the Google ecosystem.'], 'expected_answer': 'ADK offers a rich tool ecosystem, allowing the use of pre-built tools, custom functions, OpenAPI specs, or existing tools for diverse agent capabilities and tight integration with the Google ecosystem.', 'query': 'What kind of tool ecosystem does ADK provide?'}, {'citations': ['**Code-First Development**: Define agent logic, tools, and orchestration', 'directly in Python for ultimate flexibility, testability, and versioning.'], 'expected_answer': 'ADK supports code-first development, where agent logic, tools, and orchestration are defined directly in Python for flexibility, testability, and versioning.', 'query': \"What does 'Code-First Development' mean in ADK?\"}, {'citations': ['**Modular Multi-Agent Systems**: Design scalable applications by composing', 'multiple specialized agents into flexible hierarchies.'], 'expected_answer': 'ADK enables modular multi-agent systems, allowing scalable applications to be designed by composing multiple specialized agents into flexible hierarchies.', 'query': 'How does ADK support modular multi-agent systems?'}, {'citations': ['**Deploy Anywhere**: Easily containerize and deploy agents on Cloud Run or', 'scale seamlessly with Vertex AI Agent Engine.'], 'expected_answer': 'ADK agents can be easily containerized and deployed on Cloud Run or scaled with Vertex AI Agent Engine.', 'query': 'Where can ADK agents be deployed?'}, {'citations': ['You can install ADK using `pip`:', '```bash', 'pip install google-adk', '```'], 'expected_answer': 'ADK can be installed using `pip` with the command `pip install google-adk`.', 'query': 'How do you install ADK?'}, {'citations': ['Explore the full documentation for detailed guides on building, evaluating, and', 'deploying agents:', '* **[Documentation](https://google.github.io/adk-docs)**'], 'expected_answer': 'The full documentation for ADK is available at `https://google.github.io/adk-docs`.', 'query': 'Where can I find the full documentation for ADK?'}, {'citations': [\"We welcome contributions from the community! Whether it's bug reports, feature\", 'requests, documentation improvements, or code contributions, please see our', '[**Contributing Guidelines**](./CONTRIBUTING.md) to get started.'], 'expected_answer': 'Contributions are welcome, including bug reports, feature requests, documentation improvements, and code contributions. Guidelines are in `CONTRIBUTING.md`.', 'query': 'What types of contributions are welcome for ADK?'}, {'citations': ['This project is licensed under the Apache 2.0 License - see the', '[LICENSE](LICENSE) file for details.'], 'expected_answer': 'The project is licensed under the Apache 2.0 License, with details in the `LICENSE` file.', 'query': 'What is the license for the ADK project?'}, {'citations': ['This feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of the [Service Specific Terms](https://cloud.google.com/terms/service-terms#1). Pre-GA features are available \"as is\" and might have limited support. For more information, see the [launch stage descriptions](https://cloud.google.com/products?hl=en#product-launch-stages).'], 'expected_answer': 'Pre-GA features are subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of the Service Specific Terms. They are provided \"as is\" with potentially limited support. More information is available in the launch stage descriptions.', 'query': 'What are the terms for Pre-GA features?'}, {'citations': ['Building custom agents by directly implementing `_run_async_impl` provides powerful control but is more complex than using the predefined `LlmAgent` or standard `WorkflowAgent` types. We recommend understanding those foundational agent types first before tackling custom orchestration logic.'], 'expected_answer': \"Building custom agents by implementing `_run_async_impl` offers powerful control but is more complex than using `LlmAgent` or `WorkflowAgent` types. It's recommended to understand foundational agent types first.\", 'query': 'What is the recommendation before building custom agents?'}, {'citations': ['A Custom Agent is essentially any class you create that inherits from `google.adk.agents.BaseAgent` and implements its core execution logic within the `_run_async_impl` asynchronous method.'], 'expected_answer': 'A Custom Agent is a class that inherits from `google.adk.agents.BaseAgent` and implements its core execution logic within the `_run_async_impl` asynchronous method.', 'query': 'What defines a Custom Agent in ADK?'}, {'citations': [\"While the standard [Workflow Agents](workflow-agents/index.md) (`SequentialAgent`, `LoopAgent`, `ParallelAgent`) cover common orchestration patterns, you'll need a Custom agent when your requirements include:\", '* **Conditional Logic:** Executing different sub-agents or taking different paths based on runtime conditions or the results of previous steps.', '* **Complex State Management:** Implementing intricate logic for maintaining and updating state throughout the workflow beyond simple sequential passing.', '* **External Integrations:** Incorporating calls to external APIs, databases, or custom Python libraries directly within the orchestration flow control.', '* **Dynamic Agent Selection:** Choosing which sub-agent(s) to run next based on dynamic evaluation of the situation or input.', \"* **Unique Workflow Patterns:** Implementing orchestration logic that doesn't fit the standard sequential, parallel, or loop structures.\"], 'expected_answer': 'Custom agents are needed for conditional logic, complex state management, external integrations, dynamic agent selection, or unique workflow patterns that standard Workflow Agents cannot handle.', 'query': 'When should one use a Custom Agent instead of a Workflow Agent?'}, {'citations': ['The heart of any custom agent is the `_run_async_impl` method.', '* **Signature:** `async def _run_async_impl(self, ctx: InvocationContext) -> AsyncGenerator[Event, None]:`', '* **Asynchronous Generator:** It must be an `async def` function and return an `AsyncGenerator`. This allows it to `yield` events produced by sub-agents or its own logic back to the runner.', '* **`ctx` (InvocationContext):** Provides access to crucial runtime information, most importantly `ctx.session.state`, which is the primary way to share data between steps orchestrated by your custom agent.'], 'expected_answer': 'The `_run_async_impl` method is central to custom agents. It must be an `async def` function returning an `AsyncGenerator` to yield events. The `ctx` (InvocationContext) provides access to runtime information, especially `ctx.session.state` for sharing data.', 'query': 'What is the `_run_async_impl` method in a custom agent?'}, {'citations': ['**Key Capabilities within `_run_async_impl`:**', '1. **Calling Sub-Agents:** You invoke sub-agents (which are typically stored as instance attributes like `self.my_llm_agent`) using their `run_async` method and yield their events:', '    ```python', '    async for event in self.some_sub_agent.run_async(ctx):', '        # Optionally inspect or log the event', '        yield event # Pass the event up', '    ```', '2. **Managing State:** Read from and write to the session state dictionary (`ctx.session.state`) to pass data between sub-agent calls or make decisions:', '    ```python', '    # Read data set by a previous agent', '    previous_result = ctx.session.state.get(\"some_key\")', '    # Make a decision based on state', '    if previous_result == \"some_value\":', '        # ... call a specific sub-agent ...', '    else:', '        # ... call another sub-agent ...', \"    # Store a result for a later step (often done via a sub-agent's output_key)\", '    # ctx.session.state[\"my_custom_result\"] = \"calculated_value\"', '    ```', '3. **Implementing Control Flow:** Use standard Python constructs (`if`/`elif`/`else`, `for`/`while` loops, `try`/`except`) to create sophisticated, conditional, or iterative workflows involving your sub-agents.'], 'expected_answer': 'Within `_run_async_impl`, you can call sub-agents using `run_async` and yield their events, manage state by reading from and writing to `ctx.session.state` to pass data and make decisions, and implement complex control flow using standard Python constructs like `if`/`else` and loops.', 'query': 'What are the key capabilities within the `_run_async_impl` method for custom agents?'}, {'citations': ['Typically, a custom agent orchestrates other agents (like `LlmAgent`, `LoopAgent`, etc.).', \"* **Initialization:** You usually pass instances of these sub-agents into your custom agent's `__init__` method and store them as instance attributes (e.g., `self.story_generator = story_generator_instance`). This makes them accessible within `_run_async_impl`.\", \"* **`sub_agents` List:** When initializing the `BaseAgent` using `super().__init__(...)`, you should pass a `sub_agents` list. This list tells the ADK framework about the agents that are part of this custom agent's immediate hierarchy. It's important for framework features like lifecycle management, introspection, and potentially future routing capabilities, even if your `_run_async_impl` calls the agents directly via `self.xxx_agent`.\"], 'expected_answer': 'Custom agents orchestrate other agents by receiving them in their `__init__` method and storing them as instance attributes. Additionally, a `sub_agents` list should be passed to `super().__init__(...)` to inform the ADK framework about the immediate hierarchy for features like lifecycle management and introspection.', 'query': 'How are sub-agents managed and initialized within a custom agent?'}, {'citations': ['**Goal:** Create a system that generates a story, iteratively refines it through critique and revision, performs final checks, and crucially, *regenerates the story if the final tone check fails*.', '**Why Custom?** The core requirement driving the need for a custom agent here is the **conditional regeneration based on the tone check**. Standard workflow agents don\\'t have built-in conditional branching based on the outcome of a sub-agent\\'s task. We need custom Python logic (`if tone == \"negative\": ...`) within the orchestrator.'], 'expected_answer': 'The `StoryFlowAgent` example uses a custom agent to achieve conditional regeneration of a story based on a tone check. This is necessary because standard workflow agents lack built-in conditional branching based on sub-agent outcomes, requiring custom Python logic for orchestration.', 'query': 'Why is a custom agent used in the `StoryFlowAgent` example?'}, {'citations': ['The initial `story_generator` runs. Its output is expected to be in `ctx.session.state[\"current_story\"]`.', '2. The `loop_agent` runs, which internally calls the `critic` and `reviser` sequentially for `max_iterations` times. They read/write `current_story` and `criticism` from/to the state.', '3. The `sequential_agent` runs, calling `grammar_check` then `tone_check`, reading `current_story` and writing `grammar_suggestions` and `tone_check_result` to the state.', '4. **Custom Part:** The `if` statement checks the `tone_check_result` from the state. If it\\'s \"negative\", the `story_generator` is called *again*, overwriting the `current_story` in the state. Otherwise, the flow ends.'], 'expected_answer': 'The `StoryFlowAgent` first runs `story_generator`. Then, a `loop_agent` iteratively calls `critic` and `reviser`. A `sequential_agent` performs `grammar_check` and `tone_check`. Finally, a custom `if` statement checks `tone_check_result`; if \"negative\", `story_generator` is re-run.', 'query': 'Describe the execution logic of the `StoryFlowAgent`.'}, {'citations': ['In the Agent Development Kit (ADK), an **Agent** is a self-contained execution unit designed to act autonomously to achieve specific goals. Agents can perform tasks, interact with users, utilize external tools, and coordinate with other agents.'], 'expected_answer': 'In ADK, an Agent is a self-contained execution unit that acts autonomously to achieve goals, performing tasks, interacting with users, utilizing external tools, and coordinating with other agents.', 'query': 'What is an Agent in ADK?'}, {'citations': ['The foundation for all agents in ADK is the `BaseAgent` class. It serves as the fundamental blueprint. To create functional agents, you typically extend `BaseAgent` in one of three main ways, catering to different needs – from intelligent reasoning to structured process control.'], 'expected_answer': 'The `BaseAgent` class is the foundation for all agents in ADK, serving as a blueprint that is extended in three main ways to create functional agents for different needs.', 'query': 'What is the foundational class for all agents in ADK?'}, {'citations': ['1. [**LLM Agents (`LlmAgent`, `Agent`)**](llm-agents.md): These agents utilize Large Language Models (LLMs) as their core engine to understand natural language, reason, plan, generate responses, and dynamically decide how to proceed or which tools to use, making them ideal for flexible, language-centric tasks.', '2. [**Workflow Agents (`SequentialAgent`, `ParallelAgent`, `LoopAgent`)**](workflow-agents/index.md): These specialized agents control the execution flow of other agents in predefined, deterministic patterns (sequence, parallel, or loop) without using an LLM for the flow control itself, perfect for structured processes needing predictable execution.', '3. [**Custom Agents**](custom-agents.md): Created by extending `BaseAgent` directly, these agents allow you to implement unique operational logic, specific control flows, or specialized integrations not covered by the standard types, catering to highly tailored application requirements.'], 'expected_answer': 'ADK provides three core agent categories: LLM Agents (`LlmAgent`, `Agent`) for language-centric tasks using LLMs; Workflow Agents (`SequentialAgent`, `ParallelAgent`, `LoopAgent`) for deterministic control flow of other agents; and Custom Agents for unique operational logic or integrations by extending `BaseAgent` directly.', 'query': 'What are the three core agent categories in ADK?'}, {'citations': ['| Feature              | LLM Agent (`LlmAgent`)              | Workflow Agent                              | Custom Agent (`BaseAgent` subclass)      |', '| :------------------- | :---------------------------------- | :------------------------------------------ | :--------------------------------------- |', '| **Primary Function** | Reasoning, Generation, Tool Use     | Controlling Agent Execution Flow            | Implementing Unique Logic/Integrations   |', '| **Core Engine**  | Large Language Model (LLM)          | Predefined Logic (Sequence, Parallel, Loop) | Custom Python Code                       |', '| **Determinism**  | Non-deterministic (Flexible)        | Deterministic (Predictable)                 | Can be either, based on implementation |', '| **Primary Use**  | Language tasks, Dynamic decisions   | Structured processes, Orchestration         | Tailored requirements, Specific workflows|'], 'expected_answer': 'LLM Agents are primarily for reasoning, generation, and tool use, powered by an LLM, non-deterministic, and used for language tasks and dynamic decisions. Workflow Agents control agent execution flow using predefined logic, are deterministic, and are for structured processes and orchestration. Custom Agents implement unique logic/integrations using custom Python code, can be either deterministic or non-deterministic, and are for tailored requirements and specific workflows.', 'query': 'Compare LLM Agents, Workflow Agents, and Custom Agents based on their primary function, core engine, determinism, and primary use.'}, {'citations': ['While each agent type serves a distinct purpose, the true power often comes from combining them. Complex applications frequently employ [multi-agent architectures](multi-agents.md) where:', '* **LLM Agents** handle intelligent, language-based task execution.', '* **Workflow Agents** manage the overall process flow using standard patterns.', '* **Custom Agents** provide specialized capabilities or rules needed for unique integrations.'], 'expected_answer': 'Complex applications often combine different agent types in multi-agent architectures: LLM Agents for intelligent, language-based tasks; Workflow Agents for managing overall process flow; and Custom Agents for specialized capabilities or unique integrations.', 'query': 'How do different agent types work together in multi-agent systems?'}, {'citations': ['The `LlmAgent` (often aliased simply as `Agent`) is a core component in ADK, acting as the \"thinking\" part of your application. It leverages the power of a Large Language Model (LLM) for reasoning, understanding natural language, making decisions, generating responses, and interacting with tools.'], 'expected_answer': 'The `LlmAgent` (or `Agent`) is a core ADK component that acts as the \"thinking\" part of an application, using an LLM for reasoning, natural language understanding, decision-making, response generation, and tool interaction.', 'query': 'What is the primary role of an `LlmAgent` in ADK?'}, {'citations': ['Unlike deterministic [Workflow Agents](workflow-agents/index.md) that follow predefined execution paths, `LlmAgent` behavior is non-deterministic. It uses the LLM to interpret instructions and context, deciding dynamically how to proceed, which tools to use (if any), or whether to transfer control to another agent.'], 'expected_answer': '`LlmAgent` behavior is non-deterministic, unlike Workflow Agents. It dynamically interprets instructions and context using an LLM to decide its actions, tool usage, or agent transfers.', 'query': 'How does `LlmAgent` behavior differ from Workflow Agents?'}, {'citations': [\"* **`name` (Required):** Every agent needs a unique string identifier. This `name` is crucial for internal operations, especially in multi-agent systems where agents need to refer to or delegate tasks to each other. Choose a descriptive name that reflects the agent's function (e.g., `customer_support_router`, `billing_inquiry_agent`). Avoid reserved names like `user`.\", '* **`description` (Optional, Recommended for Multi-Agent):** Provide a concise summary of the agent\\'s capabilities. This description is primarily used by *other* LLM agents to determine if they should route a task to this agent. Make it specific enough to differentiate it from peers (e.g., \"Handles inquiries about current billing statements,\" not just \"Billing agent\").', '* **`model` (Required):** Specify the underlying LLM that will power this agent\\'s reasoning. This is a string identifier like `\"gemini-2.0-flash\"`. The choice of model impacts the agent\\'s capabilities, cost, and performance.'], 'expected_answer': 'To define an `LlmAgent`\\'s identity and purpose, you must provide a unique `name` for internal operations and delegation, an optional but recommended `description` for other LLM agents to understand its capabilities, and a required `model` string identifier (e.g., `\"gemini-2.0-flash\"`) to specify the underlying LLM.', 'query': \"What are the required and recommended parameters for defining an `LlmAgent`'s identity and purpose?\"}, {'citations': [\"The `instruction` parameter is arguably the most critical for shaping an `LlmAgent`'s behavior. It's a string (or a function returning a string) that tells the agent:\", '* Its core task or goal.', '* Its personality or persona (e.g., \"You are a helpful assistant,\" \"You are a witty pirate\").', '* Constraints on its behavior (e.g., \"Only answer questions about X,\" \"Never reveal Y\").', '* How and when to use its `tools`. You should explain the purpose of each tool and the circumstances under which it should be called, supplementing any descriptions within the tool itself.', '* The desired format for its output (e.g., \"Respond in JSON,\" \"Provide a bulleted list\").'], 'expected_answer': \"The `instruction` parameter, a string or function returning a string, is crucial for shaping an `LlmAgent`'s behavior. It defines the agent's core task, personality, behavioral constraints, how and when to use its tools, and the desired output format.\", 'query': 'What is the purpose of the `instruction` parameter for an `LlmAgent`?'}, {'citations': ['**Tips for Effective Instructions:**', '* **Be Clear and Specific:** Avoid ambiguity. Clearly state the desired actions and outcomes.', '* **Use Markdown:** Improve readability for complex instructions using headings, lists, etc.', '* **Provide Examples (Few-Shot):** For complex tasks or specific output formats, include examples directly in the instruction.', \"* **Guide Tool Use:** Don't just list tools; explain *when* and *why* the agent should use them.\"], 'expected_answer': 'For effective instructions, be clear and specific, use Markdown for readability, provide few-shot examples for complex tasks or formats, and guide tool use by explaining when and why tools should be used.', 'query': 'What are some tips for writing effective instructions for an `LlmAgent`?'}, {'citations': ['The instruction is a string template, you can use the `{var}` syntax to insert dynamic values into the instruction.', '* `{var}` is used to insert the value of the state variable named var.', '* `{artifact.var}` is used to insert the text content of the artifact named var.', '* If the state variable or artifact does not exist, the agent will raise an error. If you want to ignore the error, you can append a `?` to the variable name as in `{var?}`.'], 'expected_answer': \"The `instruction` parameter supports string templating: `{var}` inserts a state variable's value, `{artifact.var}` inserts an artifact's text content. An error is raised if the variable/artifact doesn't exist, unless `?` is appended (e.g., `{var?}`) to ignore the error.\", 'query': \"How can dynamic values be inserted into an `LlmAgent`'s instruction?\"}, {'citations': [\"Tools give your `LlmAgent` capabilities beyond the LLM's built-in knowledge or reasoning. They allow the agent to interact with the outside world, perform calculations, fetch real-time data, or execute specific actions.\", '* **`tools` (Optional):** Provide a list of tools the agent can use. Each item in the list can be:', '    * A Python function (automatically wrapped as a `FunctionTool`).', '    * An instance of a class inheriting from `BaseTool`.', '    * An instance of another agent (`AgentTool`, enabling agent-to-agent delegation - see [Multi-Agents](multi-agents.md)).', 'The LLM uses the function/tool names, descriptions (from docstrings or the `description` field), and parameter schemas to decide which tool to call based on the conversation and its instructions.'], 'expected_answer': \"Tools extend an `LlmAgent`'s capabilities beyond the LLM, enabling interaction with the outside world, calculations, data fetching, or specific actions. The `tools` parameter accepts Python functions, `BaseTool` instances, or other agents (`AgentTool`). The LLM uses tool names, descriptions, and parameter schemas to decide tool usage.\", 'query': 'What is the role of `tools` for an `LlmAgent` and what types of tools can be provided?'}, {'citations': ['* **`generate_content_config` (Optional):** Pass an instance of `google.genai.types.GenerateContentConfig` to control parameters like `temperature` (randomness), `max_output_tokens` (response length), `top_p`, `top_k`, and safety settings.'], 'expected_answer': 'The `generate_content_config` parameter (an instance of `google.genai.types.GenerateContentConfig`) allows fine-tuning LLM generation by controlling parameters such as `temperature`, `max_output_tokens`, `top_p`, `top_k`, and safety settings.', 'query': 'How can you fine-tune LLM generation for an `LlmAgent`?'}, {'citations': ['* **`input_schema` (Optional):** Define a Pydantic `BaseModel` class representing the expected input structure. If set, the user message content passed to this agent *must* be a JSON string conforming to this schema. Your instructions should guide the user or preceding agent accordingly.', \"* **`output_schema` (Optional):** Define a Pydantic `BaseModel` class representing the desired output structure. If set, the agent's final response *must* be a JSON string conforming to this schema.\", \"    * **Constraint:** Using `output_schema` enables controlled generation within the LLM but **disables the agent's ability to use tools or transfer control to other agents**. Your instructions must guide the LLM to produce JSON matching the schema directly.\", \"* **`output_key` (Optional):** Provide a string key. If set, the text content of the agent's *final* response will be automatically saved to the session's state dictionary under this key (e.g., `session.state[output_key] = agent_response_text`). This is useful for passing results between agents or steps in a workflow.\"], 'expected_answer': \"`input_schema` defines the expected input JSON structure. `output_schema` defines the desired output JSON structure, but disables tool use and agent transfer. `output_key` saves the agent's final response text to the session's state dictionary under the specified key.\", 'query': 'Explain the use of `input_schema`, `output_schema`, and `output_key` for `LlmAgent`.'}, {'citations': [\"* **`include_contents` (Optional, Default: `'default'`):** Determines if the `contents` (history) are sent to the LLM.\", \"    * `'default'`: The agent receives the relevant conversation history.\", \"    * `'none'`: The agent receives no prior `contents`. It operates based solely on its current instruction and any input provided in the *current* turn (useful for stateless tasks or enforcing specific contexts).\"], 'expected_answer': \"The `include_contents` parameter controls whether conversation history is sent to the LLM. `'default'` sends relevant history, while `'none'` sends no prior contents, making the agent operate solely on current instruction and input, useful for stateless tasks.\", 'query': 'What does the `include_contents` parameter do for an `LlmAgent`?'}, {'citations': ['* **`planner` (Optional):** Assign a `BasePlanner` instance to enable multi-step reasoning and planning before execution. (See [Multi-Agents](multi-agents.md) patterns).', \"* **`code_executor` (Optional):** Provide a `BaseCodeExecutor` instance to allow the agent to execute code blocks (e.g., Python) found in the LLM's response. ([See Tools/Built-in tools](../tools/built-in-tools.md)).\"], 'expected_answer': \"For complex reasoning, an `LlmAgent` can use a `planner` (a `BasePlanner` instance) for multi-step reasoning and planning. A `code_executor` (a `BaseCodeExecutor` instance) allows the agent to execute code blocks from the LLM's response.\", 'query': 'How can an `LlmAgent` be configured for planning and code execution?'}, {'citations': ['The Agent Development Kit (ADK) is designed for flexibility, allowing you to integrate various Large Language Models (LLMs) into your agents. While the setup for Google Gemini models is covered in the [Setup Foundation Models](../get-started/installation.md) guide, this page details how to leverage Gemini effectively and integrate other popular models, including those hosted externally or running locally.'], 'expected_answer': 'ADK is designed for flexibility in integrating various LLMs, including Google Gemini models and other popular models hosted externally or running locally.', 'query': \"What is ADK's approach to integrating Large Language Models (LLMs)?\"}, {'citations': ['ADK primarily uses two mechanisms for model integration:', \"1. **Direct String / Registry:** For models tightly integrated with Google Cloud (like Gemini models accessed via Google AI Studio or Vertex AI) or models hosted on Vertex AI endpoints. You typically provide the model name or endpoint resource string directly to the `LlmAgent`. ADK's internal registry resolves this string to the appropriate backend client, often utilizing the `google-genai` library.\", '2. **Wrapper Classes:** For broader compatibility, especially with models outside the Google ecosystem or those requiring specific client configurations (like models accessed via LiteLLM). You instantiate a specific wrapper class (e.g., `LiteLlm`) and pass this object as the `model` parameter to your `LlmAgent`.'], 'expected_answer': 'ADK integrates models using two primary mechanisms: direct string/registry for Google Cloud-integrated models (like Gemini via Google AI Studio or Vertex AI), where the model name or endpoint string is passed directly to `LlmAgent`; and wrapper classes (e.g., `LiteLlm`) for broader compatibility with external or locally hosted models, where an instantiated wrapper object is passed as the `model` parameter.', 'query': 'What are the two primary mechanisms ADK uses for model integration?'}, {'citations': [\"This is the most direct way to use Google's flagship models within ADK.\", \"**Integration Method:** Pass the model's identifier string directly to the `model` parameter of `LlmAgent` (or its alias, `Agent`).\"], 'expected_answer': \"To use Google Gemini models directly in ADK, pass the model's identifier string to the `model` parameter of `LlmAgent` (or `Agent`).\", 'query': 'How do you integrate Google Gemini models into ADK?'}, {'citations': ['The `google-genai` library, used internally by ADK for Gemini, can connect through either Google AI Studio or Vertex AI.'], 'expected_answer': 'The `google-genai` library, used by ADK for Gemini, can connect via Google AI Studio or Vertex AI.', 'query': 'What are the backend options for `google-genai` when using Gemini models with ADK?'}, {'citations': ['In order to use voice/video streaming in ADK, you will need to use Gemini models that support the Live API. You can find the **model ID(s)** that support the Gemini Live API in the documentation:', '- [Google AI Studio: Gemini Live API](https://ai.google.dev/gemini-api/docs/models#live-api)', '- [Vertex AI: Gemini Live API](https://cloud.google.com/vertex-ai/generative-ai/docs/live-api)'], 'expected_answer': 'To use voice/video streaming in ADK, you need Gemini models that support the Live API. Model IDs can be found in the Google AI Studio or Vertex AI documentation for the Gemini Live API.', 'query': 'What is required for voice/video streaming with Gemini models in ADK?'}, {'citations': ['**Use Case:** Google AI Studio is the easiest way to get started with Gemini. All you need is the [API key](https://aistudio.google.com/app/apikey). Best for rapid prototyping and development.', '**Setup:** Typically requires an API key set as an environment variable:', '```shell', 'export GOOGLE_API_KEY=\"YOUR_GOOGLE_API_KEY\"', 'export GOOGLE_GENAI_USE_VERTEXAI=FALSE', '```', '**Models:** Find all available models on the [Google AI for Developers site](https://ai.google.dev/gemini-api/docs/models).'], 'expected_answer': 'Google AI Studio is ideal for rapid prototyping with Gemini. Setup requires setting `GOOGLE_API_KEY` and `GOOGLE_GENAI_USE_VERTEXAI=FALSE` environment variables. Available models are listed on the Google AI for Developers site.', 'query': 'How do you set up and use Google AI Studio with Gemini models in ADK?'}, {'citations': ['**Use Case:** Recommended for production applications, leveraging Google Cloud infrastructure. Gemini on Vertex AI supports enterprise-grade features, security, and compliance controls.', '**Setup:**', '* Authenticate using Application Default Credentials (ADC):', '    ```shell', '    gcloud auth application-default login', '    ```', '* Set your Google Cloud project and location:', '    ```shell', '    export GOOGLE_CLOUD_PROJECT=\"YOUR_PROJECT_ID\"', '    export GOOGLE_CLOUD_LOCATION=\"YOUR_VERTEX_AI_LOCATION\" # e.g., us-central1', '    ```', '* Explicitly tell the library to use Vertex AI:', '    ```shell', '    export GOOGLE_GENAI_USE_VERTEXAI=TRUE', '    ```', '**Models:** Find available model IDs in the [Vertex AI documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models).'], 'expected_answer': 'Vertex AI is recommended for production applications with Gemini, offering enterprise features. Setup involves authenticating with ADC (`gcloud auth application-default login`), setting `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` environment variables, and explicitly setting `GOOGLE_GENAI_USE_VERTEXAI=TRUE`. Available model IDs are in the Vertex AI documentation.', 'query': 'What is the recommended setup for using Gemini models with Vertex AI in ADK?'}, {'citations': ['To access a vast range of LLMs from providers like OpenAI, Anthropic (non-Vertex AI), Cohere, and many others, ADK offers integration through the LiteLLM library.', '**Integration Method:** Instantiate the `LiteLlm` wrapper class and pass it to the `model` parameter of `LlmAgent`.'], 'expected_answer': 'ADK integrates with various LLMs from providers like OpenAI, Anthropic (non-Vertex AI), and Cohere via the LiteLLM library. To use them, instantiate the `LiteLlm` wrapper class and pass it to the `model` parameter of `LlmAgent`.', 'query': 'How does ADK integrate with cloud and proprietary models like OpenAI or Anthropic (non-Vertex AI)?'}, {'citations': ['[LiteLLM](https://docs.litellm.ai/) acts as a translation layer, providing a standardized, OpenAI-compatible interface to over 100+ LLMs.'], 'expected_answer': 'LiteLLM serves as a translation layer, offering a standardized, OpenAI-compatible interface to over 100+ LLMs.', 'query': 'What is LiteLLM and its primary function?'}, {'citations': ['**Setup:**', '1. **Install LiteLLM:**', '        ```shell', '        pip install litellm', '        ```', '2. **Set Provider API Keys:** Configure API keys as environment variables for the specific providers you intend to use.', '    * *Example for OpenAI:*', '        ```shell', '        export OPENAI_API_KEY=\"YOUR_OPENAI_API_KEY\"', '        ```', '    * *Example for Anthropic (non-Vertex AI):*', '        ```shell', '        export ANTHROPIC_API_KEY=\"YOUR_ANTHROPIC_API_KEY\"', '        ```', '    * *Consult the [LiteLLM Providers Documentation](https://docs.litellm.ai/docs/providers) for the correct environment variable names for other providers.*'], 'expected_answer': 'To set up LiteLLM, first install it using `pip install litellm`. Then, configure API keys for specific providers as environment variables, such as `OPENAI_API_KEY` for OpenAI or `ANTHROPIC_API_KEY` for Anthropic (non-Vertex AI).', 'query': 'What are the setup steps for using LiteLLM with ADK?'}, {'citations': ['For maximum control, cost savings, privacy, or offline use cases, you can run open-source models locally or self-host them and integrate them using LiteLLM.'], 'expected_answer': 'Open-source models can be run locally or self-hosted and integrated via LiteLLM for maximum control, cost savings, privacy, or offline use cases.', 'query': 'Why would someone use open and local models via LiteLLM?'}, {'citations': ['[Ollama](https://ollama.com/) allows you to easily run open-source models locally.'], 'expected_answer': 'Ollama enables easy local execution of open-source models.', 'query': 'What is Ollama used for?'}, {'citations': ['If your agent is relying on tools, please make sure that you select a model with tool support from [Ollama website](https://ollama.com/search?c=tools).', 'For reliable results, we recommend using a decent-sized model with tool support.', 'The tool support for the model can be checked with the following command:', '```bash', 'ollama show mistral-small3.1', '  Model', '    architecture        mistral3', '    parameters          24.0B', '    context length      131072', '    embedding length    5120', '    quantization        Q4_K_M', '  Capabilities', '    completion', '    vision', '    tools', '```', 'You are supposed to see `tools` listed under capabilities.'], 'expected_answer': 'When using Ollama models with agents that rely on tools, select a model with tool support from the Ollama website. Verify tool support by running `ollama show <model_name>` and checking for `tools` under \"Capabilities\". A decent-sized model with tool support is recommended for reliable results.', 'query': 'How do you ensure an Ollama model supports tools for an ADK agent?'}, {'citations': ['You can also look at the template the model is using and tweak it based on your needs.', '```bash', 'ollama show --modelfile llama3.2 > model_file_to_modify', '```', 'For instance, the default template for the above model inherently suggests that the model shall call a function all the time. This may result in an infinite loop of function calls.', '```', 'Given the following functions, please respond with a JSON for a function call', 'with its proper arguments that best answers the given prompt.', 'Respond in the format {\"name\": function name, \"parameters\": dictionary of', 'argument name and its value}. Do not use variables.', '```', 'You can swap such prompts with a more descriptive one to prevent infinite tool call loops.', 'For instance:', '```', \"Review the user's prompt and the available functions listed below.\", 'First, determine if calling one of these functions is the most appropriate way to respond. A function call is likely needed if the prompt asks for a specific action, requires external data lookup, or involves calculations handled by the functions. If the prompt is a general question or can be answered directly, a function call is likely NOT needed.', 'If you determine a function call IS required: Respond ONLY with a JSON object in the format {\"name\": \"function_name\", \"parameters\": {\"argument_name\": \"value\"}}. Ensure parameter values are concrete, not variables.', \"If you determine a function call IS NOT required: Respond directly to the user's prompt in plain text, providing the answer or information requested. Do not output any JSON.\", '```', 'Then you can create a new model with the following command:', '```bash', 'ollama create llama3.2-modified -f model_file_to_modify', '```'], 'expected_answer': \"To prevent infinite tool call loops with Ollama models, you can modify the model's template. First, export the model's modelfile using `ollama show --modelfile <model_name> > model_file_to_modify`. Then, edit the prompt to include conditional logic for tool calls, such as determining if a function call is appropriate or if a direct plain text response is needed. Finally, create a new model with the modified modelfile using `ollama create <new_model_name> -f model_file_to_modify`.\", 'query': 'How can you prevent infinite tool call loops when using Ollama models with ADK agents?'}, {'citations': ['Our LiteLLM wrapper can be used to create agents with Ollama models.', '```py', 'root_agent = Agent(', '    model=LiteLlm(model=\"ollama_chat/mistral-small3.1\"),', '    name=\"dice_agent\",', '    description=(', '        \"hello world agent that can roll a dice of 8 sides and check prime\"', '        \" numbers.\"', '    ),', '    instruction=\"\"\"', '      You roll dice and answer questions about the outcome of the dice rolls.', '    \"\"\",', '    tools=[', '        roll_die,', '        check_prime,', '    ],', ')', '```', '**It is important to set the provider `ollama_chat` instead of `ollama`. Using `ollama` will result in unexpected behaviors such as infinite tool call loops and ignoring previous context.**'], 'expected_answer': 'To create agents with Ollama models using the LiteLLM wrapper, set the `model` parameter to `LiteLlm(model=\"ollama_chat/<model_name>\")`. It is crucial to use `ollama_chat` as the provider instead of `ollama` to avoid issues like infinite tool call loops and ignoring previous context.', 'query': 'How do you configure an ADK agent to use an Ollama model via LiteLLM, and what is an important consideration?'}, {'citations': ['While `api_base` can be provided inside LiteLLM for generation, LiteLLM library is calling other APIs relying on the env variable instead as of v1.65.5 after completion. So at this time, we recommend setting the env variable `OLLAMA_API_BASE` to point to the ollama server.', '```bash', 'export OLLAMA_API_BASE=\"http://localhost:11434\"', 'adk web', '```'], 'expected_answer': 'It is recommended to set the `OLLAMA_API_BASE` environment variable to point to the Ollama server (e.g., `http://localhost:11434`) because the LiteLLM library relies on this environment variable for API calls after completion, as of v1.65.5.', 'query': 'Why is it recommended to set the `OLLAMA_API_BASE` environment variable when using Ollama with ADK?'}, {'citations': ['Alternatively, `openai` can be used as the provider name. But this will also require setting the `OPENAI_API_BASE=http://localhost:11434/v1` and `OPENAI_API_KEY=anything` env variables instead of `OLLAMA_API_BASE`. **Please note that api base now has `/v1` at the end.**', '```py', 'root_agent = Agent(', '    model=LiteLlm(model=\"openai/mistral-small3.1\"),', '    name=\"dice_agent\",', '    description=(', '        \"hello world agent that can roll a dice of 8 sides and check prime\"', '        \" numbers.\"', '    ),', '    instruction=\"\"\"', '      You roll dice and answer questions about the outcome of the dice rolls.', '    \"\"\",', '    tools=[', '        roll_die,', '        check_prime,', '    ],', ')', '```', '```bash', 'export OPENAI_API_BASE=http://localhost:11434/v1', 'export OPENAI_API_KEY=anything', 'adk web', '```'], 'expected_answer': 'Alternatively, you can use `openai` as the provider name for Ollama models with LiteLLM. This requires setting the environment variables `OPENAI_API_BASE` to `http://localhost:11434/v1` (note the `/v1` suffix) and `OPENAI_API_KEY` to any value.', 'query': 'Can Ollama models be used with the `openai` provider name in LiteLLM, and if so, what environment variables are needed?'}, {'citations': ['You can see the request sent to the Ollama server by adding the following in your agent code just after imports.', '```py', 'import litellm', 'litellm._turn_on_debug()', '```'], 'expected_answer': 'To debug requests sent to the Ollama server, add `import litellm` and `litellm._turn_on_debug()` in your agent code after imports.', 'query': 'How can you debug requests sent to the Ollama server from an ADK agent?'}, {'citations': ['Tools such as [vLLM](https://github.com/vllm-project/vllm) allow you to host models efficiently and often expose an OpenAI-compatible API endpoint.'], 'expected_answer': 'Tools like vLLM enable efficient hosting of models and often expose an OpenAI-compatible API endpoint.', 'query': 'What is vLLM used for in the context of model hosting?'}, {'citations': ['**Setup:**', '1. **Deploy Model:** Deploy your chosen model using vLLM (or a similar tool). Note the API base URL (e.g., `https://your-vllm-endpoint.run.app/v1`).', '    * *Important for ADK Tools:* When deploying, ensure the serving tool supports and enables OpenAI-compatible tool/function calling. For vLLM, this might involve flags like `--enable-auto-tool-choice` and potentially a specific `--tool-call-parser`, depending on the model.', '2. **Authentication:** Determine how your endpoint handles authentication (e.g., API key, bearer token).'], 'expected_answer': 'To set up a self-hosted endpoint like vLLM, first deploy your model and note the API base URL. Crucially, ensure the serving tool supports OpenAI-compatible tool/function calling (e.g., with `--enable-auto-tool-choice` for vLLM). Then, determine the authentication method for your endpoint.', 'query': 'What are the setup steps for using a self-hosted endpoint (e.g., vLLM) with ADK?'}, {'citations': [\"For enterprise-grade scalability, reliability, and integration with Google Cloud's MLOps ecosystem, you can use models deployed to Vertex AI Endpoints. This includes models from Model Garden or your own fine-tuned models.\"], 'expected_answer': \"Models deployed to Vertex AI Endpoints, including those from Model Garden or fine-tuned models, can be used for enterprise-grade scalability, reliability, and integration with Google Cloud's MLOps ecosystem.\", 'query': 'What are the benefits of using hosted and tuned models on Vertex AI?'}, {'citations': ['**Vertex AI Setup (Consolidated):**', 'Ensure your environment is configured for Vertex AI:', '1. **Authentication:** Use Application Default Credentials (ADC):', '    ```shell', '    gcloud auth application-default login', '    ```', '2. **Environment Variables:** Set your project and location:', '    ```shell', '    export GOOGLE_CLOUD_PROJECT=\"YOUR_PROJECT_ID\"', '    export GOOGLE_CLOUD_LOCATION=\"YOUR_VERTEX_AI_LOCATION\" # e.g., us-central1', '    ```', '3. **Enable Vertex Backend:** Crucially, ensure the `google-genai` library targets Vertex AI:', '    ```shell', '    export GOOGLE_GENAI_USE_VERTEXAI=TRUE', '    ```'], 'expected_answer': 'To configure your environment for Vertex AI, authenticate using Application Default Credentials (`gcloud auth application-default login`), set `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` environment variables, and enable the Vertex backend by setting `GOOGLE_GENAI_USE_VERTEXAI=TRUE`.', 'query': 'What is the consolidated setup for Vertex AI when using ADK?'}, {'citations': ['You can deploy various open and proprietary models from the [Vertex AI Model Garden](https://console.cloud.google.com/vertex-ai/model-garden) to an endpoint.'], 'expected_answer': 'Various open and proprietary models from the Vertex AI Model Garden can be deployed to an endpoint.', 'query': 'What types of models can be deployed from Vertex AI Model Garden?'}, {'citations': ['Deploying your fine-tuned models (whether based on Gemini or other architectures supported by Vertex AI) results in an endpoint that can be used directly.'], 'expected_answer': 'Fine-tuned models, including those based on Gemini or other Vertex AI-supported architectures, can be deployed to a directly usable endpoint.', 'query': 'Can fine-tuned models be used directly via Vertex AI endpoints?'}, {'citations': ['Some providers, like Anthropic, make their models available directly through Vertex AI.', '**Integration Method:** Uses the direct model string (e.g., `\"claude-3-sonnet@20240229\"`), *but requires manual registration* within ADK.', \"**Why Registration?** ADK's registry automatically recognizes `gemini-*` strings and standard Vertex AI endpoint strings (`projects/.../endpoints/...`) and routes them via the `google-genai` library. For other model types used directly via Vertex AI (like Claude), you must explicitly tell the ADK registry which specific wrapper class (`Claude` in this case) knows how to handle that model identifier string with the Vertex AI backend.\"], 'expected_answer': 'Some third-party models, like Anthropic\\'s Claude, are available directly through Vertex AI. Their integration uses a direct model string (e.g., `\"claude-3-sonnet@20240229\"`) but requires manual registration within ADK. This is because ADK\\'s registry needs to be explicitly told which wrapper class (e.g., `Claude`) handles that model identifier with the Vertex AI backend, unlike `gemini-*` or standard Vertex AI endpoint strings which are automatically recognized.', 'query': 'How are third-party models like Anthropic Claude integrated with ADK via Vertex AI, and why is manual registration needed?'}, {'citations': ['**Setup:**', '1. **Vertex AI Environment:** Ensure the consolidated Vertex AI setup (ADC, Env Vars, `GOOGLE_GENAI_USE_VERTEXAI=TRUE`) is complete.', '2. **Install Provider Library:** Install the necessary client library configured for Vertex AI.', '    ```shell', '    pip install \"anthropic[vertex]\"', '    ```', '3. **Register Model Class:** Add this code near the start of your application, *before* creating an agent using the Claude model string:', '    ```python', '    # Required for using Claude model strings directly via Vertex AI with LlmAgent', '    from google.adk.models.anthropic_llm import Claude', '    from google.adk.models.registry import LLMRegistry', '    LLMRegistry.register(Claude)', '    ```'], 'expected_answer': 'To set up third-party models like Anthropic Claude on Vertex AI with ADK, first ensure the consolidated Vertex AI environment setup is complete. Then, install the necessary provider client library (e.g., `pip install \"anthropic[vertex]\"`). Finally, register the model class (e.g., `Claude`) with `LLMRegistry.register(Claude)` at the start of your application, before creating any agents using that model string.', 'query': 'What are the setup steps for integrating third-party models (e.g., Anthropic Claude) on Vertex AI with ADK?'}, {'citations': ['The Agent Development Kit (ADK) supports building sophisticated applications by composing multiple, distinct `BaseAgent` instances into a **Multi-Agent System (MAS)**.'], 'expected_answer': 'The Agent Development Kit (ADK) supports building sophisticated applications by composing multiple, distinct `BaseAgent` instances into a Multi-Agent System (MAS).', 'query': 'What is a Multi-Agent System (MAS) in ADK?'}, {'citations': ['In ADK, a multi-agent system is an application where different agents, often forming a hierarchy, collaborate or coordinate to achieve a larger goal. Structuring your application this way offers significant advantages, including enhanced modularity, specialization, reusability, maintainability, and the ability to define structured control flows using dedicated workflow agents.'], 'expected_answer': 'In ADK, a multi-agent system involves different agents, often in a hierarchy, collaborating to achieve a larger goal. This structure enhances modularity, specialization, reusability, maintainability, and allows for structured control flows using workflow agents.', 'query': 'What are the advantages of structuring an application as a Multi-Agent System in ADK?'}, {'citations': ['You can compose various types of agents derived from `BaseAgent` to build these systems:', '* **LLM Agents:** Agents powered by large language models. (See [LLM Agents](llm-agents.md))', '* **Workflow Agents:** Specialized agents (`SequentialAgent`, `ParallelAgent`, `LoopAgent`) designed to manage the execution flow of their sub-agents. (See [Workflow Agents](workflow-agents/index.md))', '* **Custom agents:** Your own agents inheriting from `BaseAgent` with specialized, non-LLM logic. (See [Custom Agents](custom-agents.md))'], 'expected_answer': 'Multi-agent systems in ADK can be composed of LLM Agents (powered by LLMs), Workflow Agents (`SequentialAgent`, `ParallelAgent`, `LoopAgent`) for managing execution flow, and Custom Agents (inheriting from `BaseAgent`) for specialized, non-LLM logic.', 'query': 'What types of agents can be composed to build Multi-Agent Systems in ADK?'}, {'citations': ['The foundation for structuring multi-agent systems is the parent-child relationship defined in `BaseAgent`.', '* **Establishing Hierarchy:** You create a tree structure by passing a list of agent instances to the `sub_agents` argument when initializing a parent agent. ADK automatically sets the `parent_agent` attribute on each child agent during initialization (`google.adk.agents.base_agent.py` - `model_post_init`).', '* **Single Parent Rule:** An agent instance can only be added as a sub-agent once. Attempting to assign a second parent will result in a `ValueError`.', '* **Importance:** This hierarchy defines the scope for [Workflow Agents](#22-workflow-agents-as-orchestrators) and influences the potential targets for LLM-Driven Delegation. You can navigate the hierarchy using `agent.parent_agent` or find descendants using `agent.find_agent(name)`.'], 'expected_answer': 'The parent-child relationship in `BaseAgent` forms the hierarchy of multi-agent systems. This is established by passing a list of agent instances to the `sub_agents` argument of a parent agent, which automatically sets the `parent_agent` attribute on children. An agent can only have one parent. This hierarchy is crucial for Workflow Agents and LLM-Driven Delegation, and can be navigated using `agent.parent_agent` or `agent.find_agent(name)`.', 'query': 'How is agent hierarchy established and managed in ADK, and why is it important?'}, {'citations': [\"ADK includes specialized agents derived from `BaseAgent` that don't perform tasks themselves but orchestrate the execution flow of their `sub_agents`.\"], 'expected_answer': 'ADK includes specialized agents derived from `BaseAgent` that orchestrate the execution flow of their `sub_agents` rather than performing tasks themselves.', 'query': 'What is the primary role of Workflow Agents in ADK?'}, {'citations': ['* **[`SequentialAgent`](workflow-agents/sequential-agents.md):** Executes its `sub_agents` one after another in the order they are listed.', '    * **Context:** Passes the *same* [`InvocationContext`](../runtime/index.md) sequentially, allowing agents to easily pass results via shared state.'], 'expected_answer': 'A `SequentialAgent` executes its `sub_agents` one after another in order, passing the same `InvocationContext` sequentially to allow results to be shared via a shared state.', 'query': 'How does `SequentialAgent` work and manage context?'}, {'citations': ['* **[`ParallelAgent`](workflow-agents/parallel-agents.md):** Executes its `sub_agents` in parallel. Events from sub-agents may be interleaved.', '    * **Context:** Modifies the `InvocationContext.branch` for each child agent (e.g., `ParentBranch.ChildName`), providing a distinct contextual path which can be useful for isolating history in some memory implementations.', '    * **State:** Despite different branches, all parallel children access the *same shared* `session.state`, enabling them to read initial state and write results (use distinct keys to avoid race conditions).'], 'expected_answer': 'A `ParallelAgent` executes its `sub_agents` in parallel, potentially interleaving events. It modifies `InvocationContext.branch` for each child for distinct contextual paths, but all parallel children access the same shared `session.state` for reading initial state and writing results, requiring distinct keys to avoid race conditions.', 'query': 'How does `ParallelAgent` execute sub-agents and manage context and state?'}, {'citations': ['* **[`LoopAgent`](workflow-agents/loop-agents.md):** Executes its `sub_agents` sequentially in a loop.', '    * **Termination:** The loop stops if the optional `max_iterations` is reached, or if any sub-agent yields an [`Event`](../events/index.md) with `actions.escalate=True`.', '    * **Context & State:** Passes the *same* `InvocationContext` in each iteration, allowing state changes (e.g., counters, flags) to persist across loops.'], 'expected_answer': 'A `LoopAgent` executes its `sub_agents` sequentially in a loop. The loop terminates if `max_iterations` is reached or if a sub-agent yields an `Event` with `actions.escalate=True`. It passes the same `InvocationContext` in each iteration, allowing state changes to persist.', 'query': 'How does `LoopAgent` function, and what are its termination conditions?'}, {'citations': [\"The `instruction` parameter is arguably the most critical for shaping an `LlmAgent`'s behavior. It's a string (or a function returning a string) that tells the agent:\", '* Its core task or goal.', '* Its personality or persona (e.g., \"You are a helpful assistant,\" \"You are a witty pirate\").', '* Constraints on its behavior (e.g., \"Only answer questions about X,\" \"Never reveal Y\").', '* How and when to use its `tools`. You should explain the purpose of each tool and the circumstances under which it should be called, supplementing any descriptions within the tool itself.', '* The desired format for its output (e.g., \"Respond in JSON,\" \"Provide a bulleted list\").'], 'expected_answer': \"The `instruction` parameter is crucial for defining an `LlmAgent`'s core task, personality, behavioral constraints, tool usage guidelines, and desired output format.\", 'query': \"What aspects of an `LlmAgent`'s behavior are defined by the `instruction` parameter?\"}, {'citations': ['**Tips for Effective Instructions:**', '* **Be Clear and Specific:** Avoid ambiguity. Clearly state the desired actions and outcomes.', '* **Use Markdown:** Improve readability for complex instructions using headings, lists, etc.', '* **Provide Examples (Few-Shot):** For complex tasks or specific output formats, include examples directly in the instruction.', \"* **Guide Tool Use:** Don't just list tools; explain *when* and *why* the agent should use them.\"], 'expected_answer': 'To write effective instructions, be clear and specific, use Markdown for readability, provide few-shot examples for complex tasks, and guide tool use by explaining when and why tools should be used.', 'query': 'What are the best practices for crafting effective instructions for an `LlmAgent`?'}, {'citations': ['The instruction is a string template, you can use the `{var}` syntax to insert dynamic values into the instruction.', '* `{var}` is used to insert the value of the state variable named var.', '* `{artifact.var}` is used to insert the text content of the artifact named var.', '* If the state variable or artifact does not exist, the agent will raise an error. If you want to ignore the error, you can append a `?` to the variable name as in `{var?}`.'], 'expected_answer': 'Dynamic values can be inserted into instructions using `{var}` for state variables and `{artifact.var}` for artifact content. Appending `?` to the variable name (e.g., `{var?}`) prevents errors if the variable or artifact does not exist.', 'query': 'How does string templating work within `LlmAgent` instructions?'}, {'citations': [\"Tools give your `LlmAgent` capabilities beyond the LLM's built-in knowledge or reasoning. They allow the agent to interact with the outside world, perform calculations, fetch real-time data, or execute specific actions.\"], 'expected_answer': \"Tools extend an `LlmAgent`'s capabilities by enabling interaction with external systems, performing calculations, fetching real-time data, and executing specific actions.\", 'query': 'What capabilities do tools provide to an `LlmAgent`?'}, {'citations': ['* **`tools` (Optional):** Provide a list of tools the agent can use. Each item in the list can be:', '    * A Python function (automatically wrapped as a `FunctionTool`).', '    * An instance of a class inheriting from `BaseTool`.', '    * An instance of another agent (`AgentTool`, enabling agent-to-agent delegation - see [Multi-Agents](multi-agents.md)).'], 'expected_answer': 'The `tools` parameter accepts a list of Python functions (wrapped as `FunctionTool`), instances of `BaseTool` subclasses, or instances of other agents (`AgentTool`) for agent-to-agent delegation.', 'query': 'What are the different types of items that can be included in the `tools` list for an `LlmAgent`?'}, {'citations': ['The LLM uses the function/tool names, descriptions (from docstrings or the `description` field), and parameter schemas to decide which tool to call based on the conversation and its instructions.'], 'expected_answer': \"The LLM decides which tool to call based on the tool's name, description (from docstrings or the `description` field), and parameter schemas, in conjunction with the conversation context and its instructions.\", 'query': 'How does the LLM decide which tool to call?'}, {'citations': ['* **`generate_content_config` (Optional):** Pass an instance of `google.genai.types.GenerateContentConfig` to control parameters like `temperature` (randomness), `max_output_tokens` (response length), `top_p`, `top_k`, and safety settings.'], 'expected_answer': 'The `generate_content_config` parameter, an instance of `google.genai.types.GenerateContentConfig`, allows control over LLM generation parameters such as `temperature`, `max_output_tokens`, `top_p`, `top_k`, and safety settings.', 'query': 'Which parameters can be controlled using `generate_content_config`?'}, {'citations': ['* **`input_schema` (Optional):** Define a Pydantic `BaseModel` class representing the expected input structure. If set, the user message content passed to this agent *must* be a JSON string conforming to this schema. Your instructions should guide the user or preceding agent accordingly.'], 'expected_answer': 'The `input_schema` parameter defines a Pydantic `BaseModel` for the expected input structure. If set, the user message content must be a JSON string conforming to this schema, and instructions should guide users or preceding agents accordingly.', 'query': 'What is the purpose of `input_schema` in `LlmAgent` configuration?'}, {'citations': [\"* **`output_schema` (Optional):** Define a Pydantic `BaseModel` class representing the desired output structure. If set, the agent's final response *must* be a JSON string conforming to this schema.\", \"    * **Constraint:** Using `output_schema` enables controlled generation within the LLM but **disables the agent's ability to use tools or transfer control to other agents**. Your instructions must guide the LLM to produce JSON matching the schema directly.\"], 'expected_answer': \"The `output_schema` parameter defines a Pydantic `BaseModel` for the desired output JSON structure. When used, the agent's final response must conform to this schema, but it disables the agent's ability to use tools or transfer control to other agents, requiring the LLM to directly produce the JSON.\", 'query': \"What is the effect of using `output_schema` on an `LlmAgent`'s behavior?\"}, {'citations': [\"* **`output_key` (Optional):** Provide a string key. If set, the text content of the agent's *final* response will be automatically saved to the session's state dictionary under this key (e.g., `session.state[output_key] = agent_response_text`). This is useful for passing results between agents or steps in a workflow.\"], 'expected_answer': \"The `output_key` parameter, if set, automatically saves the `LlmAgent`'s final response text to the session's state dictionary under the specified key, facilitating result passing between agents or workflow steps.\", 'query': 'How does `output_key` facilitate data transfer between agents?'}, {'citations': [\"* **`include_contents` (Optional, Default: `'default'`):** Determines if the `contents` (history) are sent to the LLM.\", \"    * `'default'`: The agent receives the relevant conversation history.\", \"    * `'none'`: The agent receives no prior `contents`. It operates based solely on its current instruction and any input provided in the *current* turn (useful for stateless tasks or enforcing specific contexts).\"], 'expected_answer': \"The `include_contents` parameter controls whether the LLM receives conversation history. `'default'` includes history, while `'none'` excludes it, making the agent stateless and reliant only on current input and instructions.\", 'query': \"Explain the difference between `include_contents='default'` and `include_contents='none'`.\"}, {'citations': ['* **`planner` (Optional):** Assign a `BasePlanner` instance to enable multi-step reasoning and planning before execution. (See [Multi-Agents](multi-agents.md) patterns).', \"* **`code_executor` (Optional):** Provide a `BaseCodeExecutor` instance to allow the agent to execute code blocks (e.g., Python) found in the LLM's response. ([See Tools/Built-in tools](../tools/built-in-tools.md)).\"], 'expected_answer': \"An `LlmAgent` can be configured with a `planner` (a `BasePlanner` instance) for multi-step reasoning and planning, and a `code_executor` (a `BaseCodeExecutor` instance) to execute code blocks from the LLM's response.\", 'query': 'What are `planner` and `code_executor` used for in `LlmAgent`?'}, {'citations': ['The `google-genai` library, used internally by ADK for Gemini, can connect through either Google AI Studio or Vertex AI.'], 'expected_answer': 'The `google-genai` library, which ADK uses for Gemini, can connect via Google AI Studio or Vertex AI.', 'query': 'What are the two connection options for the `google-genai` library with Gemini in ADK?'}, {'citations': ['In order to use voice/video streaming in ADK, you will need to use Gemini models that support the Live API.'], 'expected_answer': 'To use voice/video streaming in ADK, Gemini models that support the Live API are required.', 'query': 'What is a prerequisite for using voice/video streaming with Gemini models in ADK?'}, {'citations': ['Google AI Studio is the easiest way to get started with Gemini. All you need is the [API key](https://aistudio.google.com/app/apikey). Best for rapid prototyping and development.'], 'expected_answer': 'Google AI Studio is the easiest way to start with Gemini, requiring only an API key, and is best for rapid prototyping and development.', 'query': 'What is the primary benefit of using Google AI Studio for Gemini with ADK?'}, {'citations': ['Vertex AI is recommended for production applications, leveraging Google Cloud infrastructure. Gemini on Vertex AI supports enterprise-grade features, security, and compliance controls.'], 'expected_answer': 'Vertex AI is recommended for production applications due to its enterprise-grade features, security, and compliance controls, leveraging Google Cloud infrastructure.', 'query': 'Why is Vertex AI recommended for production applications with Gemini in ADK?'}, {'citations': ['To access a vast range of LLMs from providers like OpenAI, Anthropic (non-Vertex AI), Cohere, and many others, ADK offers integration through the LiteLLM library.'], 'expected_answer': 'ADK integrates with a wide range of LLMs from providers like OpenAI, Anthropic (non-Vertex AI), and Cohere through the LiteLLM library.', 'query': 'Which library does ADK use to integrate with various LLM providers like OpenAI and Anthropic (non-Vertex AI)?'}, {'citations': ['[LiteLLM](https://docs.litellm.ai/) acts as a translation layer, providing a standardized, OpenAI-compatible interface to over 100+ LLMs.'], 'expected_answer': 'LiteLLM functions as a translation layer, offering a standardized, OpenAI-compatible interface to more than 100 LLMs.', 'query': 'How many LLMs does LiteLLM provide an interface to?'}, {'citations': ['If your agent is relying on tools, please make sure that you select a model with tool support from [Ollama website](https://ollama.com/search?c=tools).'], 'expected_answer': 'If an ADK agent relies on tools, the Ollama model selected must have tool support.', 'query': 'What is a key consideration when selecting an Ollama model for an ADK agent that uses tools?'}, {'citations': ['It is important to set the provider `ollama_chat` instead of `ollama`. Using `ollama` will result in unexpected behaviors such as infinite tool call loops and ignoring previous context.'], 'expected_answer': 'When using Ollama models with LiteLLM, it is important to set the provider to `ollama_chat` instead of `ollama` to avoid issues like infinite tool call loops and ignoring previous context.', 'query': 'What is the correct provider name for Ollama models in LiteLLM, and why is it important?'}, {'citations': ['While `api_base` can be provided inside LiteLLM for generation, LiteLLM library is calling other APIs relying on the env variable instead as of v1.65.5 after completion. So at this time, we recommend setting the env variable `OLLAMA_API_BASE` to point to the ollama server.'], 'expected_answer': 'It is recommended to set the `OLLAMA_API_BASE` environment variable to point to the Ollama server because the LiteLLM library, as of v1.65.5, relies on this environment variable for API calls after completion.', 'query': 'Why is `OLLAMA_API_BASE` environment variable recommended for Ollama integration?'}, {'citations': ['Alternatively, `openai` can be used as the provider name. But this will also require setting the `OPENAI_API_BASE=http://localhost:11434/v1` and `OPENAI_API_KEY=anything` env variables instead of `OLLAMA_API_BASE`. **Please note that api base now has `/v1` at the end.**'], 'expected_answer': 'The `openai` provider name can be used for Ollama models, but it requires setting `OPENAI_API_BASE` to `http://localhost:11434/v1` and `OPENAI_API_KEY` to any value, instead of `OLLAMA_API_BASE`.', 'query': 'What environment variables are needed if using `openai` as the provider name for Ollama models?'}, {'citations': ['Tools such as [vLLM](https://github.com/vllm-project/vllm) allow you to host models efficiently and often expose an OpenAI-compatible API endpoint.'], 'expected_answer': 'vLLM allows efficient hosting of models and often provides an OpenAI-compatible API endpoint.', 'query': 'What kind of API endpoint does vLLM typically expose?'}, {'citations': ['* *Important for ADK Tools:* When deploying, ensure the serving tool supports and enables OpenAI-compatible tool/function calling. For vLLM, this might involve flags like `--enable-auto-tool-choice` and potentially a specific `--tool-call-parser`, depending on the model.'], 'expected_answer': \"When deploying a model with vLLM for ADK tools, it's crucial to ensure the serving tool supports and enables OpenAI-compatible tool/function calling, potentially using flags like `--enable-auto-tool-choice` and a specific `--tool-call-parser`.\", 'query': 'What is important for ADK Tools when deploying a model with vLLM?'}, {'citations': [\"For enterprise-grade scalability, reliability, and integration with Google Cloud's MLOps ecosystem, you can use models deployed to Vertex AI Endpoints. This includes models from Model Garden or your own fine-tuned models.\"], 'expected_answer': \"Models deployed to Vertex AI Endpoints, including those from Model Garden or fine-tuned models, offer enterprise-grade scalability, reliability, and integration with Google Cloud's MLOps ecosystem.\", 'query': 'What are the advantages of using Vertex AI Endpoints for models?'}, {'citations': ['Some providers, like Anthropic, make their models available directly through Vertex AI.'], 'expected_answer': 'Providers such as Anthropic offer their models directly through Vertex AI.', 'query': 'Are third-party models available directly through Vertex AI?'}, {'citations': [\"ADK's registry automatically recognizes `gemini-*` strings and standard Vertex AI endpoint strings (`projects/.../endpoints/...`) and routes them via the `google-genai` library. For other model types used directly via Vertex AI (like Claude), you must explicitly tell the ADK registry which specific wrapper class (`Claude` in this case) knows how to handle that model identifier string with the Vertex AI backend.\"], 'expected_answer': \"ADK's registry automatically routes `gemini-*` and standard Vertex AI endpoint strings via `google-genai`. However, for other Vertex AI model types like Claude, you must explicitly register the corresponding wrapper class (e.g., `Claude`) with the ADK registry.\", 'query': 'Why is explicit registration needed for some Vertex AI models in ADK?'}, {'citations': ['The Agent Development Kit (ADK) supports building sophisticated applications by composing multiple, distinct `BaseAgent` instances into a **Multi-Agent System (MAS)**.'], 'expected_answer': 'ADK facilitates the creation of sophisticated applications by allowing the composition of multiple, distinct `BaseAgent` instances into a Multi-Agent System (MAS).', 'query': 'What is the primary capability of ADK regarding complex applications?'}, {'citations': ['In ADK, a multi-agent system is an application where different agents, often forming a hierarchy, collaborate or coordinate to achieve a larger goal.'], 'expected_answer': 'In ADK, a multi-agent system is an application where various agents, often organized hierarchically, collaborate or coordinate to achieve a broader objective.', 'query': 'What is the fundamental structure of a multi-agent system in ADK?'}, {'citations': ['Structuring your application this way offers significant advantages, including enhanced modularity, specialization, reusability, maintainability, and the ability to define structured control flows using dedicated workflow agents.'], 'expected_answer': 'Structuring an application as a multi-agent system offers enhanced modularity, specialization, reusability, maintainability, and the ability to define structured control flows using dedicated workflow agents.', 'query': 'What are the key benefits of adopting a multi-agent system architecture?'}, {'citations': ['The foundation for structuring multi-agent systems is the parent-child relationship defined in `BaseAgent`.'], 'expected_answer': 'The parent-child relationship defined in `BaseAgent` forms the foundation for structuring multi-agent systems.', 'query': 'What is the foundational concept for structuring multi-agent systems in ADK?'}, {'citations': ['You create a tree structure by passing a list of agent instances to the `sub_agents` argument when initializing a parent agent. ADK automatically sets the `parent_agent` attribute on each child agent during initialization (`google.adk.agents.base_agent.py` - `model_post_init`).'], 'expected_answer': 'A tree structure is created by passing a list of agent instances to the `sub_agents` argument during parent agent initialization, and ADK automatically sets the `parent_agent` attribute on each child.', 'query': \"How is a tree structure created in ADK's agent hierarchy?\"}, {'citations': ['An agent instance can only be added as a sub-agent once. Attempting to assign a second parent will result in a `ValueError`.'], 'expected_answer': 'An agent instance can only be a sub-agent to one parent; assigning a second parent will cause a `ValueError`.', 'query': 'What happens if an agent instance is assigned to multiple parents?'}, {'citations': ['This hierarchy defines the scope for [Workflow Agents](#22-workflow-agents-as-orchestrators) and influences the potential targets for LLM-Driven Delegation. You can navigate the hierarchy using `agent.parent_agent` or find descendants using `agent.find_agent(name)`.'], 'expected_answer': 'The agent hierarchy defines the scope for Workflow Agents and influences LLM-Driven Delegation targets. It can be navigated using `agent.parent_agent` or by finding descendants with `agent.find_agent(name)`.', 'query': 'How can one navigate the agent hierarchy in ADK?'}, {'citations': [\"ADK includes specialized agents derived from `BaseAgent` that don't perform tasks themselves but orchestrate the execution flow of their `sub_agents`.\"], 'expected_answer': \"ADK's specialized agents, derived from `BaseAgent`, orchestrate the execution flow of their `sub_agents` instead of performing tasks directly.\", 'query': 'What is the primary function of specialized agents in ADK?'}, {'citations': ['Executes its `sub_agents` one after another in the order they are listed.'], 'expected_answer': 'A `SequentialAgent` executes its `sub_agents` sequentially in the order they are listed.', 'query': 'How does a `SequentialAgent` execute its sub-agents?'}, {'citations': ['Passes the *same* [`InvocationContext`](../runtime/index.md) sequentially, allowing agents to easily pass results via shared state.'], 'expected_answer': 'A `SequentialAgent` passes the same `InvocationContext` sequentially, enabling agents to share results through a shared state.', 'query': 'How does `SequentialAgent` manage `InvocationContext` for its sub-agents?'}, {'citations': ['Executes its `sub_agents` in parallel. Events from sub-agents may be interleaved.'], 'expected_answer': 'A `ParallelAgent` executes its `sub_agents` concurrently, and events from these sub-agents may be interleaved.', 'query': 'How does a `ParallelAgent` execute its sub-agents?'}, {'citations': ['Modifies the `InvocationContext.branch` for each child agent (e.g., `ParentBranch.ChildName`), providing a distinct contextual path which can be useful for isolating history in some memory implementations.'], 'expected_answer': 'A `ParallelAgent` modifies the `InvocationContext.branch` for each child agent, creating a distinct contextual path useful for isolating history in memory implementations.', 'query': 'What is the purpose of modifying `InvocationContext.branch` in a `ParallelAgent`?'}, {'citations': ['Despite different branches, all parallel children access the *same shared* `session.state`, enabling them to read initial state and write results (use distinct keys to avoid race conditions).'], 'expected_answer': 'All parallel children in a `ParallelAgent` access the same shared `session.state`, allowing them to read initial state and write results. However, distinct keys should be used to prevent race conditions.', 'query': 'How do parallel children agents manage shared state, and what precaution should be taken?'}, {'citations': ['Executes its `sub_agents` sequentially in a loop.'], 'expected_answer': 'A `LoopAgent` executes its `sub_agents` sequentially within a loop.', 'query': 'What is the execution pattern of a `LoopAgent`?'}, {'citations': ['The loop stops if the optional `max_iterations` is reached, or if any sub-agent yields an [`Event`](../events/index.md) with `actions.escalate=True`.'], 'expected_answer': 'A `LoopAgent` terminates if it reaches its `max_iterations` or if any sub-agent yields an `Event` with `actions.escalate=True`.', 'query': 'What are the conditions for a `LoopAgent` to terminate?'}, {'citations': ['Passes the *same* `InvocationContext` in each iteration, allowing state changes (e.g., counters, flags) to persist across loops.'], 'expected_answer': 'A `LoopAgent` passes the same `InvocationContext` in each iteration, ensuring that state changes, such as counters or flags, persist across loops.', 'query': 'How does `LoopAgent` ensure state changes persist across iterations?'}, {'citations': [\"The `instruction` parameter is arguably the most critical for shaping an `LlmAgent`'s behavior. It's a string (or a function returning a string) that tells the agent:\", '* Its core task or goal.', '* Its personality or persona (e.g., \"You are a helpful assistant,\" \"You are a witty pirate\").', '* Constraints on its behavior (e.g., \"Only answer questions about X,\" \"Never reveal Y\").', '* How and when to use its `tools`. You should explain the purpose of each tool and the circumstances under which it should be called, supplementing any descriptions within the tool itself.', '* The desired format for its output (e.g., \"Respond in JSON,\" \"Provide a bulleted list\").'], 'expected_answer': \"The `instruction` parameter defines an `LlmAgent`'s core task, personality, behavioral constraints, tool usage, and desired output format.\", 'query': 'What are the key elements defined by the `instruction` parameter for an `LlmAgent`?'}, {'citations': ['**Tips for Effective Instructions:**', '* **Be Clear and Specific:** Avoid ambiguity. Clearly state the desired actions and outcomes.', '* **Use Markdown:** Improve readability for complex instructions using headings, lists, etc.', '* **Provide Examples (Few-Shot):** For complex tasks or specific output formats, include examples directly in the instruction.', \"* **Guide Tool Use:** Don't just list tools; explain *when* and *why* the agent should use them.\"], 'expected_answer': 'Effective instructions should be clear and specific, use Markdown for readability, include few-shot examples for complex tasks, and guide tool use by explaining when and why tools should be utilized.', 'query': 'What are the best practices for writing `LlmAgent` instructions?'}, {'citations': ['The instruction is a string template, you can use the `{var}` syntax to insert dynamic values into the instruction.', '* `{var}` is used to insert the value of the state variable named var.', '* `{artifact.var}` is used to insert the text content of the artifact named var.', '* If the state variable or artifact does not exist, the agent will raise an error. If you want to ignore the error, you can append a `?` to the variable name as in `{var?}`.'], 'expected_answer': 'Dynamic values can be inserted into `LlmAgent` instructions using `{var}` for state variables and `{artifact.var}` for artifact content. Adding `?` (e.g., `{var?}`) prevents errors if the variable or artifact is missing.', 'query': 'How can dynamic data be incorporated into `LlmAgent` instructions?'}, {'citations': [\"Tools give your `LlmAgent` capabilities beyond the LLM's built-in knowledge or reasoning. They allow the agent to interact with the outside world, perform calculations, fetch real-time data, or execute specific actions.\"], 'expected_answer': \"Tools provide `LlmAgent` with capabilities to interact with external systems, perform calculations, retrieve real-time data, and execute specific actions, extending beyond the LLM's inherent knowledge.\", 'query': 'What is the primary function of tools in an `LlmAgent`?'}, {'citations': ['* **`tools` (Optional):** Provide a list of tools the agent can use. Each item in the list can be:', '    * A Python function (automatically wrapped as a `FunctionTool`).', '    * An instance of a class inheriting from `BaseTool`.', '    * An instance of another agent (`AgentTool`, enabling agent-to-agent delegation - see [Multi-Agents](multi-agents.md)).'], 'expected_answer': 'The `tools` parameter accepts Python functions (wrapped as `FunctionTool`), instances of `BaseTool` subclasses, or instances of other agents (`AgentTool`) for delegation.', 'query': 'What are the acceptable types for items in the `tools` list of an `LlmAgent`?'}, {'citations': ['The LLM uses the function/tool names, descriptions (from docstrings or the `description` field), and parameter schemas to decide which tool to call based on the conversation and its instructions.'], 'expected_answer': \"The LLM determines which tool to call by evaluating the tool's name, description, and parameter schemas in conjunction with the conversation context and its own instructions.\", 'query': 'What factors does the LLM consider when deciding which tool to invoke?'}, {'citations': ['* **`generate_content_config` (Optional):** Pass an instance of `google.genai.types.GenerateContentConfig` to control parameters like `temperature` (randomness), `max_output_tokens` (response length), `top_p`, `top_k`, and safety settings.'], 'expected_answer': 'The `generate_content_config` parameter, an instance of `google.genai.types.GenerateContentConfig`, allows control over LLM generation attributes such as `temperature`, `max_output_tokens`, `top_p`, `top_k`, and safety settings.', 'query': 'Which configuration object is used to control LLM generation parameters?'}, {'citations': ['* **`input_schema` (Optional):** Define a Pydantic `BaseModel` class representing the expected input structure. If set, the user message content passed to this agent *must* be a JSON string conforming to this schema. Your instructions should guide the user or preceding agent accordingly.'], 'expected_answer': 'The `input_schema` parameter defines a Pydantic `BaseModel` for the expected input structure, requiring user message content to be a JSON string conforming to it. Instructions should guide users or preceding agents to adhere to this schema.', 'query': 'What is the role of `input_schema` in ensuring structured input for an `LlmAgent`?'}, {'citations': [\"* **`output_schema` (Optional):** Define a Pydantic `BaseModel` class representing the desired output structure. If set, the agent's final response *must* be a JSON string conforming to this schema.\", \"    * **Constraint:** Using `output_schema` enables controlled generation within the LLM but **disables the agent's ability to use tools or transfer control to other agents**. Your instructions must guide the LLM to produce JSON matching the schema directly.\"], 'expected_answer': \"The `output_schema` parameter defines a Pydantic `BaseModel` for the desired output JSON structure. While it enables controlled LLM generation, it disables the agent's ability to use tools or transfer control, requiring the LLM to directly produce JSON matching the schema.\", 'query': 'What are the implications of using `output_schema` for an `LlmAgent`?'}, {'citations': [\"* **`output_key` (Optional):** Provide a string key. If set, the text content of the agent's *final* response will be automatically saved to the session's state dictionary under this key (e.g., `session.state[output_key] = agent_response_text`). This is useful for passing results between agents or steps in a workflow.\"], 'expected_answer': \"The `output_key` parameter saves the `LlmAgent`'s final response text to the session's state dictionary under the specified key, facilitating result sharing between agents or workflow steps.\", 'query': 'How does `output_key` contribute to inter-agent communication?'}, {'citations': [\"* **`include_contents` (Optional, Default: `'default'`):** Determines if the `contents` (history) are sent to the LLM.\", \"    * `'default'`: The agent receives the relevant conversation history.\", \"    * `'none'`: The agent receives no prior `contents`. It operates based solely on its current instruction and any input provided in the *current* turn (useful for stateless tasks or enforcing specific contexts).\"], 'expected_answer': \"The `include_contents` parameter controls whether conversation history is provided to the LLM. `'default'` includes relevant history, while `'none'` ensures the agent operates solely on current input and instructions, useful for stateless operations.\", 'query': \"When would you set `include_contents` to `'none'` for an `LlmAgent`?\"}, {'citations': ['* **`planner` (Optional):** Assign a `BasePlanner` instance to enable multi-step reasoning and planning before execution. (See [Multi-Agents](multi-agents.md) patterns).', \"* **`code_executor` (Optional):** Provide a `BaseCodeExecutor` instance to allow the agent to execute code blocks (e.g., Python) found in the LLM's response. ([See Tools/Built-in tools](../tools/built-in-tools.md)).\"], 'expected_answer': \"An `LlmAgent` can use a `BasePlanner` instance for multi-step reasoning and planning, and a `BaseCodeExecutor` instance to execute code blocks from the LLM's response.\", 'query': 'What are the roles of `planner` and `code_executor` in enhancing `LlmAgent` capabilities?'}, {'citations': ['The `google-genai` library, used internally by ADK for Gemini, can connect through either Google AI Studio or Vertex AI.'], 'expected_answer': 'The `google-genai` library, which ADK uses for Gemini, supports connections via Google AI Studio or Vertex AI.', 'query': 'What are the two primary connection methods for `google-genai` with Gemini in ADK?'}, {'citations': ['In order to use voice/video streaming in ADK, you will need to use Gemini models that support the Live API.'], 'expected_answer': 'To enable voice/video streaming in ADK, you must use Gemini models that support the Live API.', 'query': 'What specific Gemini model feature is required for voice/video streaming in ADK?'}, {'citations': ['Google AI Studio is the easiest way to get started with Gemini. All you need is the [API key](https://aistudio.google.com/app/apikey). Best for rapid prototyping and development.'], 'expected_answer': 'Google AI Studio is the simplest way to begin with Gemini, requiring only an API key, and is ideal for rapid prototyping and development.', 'query': 'What makes Google AI Studio suitable for rapid prototyping with Gemini?'}, {'citations': ['Vertex AI is recommended for production applications, leveraging Google Cloud infrastructure. Gemini on Vertex AI supports enterprise-grade features, security, and compliance controls.'], 'expected_answer': 'Vertex AI is recommended for production applications because it offers enterprise-grade features, security, and compliance controls, leveraging Google Cloud infrastructure.', 'query': 'Why is Vertex AI the preferred choice for production applications using Gemini?'}, {'citations': ['To access a vast range of LLMs from providers like OpenAI, Anthropic (non-Vertex AI), Cohere, and many others, ADK offers integration through the LiteLLM library.'], 'expected_answer': 'ADK integrates with a wide array of LLMs from providers such as OpenAI, Anthropic (non-Vertex AI), and Cohere via the LiteLLM library.', 'query': \"Which library facilitates ADK's integration with diverse LLM providers?\"}, {'citations': ['[LiteLLM](https://docs.litellm.ai/) acts as a translation layer, providing a standardized, OpenAI-compatible interface to over 100+ LLMs.'], 'expected_answer': 'LiteLLM serves as a translation layer, offering a standardized, OpenAI-compatible interface for over 100 LLMs.', 'query': 'What is the primary function of LiteLLM?'}, {'citations': ['If your agent is relying on tools, please make sure that you select a model with tool support from [Ollama website](https://ollama.com/search?c=tools).'], 'expected_answer': 'For agents relying on tools, ensure the selected Ollama model supports tools, which can be verified on the Ollama website.', 'query': 'How can you verify tool support for an Ollama model?'}, {'citations': ['It is important to set the provider `ollama_chat` instead of `ollama`. Using `ollama` will result in unexpected behaviors such as infinite tool call loops and ignoring previous context.'], 'expected_answer': 'It is crucial to use `ollama_chat` as the provider instead of `ollama` to prevent issues like infinite tool call loops and context loss.', 'query': 'What are the potential issues if `ollama` is used as the provider instead of `ollama_chat`?'}, {'citations': ['While `api_base` can be provided inside LiteLLM for generation, LiteLLM library is calling other APIs relying on the env variable instead as of v1.65.5 after completion. So at this time, we recommend setting the env variable `OLLAMA_API_BASE` to point to the ollama server.'], 'expected_answer': 'Setting the `OLLAMA_API_BASE` environment variable to the Ollama server is recommended because the LiteLLM library, as of v1.65.5, uses this variable for API calls after completion, even if `api_base` is provided internally.', 'query': 'Why is `OLLAMA_API_BASE` environment variable preferred over `api_base` within LiteLLM for Ollama integration?'}, {'citations': ['Alternatively, `openai` can be used as the provider name. But this will also require setting the `OPENAI_API_BASE=http://localhost:11434/v1` and `OPENAI_API_KEY=anything` env variables instead of `OLLAMA_API_BASE`. **Please note that api base now has `/v1` at the end.**'], 'expected_answer': 'Using `openai` as the provider name for Ollama models requires setting `OPENAI_API_BASE` to `http://localhost:11434/v1` and `OPENAI_API_KEY` to any value, replacing `OLLAMA_API_BASE`.', 'query': 'What specific changes are needed to use the `openai` provider name for Ollama models?'}, {'citations': ['Tools such as [vLLM](https://github.com/vllm-project/vllm) allow you to host models efficiently and often expose an OpenAI-compatible API endpoint.'], 'expected_answer': 'vLLM enables efficient model hosting and typically provides an OpenAI-compatible API endpoint.', 'query': 'What is the primary advantage of using vLLM for model hosting?'}, {'citations': ['* *Important for ADK Tools:* When deploying, ensure the serving tool supports and enables OpenAI-compatible tool/function calling. For vLLM, this might involve flags like `--enable-auto-tool-choice` and potentially a specific `--tool-call-parser`, depending on the model.'], 'expected_answer': \"When deploying models with vLLM for ADK Tools, it's crucial to ensure the serving tool supports OpenAI-compatible tool/function calling, potentially requiring specific flags like `--enable-auto-tool-choice` and a `--tool-call-parser`.\", 'query': 'What specific configurations are important for vLLM deployments to support ADK Tools?'}, {'citations': [\"For enterprise-grade scalability, reliability, and integration with Google Cloud's MLOps ecosystem, you can use models deployed to Vertex AI Endpoints. This includes models from Model Garden or your own fine-tuned models.\"], 'expected_answer': \"Models deployed to Vertex AI Endpoints, including those from Model Garden or fine-tuned models, offer enterprise-grade scalability, reliability, and integration with Google Cloud's MLOps ecosystem.\", 'query': 'What are the benefits of deploying models to Vertex AI Endpoints?'}, {'citations': ['Some providers, like Anthropic, make their models available directly through Vertex AI.'], 'expected_answer': 'Yes, some providers, such as Anthropic, offer their models directly through Vertex AI.', 'query': 'Are there third-party LLM providers whose models are directly accessible via Vertex AI?'}, {'citations': [\"ADK's registry automatically recognizes `gemini-*` strings and standard Vertex AI endpoint strings (`projects/.../endpoints/...`) and routes them via the `google-genai` library. For other model types used directly via Vertex AI (like Claude), you must explicitly tell the ADK registry which specific wrapper class (`Claude` in this case) knows how to handle that model identifier string with the Vertex AI backend.\"], 'expected_answer': \"ADK's registry automatically handles `gemini-*` and standard Vertex AI endpoint strings via `google-genai`. However, for other Vertex AI model types (e.g., Claude), explicit registration of the corresponding wrapper class is required to inform the ADK registry how to handle that model identifier.\", 'query': 'Explain the difference in model registration for Gemini vs. other Vertex AI models in ADK.'}, {'citations': ['The Agent Development Kit (ADK) supports building sophisticated applications by composing multiple, distinct `BaseAgent` instances into a **Multi-Agent System (MAS)**.'], 'expected_answer': 'ADK supports building sophisticated applications by composing multiple, distinct `BaseAgent` instances into a Multi-Agent System (MAS).', 'query': 'What is the core capability of ADK for complex application development?'}, {'citations': ['In ADK, a multi-agent system is an application where different agents, often forming a hierarchy, collaborate or coordinate to achieve a larger goal.'], 'expected_answer': 'A multi-agent system in ADK is an application where various agents, often organized hierarchically, collaborate or coordinate to achieve a common goal.', 'query': 'Describe the nature of a multi-agent system within ADK.'}, {'citations': ['Structuring your application this way offers significant advantages, including enhanced modularity, specialization, reusability, maintainability, and the ability to define structured control flows using dedicated workflow agents.'], 'expected_answer': 'Structuring an application as a multi-agent system provides benefits such as enhanced modularity, specialization, reusability, maintainability, and the ability to define structured control flows using workflow agents.', 'query': 'What are the primary advantages of using a multi-agent system architecture in ADK?'}, {'citations': ['The foundation for structuring multi-agent systems is the parent-child relationship defined in `BaseAgent`.'], 'expected_answer': 'The parent-child relationship, defined in `BaseAgent`, serves as the foundation for structuring multi-agent systems.', 'query': 'What is the fundamental principle for organizing multi-agent systems in ADK?'}, {'citations': ['You create a tree structure by passing a list of agent instances to the `sub_agents` argument when initializing a parent agent. ADK automatically sets the `parent_agent` attribute on each child agent during initialization (`google.adk.agents.base_agent.py` - `model_post_init`).'], 'expected_answer': 'A tree structure is formed by providing a list of agent instances to the `sub_agents` argument during parent agent initialization, and ADK automatically assigns the `parent_agent` attribute to each child.', 'query': 'How is the hierarchical structure of agents established in ADK?'}, {'citations': ['An agent instance can only be added as a sub-agent once. Attempting to assign a second parent will result in a `ValueError`.'], 'expected_answer': 'An agent instance can only be a sub-agent to a single parent; attempting to assign it to another parent will raise a `ValueError`.', 'query': 'What is the constraint on an agent instance having multiple parents?'}, {'citations': ['This hierarchy defines the scope for [Workflow Agents](#22-workflow-agents-as-orchestrators) and influences the potential targets for LLM-Driven Delegation. You can navigate the hierarchy using `agent.parent_agent` or find descendants using `agent.find_agent(name)`.'], 'expected_answer': 'The agent hierarchy defines the scope for Workflow Agents and influences LLM-Driven Delegation targets. Navigation is possible via `agent.parent_agent` or by finding descendants using `agent.find_agent(name)`.', 'query': 'What are the practical implications of the agent hierarchy in ADK?'}, {'citations': [\"ADK includes specialized agents derived from `BaseAgent` that don't perform tasks themselves but orchestrate the execution flow of their `sub_agents`.\"], 'expected_answer': 'ADK provides specialized agents, derived from `BaseAgent`, that orchestrate the execution flow of their `sub_agents` rather than performing tasks directly.', 'query': 'What is the primary characteristic of specialized agents in ADK?'}, {'citations': ['Executes its `sub_agents` one after another in the order they are listed.'], 'expected_answer': 'A `SequentialAgent` executes its `sub_agents` sequentially, following the order in which they are listed.', 'query': 'Describe the execution order of sub-agents in a `SequentialAgent`.'}, {'citations': ['Passes the *same* [`InvocationContext`](../runtime/index.md) sequentially, allowing agents to easily pass results via shared state.'], 'expected_answer': 'A `SequentialAgent` passes the same `InvocationContext` to its sub-agents sequentially, enabling them to share results through a common state.', 'query': 'How does `SequentialAgent` facilitate data sharing among its sub-agents?'}, {'citations': ['Executes its `sub_agents` in parallel. Events from sub-agents may be interleaved.'], 'expected_answer': 'A `ParallelAgent` executes its `sub_agents` concurrently, and events generated by these sub-agents may occur in an interleaved fashion.', 'query': 'What is the concurrency behavior of a `ParallelAgent`?'}, {'citations': ['Modifies the `InvocationContext.branch` for each child agent (e.g., `ParentBranch.ChildName`), providing a distinct contextual path which can be useful for isolating history in some memory implementations.'], 'expected_answer': 'The `InvocationContext.branch` is modified for each child agent in a `ParallelAgent` to provide a distinct contextual path, which can help in isolating history within certain memory implementations.', 'query': 'Why is `InvocationContext.branch` modified for child agents in a `ParallelAgent`?'}, {'citations': ['Despite different branches, all parallel children access the *same shared* `session.state`, enabling them to read initial state and write results (use distinct keys to avoid race conditions).'], 'expected_answer': \"All parallel children agents share the same `session.state`, allowing them to read initial state and write results. However, it's important to use distinct keys when writing to avoid race conditions.\", 'query': 'How do parallel children agents interact with the `session.state`?'}, {'citations': ['Executes its `sub_agents` sequentially in a loop.'], 'expected_answer': 'A `LoopAgent` executes its `sub_agents` sequentially within a loop.', 'query': 'Describe the looping mechanism of a `LoopAgent`.'}, {'citations': ['The loop stops if the optional `max_iterations` is reached, or if any sub-agent yields an [`Event`](../events/index.md) with `actions.escalate=True`.'], 'expected_answer': 'A `LoopAgent` terminates either when it reaches the specified `max_iterations` or when any of its sub-agents yields an `Event` with `actions.escalate=True`.', 'query': 'Under what conditions does a `LoopAgent` cease execution?'}, {'citations': ['Passes the *same* `InvocationContext` in each iteration, allowing state changes (e.g., counters, flags) to persist across loops.'], 'expected_answer': 'A `LoopAgent` maintains state changes across iterations by passing the same `InvocationContext` in each loop, allowing elements like counters or flags to persist.', 'query': 'How does a `LoopAgent` ensure persistence of state changes?'}, {'citations': [\"The `instruction` parameter is arguably the most critical for shaping an `LlmAgent`'s behavior. It's a string (or a function returning a string) that tells the agent:\", '* Its core task or goal.', '* Its personality or persona (e.g., \"You are a helpful assistant,\" \"You are a witty pirate\").', '* Constraints on its behavior (e.g., \"Only answer questions about X,\" \"Never reveal Y\").', '* How and when to use its `tools`. You should explain the purpose of each tool and the circumstances under which it should be called, supplementing any descriptions within the tool itself.', '* The desired format for its output (e.g., \"Respond in JSON,\" \"Provide a bulleted list\").'], 'expected_answer': \"The `instruction` parameter is crucial for defining an `LlmAgent`'s core task, personality, behavioral constraints, tool usage, and desired output format.\", 'query': 'What are the key aspects controlled by the `instruction` parameter in an `LlmAgent`?'}, {'citations': ['**Tips for Effective Instructions:**', '* **Be Clear and Specific:** Avoid ambiguity. Clearly state the desired actions and outcomes.', '* **Use Markdown:** Improve readability for complex instructions using headings, lists, etc.', '* **Provide Examples (Few-Shot):** For complex tasks or specific output formats, include examples directly in the instruction.', \"* **Guide Tool Use:** Don't just list tools; explain *when* and *why* the agent should use them.\"], 'expected_answer': 'To write effective instructions, be clear and specific, use Markdown for readability, provide few-shot examples for complex tasks, and guide tool use by explaining when and why tools should be utilized.', 'query': 'What are the recommended practices for crafting effective `LlmAgent` instructions?'}, {'citations': ['The instruction is a string template, you can use the `{var}` syntax to insert dynamic values into the instruction.', '* `{var}` is used to insert the value of the state variable named var.', '* `{artifact.var}` is used to insert the text content of the artifact named var.', '* If the state variable or artifact does not exist, the agent will raise an error. If you want to ignore the error, you can append a `?` to the variable name as in `{var?}`.'], 'expected_answer': 'Dynamic values can be embedded in `LlmAgent` instructions using `{var}` for state variables and `{artifact.var}` for artifact content. Appending `?` to the variable name (e.g., `{var?}`) allows ignoring errors if the variable or artifact is absent.', 'query': 'How can dynamic content be embedded within `LlmAgent` instructions?'}, {'citations': [\"Tools give your `LlmAgent` capabilities beyond the LLM's built-in knowledge or reasoning. They allow the agent to interact with the outside world, perform calculations, fetch real-time data, or execute specific actions.\"], 'expected_answer': \"Tools extend an `LlmAgent`'s capabilities by enabling interaction with external systems, performing calculations, retrieving real-time data, and executing specific actions, thereby going beyond the LLM's inherent knowledge.\", 'query': 'What is the primary benefit of providing tools to an `LlmAgent`?'}, {'citations': ['* **`tools` (Optional):** Provide a list of tools the agent can use. Each item in the list can be:', '    * A Python function (automatically wrapped as a `FunctionTool`).', '    * An instance of a class inheriting from `BaseTool`.', '    * An instance of another agent (`AgentTool`, enabling agent-to-agent delegation - see [Multi-Agents](multi-agents.md)).'], 'expected_answer': 'The `tools` parameter accepts a list of Python functions (automatically wrapped as `FunctionTool`), instances of `BaseTool` subclasses, or instances of other agents (`AgentTool`) for agent-to-agent delegation.', 'query': 'What are the permissible types of entries in the `tools` list for an `LlmAgent`?'}, {'citations': ['The LLM uses the function/tool names, descriptions (from docstrings or the `description` field), and parameter schemas to decide which tool to call based on the conversation and its instructions.'], 'expected_answer': \"The LLM determines which tool to invoke by considering the tool's name, description, and parameter schemas, in conjunction with the ongoing conversation and its own instructions.\", 'query': 'What criteria does the LLM use to select a tool?'}, {'citations': ['* **`generate_content_config` (Optional):** Pass an instance of `google.genai.types.GenerateContentConfig` to control parameters like `temperature` (randomness), `max_output_tokens` (response length), `top_p`, `top_k`, and safety settings.'], 'expected_answer': 'The `generate_content_config` parameter, an instance of `google.genai.types.GenerateContentConfig`, allows fine-grained control over LLM generation attributes such as `temperature`, `max_output_tokens`, `top_p`, `top_k`, and safety settings.', 'query': 'Which configuration object is used to fine-tune LLM generation parameters?'}, {'citations': ['* **`input_schema` (Optional):** Define a Pydantic `BaseModel` class representing the expected input structure. If set, the user message content passed to this agent *must* be a JSON string conforming to this schema. Your instructions should guide the user or preceding agent accordingly.'], 'expected_answer': 'The `input_schema` parameter defines a Pydantic `BaseModel` for the expected input structure. If set, the user message content must be a JSON string adhering to this schema, and instructions should guide users or preceding agents accordingly.', 'query': 'What is the role of `input_schema` in enforcing structured input for an `LlmAgent`?'}, {'citations': [\"* **`output_schema` (Optional):** Define a Pydantic `BaseModel` class representing the desired output structure. If set, the agent's final response *must* be a JSON string conforming to this schema.\", \"    * **Constraint:** Using `output_schema` enables controlled generation within the LLM but **disables the agent's ability to use tools or transfer control to other agents**. Your instructions must guide the LLM to produce JSON matching the schema directly.\"], 'expected_answer': 'The `output_schema` parameter defines a Pydantic `BaseModel` for the desired output JSON structure. While it enables controlled LLM generation, it restricts the agent from using tools or transferring control, requiring the LLM to directly produce JSON that matches the schema.', 'query': 'What are the limitations imposed by using `output_schema` on an `LlmAgent`?'}, {'citations': [\"* **`output_key` (Optional):** Provide a string key. If set, the text content of the agent's *final* response will be automatically saved to the session's state dictionary under this key (e.g., `session.state[output_key] = agent_response_text`). This is useful for passing results between agents or steps in a workflow.\"], 'expected_answer': \"The `output_key` parameter automatically saves the `LlmAgent`'s final response text to the session's state dictionary under the specified key, facilitating the transfer of results between agents or workflow steps.\", 'query': 'How does `output_key` enable result sharing in workflows?'}, {'citations': [\"* **`include_contents` (Optional, Default: `'default'`):** Determines if the `contents` (history) are sent to the LLM.\", \"    * `'default'`: The agent receives the relevant conversation history.\", \"    * `'none'`: The agent receives no prior `contents`. It operates based solely on its current instruction and any input provided in the *current* turn (useful for stateless tasks or enforcing specific contexts).\"], 'expected_answer': \"The `include_contents` parameter controls whether conversation history is sent to the LLM. `'default'` includes relevant history, while `'none'` makes the agent operate solely on current input and instructions, suitable for stateless tasks.\", 'query': \"What is the impact of `include_contents='none'` on an `LlmAgent`'s operation?\"}, {'citations': ['* **`planner` (Optional):** Assign a `BasePlanner` instance to enable multi-step reasoning and planning before execution. (See [Multi-Agents](multi-agents.md) patterns).', \"* **`code_executor` (Optional):** Provide a `BaseCodeExecutor` instance to allow the agent to execute code blocks (e.g., Python) found in the LLM's response. ([See Tools/Built-in tools](../tools/built-in-tools.md)).\"], 'expected_answer': \"An `LlmAgent` can be configured with a `BasePlanner` instance for multi-step reasoning and planning, and a `BaseCodeExecutor` instance to enable execution of code blocks from the LLM's response.\", 'query': 'What are the functionalities provided by `planner` and `code_executor` in an `LlmAgent`?'}, {'citations': ['The `google-genai` library, used internally by ADK for Gemini, can connect through either Google AI Studio or Vertex AI.'], 'expected_answer': 'The `google-genai` library, used by ADK for Gemini, supports connections via Google AI Studio or Vertex AI.', 'query': 'What are the two connection options for the `google-genai` library when integrating Gemini with ADK?'}, {'citations': ['In order to use voice/video streaming in ADK, you will need to use Gemini models that support the Live API.'], 'expected_answer': 'To use voice/video streaming in ADK, Gemini models must support the Live API.', 'query': 'What is a prerequisite for enabling voice/video streaming with Gemini models in ADK?'}, {'citations': ['Google AI Studio is the easiest way to get started with Gemini. All you need is the [API key](https://aistudio.google.com/app/apikey). Best for rapid prototyping and development.'], 'expected_answer': 'Google AI Studio is the simplest way to begin with Gemini, requiring only an API key, and is ideal for rapid prototyping and development.', 'query': 'What makes Google AI Studio a good choice for rapid prototyping with Gemini?'}, {'citations': ['Vertex AI is recommended for production applications, leveraging Google Cloud infrastructure. Gemini on Vertex AI supports enterprise-grade features, security, and compliance controls.'], 'expected_answer': 'Vertex AI is recommended for production applications because it offers enterprise-grade features, security, and compliance controls, leveraging Google Cloud infrastructure.', 'query': 'Why is Vertex AI the recommended platform for production applications using Gemini?'}, {'citations': ['To access a vast range of LLMs from providers like OpenAI, Anthropic (non-Vertex AI), Cohere, and many others, ADK offers integration through the LiteLLM library.'], 'expected_answer': 'ADK integrates with a wide range of LLMs from providers such as OpenAI, Anthropic (non-Vertex AI), and Cohere via the LiteLLM library.', 'query': 'Which library does ADK use to integrate with various LLM providers?'}, {'citations': ['[LiteLLM](https://docs.litellm.ai/) acts as a translation layer, providing a standardized, OpenAI-compatible interface to over 100+ LLMs.'], 'expected_answer': 'LiteLLM serves as a translation layer, offering a standardized, OpenAI-compatible interface to over 100 LLMs.', 'query': \"What is the primary role of LiteLLM in ADK's model integration?\"}, {'citations': ['If your agent is relying on tools, please make sure that you select a model with tool support from [Ollama website](https://ollama.com/search?c=tools).'], 'expected_answer': 'For agents relying on tools, ensure the selected Ollama model has tool support, which can be verified on the Ollama website.', 'query': 'What is a crucial requirement for Ollama models when used with ADK agents that utilize tools?'}, {'citations': ['It is important to set the provider `ollama_chat` instead of `ollama`. Using `ollama` will result in unexpected behaviors such as infinite tool call loops and ignoring previous context.'], 'expected_answer': 'It is important to use `ollama_chat` as the provider instead of `ollama` to prevent issues like infinite tool call loops and context loss.', 'query': 'What are the consequences of using `ollama` instead of `ollama_chat` as the provider?'}, {'citations': ['While `api_base` can be provided inside LiteLLM for generation, LiteLLM library is calling other APIs relying on the env variable instead as of v1.65.5 after completion. So at this time, we recommend setting the env variable `OLLAMA_API_BASE` to point to the ollama server.'], 'expected_answer': 'Setting the `OLLAMA_API_BASE` environment variable to the Ollama server is recommended because the LiteLLM library, as of v1.65.5, relies on this variable for API calls after completion, even if `api_base` is provided internally.', 'query': 'Why is it advised to set the `OLLAMA_API_BASE` environment variable for Ollama integration?'}, {'citations': ['Alternatively, `openai` can be used as the provider name. But this will also require setting the `OPENAI_API_BASE=http://localhost:11434/v1` and `OPENAI_API_KEY=anything` env variables instead of `OLLAMA_API_BASE`. **Please note that api base now has `/v1` at the end.**'], 'expected_answer': 'Using `openai` as the provider name for Ollama models requires setting `OPENAI_API_BASE` to `http://localhost:11434/v1` and `OPENAI_API_KEY` to any value, replacing `OLLAMA_API_BASE`.', 'query': 'What environment variables are necessary when using `openai` as the provider for Ollama models?'}, {'citations': ['Tools such as [vLLM](https://github.com/vllm-project/vllm) allow you to host models efficiently and often expose an OpenAI-compatible API endpoint.'], 'expected_answer': 'vLLM enables efficient model hosting and typically provides an OpenAI-compatible API endpoint.', 'query': 'What is the main advantage of using vLLM for model hosting?'}, {'citations': ['* *Important for ADK Tools:* When deploying, ensure the serving tool supports and enables OpenAI-compatible tool/function calling. For vLLM, this might involve flags like `--enable-auto-tool-choice` and potentially a specific `--tool-call-parser`, depending on the model.'], 'expected_answer': \"When deploying models with vLLM for ADK Tools, it's crucial to ensure the serving tool supports OpenAI-compatible tool/function calling, potentially requiring specific flags like `--enable-auto-tool-choice` and a `--tool-call-parser`.\", 'query': 'What are the critical considerations for vLLM deployments to ensure compatibility with ADK Tools?'}, {'citations': [\"For enterprise-grade scalability, reliability, and integration with Google Cloud's MLOps ecosystem, you can use models deployed to Vertex AI Endpoints. This includes models from Model Garden or your own fine-tuned models.\"], 'expected_answer': \"Models deployed to Vertex AI Endpoints, including those from Model Garden or fine-tuned models, offer enterprise-grade scalability, reliability, and integration with Google Cloud's MLOps ecosystem.\", 'query': 'What are the key benefits of using Vertex AI Endpoints for model deployment?'}, {'citations': ['Some providers, like Anthropic, make their models available directly through Vertex AI.'], 'expected_answer': 'Yes, some providers, such as Anthropic, offer their models directly through Vertex AI.', 'query': 'Are there any third-party LLM providers whose models are directly available through Vertex AI?'}, {'citations': [\"ADK's registry automatically recognizes `gemini-*` strings and standard Vertex AI endpoint strings (`projects/.../endpoints/...`) and routes them via the `google-genai` library. For other model types used directly via Vertex AI (like Claude), you must explicitly tell the ADK registry which specific wrapper class (`Claude` in this case) knows how to handle that model identifier string with the Vertex AI backend.\"], 'expected_answer': \"ADK's registry automatically handles `gemini-*` and standard Vertex AI endpoint strings via `google-genai`. However, for other Vertex AI model types (e.g., Claude), explicit registration of the corresponding wrapper class is required to inform the ADK registry how to handle that model identifier.\", 'query': 'What is the process for registering non-Gemini Vertex AI models with ADK?'}, {'citations': ['The Agent Development Kit (ADK) supports building sophisticated applications by composing multiple, distinct `BaseAgent` instances into a **Multi-Agent System (MAS)**.'], 'expected_answer': 'ADK supports building sophisticated applications by composing multiple, distinct `BaseAgent` instances into a Multi-Agent System (MAS).', 'query': 'What is the primary architectural approach ADK supports for complex applications?'}, {'citations': ['In ADK, a multi-agent system is an application where different agents, often forming a hierarchy, collaborate or coordinate to achieve a larger goal.'], 'expected_answer': 'A multi-agent system in ADK is an application where various agents, often organized hierarchically, collaborate or coordinate to achieve a common goal.', 'query': 'How are agents organized and what is their purpose in a multi-agent system?'}, {'citations': ['Structuring your application this way offers significant advantages, including enhanced modularity, specialization, reusability, maintainability, and the ability to define structured control flows using dedicated workflow agents.'], 'expected_answer': 'Structuring an application as a multi-agent system offers enhanced modularity, specialization, reusability, maintainability, and the ability to define structured control flows using workflow agents.', 'query': 'What are the main advantages of using a multi-agent system architecture in ADK?'}, {'citations': ['The foundation for structuring multi-agent systems is the parent-child relationship defined in `BaseAgent`.'], 'expected_answer': 'The parent-child relationship defined in `BaseAgent` forms the foundation for structuring multi-agent systems.', 'query': 'What is the foundational concept for organizing multi-agent systems in ADK?'}, {'citations': ['You create a tree structure by passing a list of agent instances to the `sub_agents` argument when initializing a parent agent. ADK automatically sets the `parent_agent` attribute on each child agent during initialization (`google.adk.agents.base_agent.py` - `model_post_init`).'], 'expected_answer': 'A tree structure is created by providing a list of agent instances to the `sub_agents` argument during parent agent initialization, and ADK automatically assigns the `parent_agent` attribute to each child.', 'query': 'How is the hierarchical structure of agents established in ADK?'}, {'citations': ['An agent instance can only be added as a sub-agent once. Attempting to assign a second parent will result in a `ValueError`.'], 'expected_answer': 'An agent instance can only be a sub-agent to a single parent; attempting to assign it to another parent will raise a `ValueError`.', 'query': 'What is the rule regarding an agent instance having multiple parents?'}, {'citations': ['This hierarchy defines the scope for [Workflow Agents](#22-workflow-agents-as-orchestrators) and influences the potential targets for LLM-Driven Delegation. You can navigate the hierarchy using `agent.parent_agent` or find descendants using `agent.find_agent(name)`.'], 'expected_answer': 'The agent hierarchy defines the scope for Workflow Agents and influences LLM-Driven Delegation targets. It can be navigated using `agent.parent_agent` or by finding descendants with `agent.find_agent(name)`.', 'query': 'What are the practical implications of the agent hierarchy in ADK?'}, {'citations': [\"ADK includes specialized agents derived from `BaseAgent` that don't perform tasks themselves but orchestrate the execution flow of their `sub_agents`.\"], 'expected_answer': \"ADK's specialized agents, derived from `BaseAgent`, orchestrate the execution flow of their `sub_agents` rather than performing tasks directly.\", 'query': 'What is the primary characteristic of specialized agents in ADK?'}, {'citations': ['Executes its `sub_agents` one after another in the order they are listed.'], 'expected_answer': 'A `SequentialAgent` executes its `sub_agents` sequentially, following the order in which they are listed.', 'query': 'Describe the execution order of sub-agents in a `SequentialAgent`.'}, {'citations': ['Passes the *same* [`InvocationContext`](../runtime/index.md) sequentially, allowing agents to easily pass results via shared state.'], 'expected_answer': 'A `SequentialAgent` passes the same `InvocationContext` to its sub-agents sequentially, enabling them to share results through a common state.', 'query': 'How does `SequentialAgent` facilitate data sharing among its sub-agents?'}, {'citations': ['Executes its `sub_agents` in parallel. Events from sub-agents may be interleaved.'], 'expected_answer': 'A `ParallelAgent` executes its `sub_agents` concurrently, and events generated by these sub-agents may occur in an interleaved fashion.', 'query': 'What is the concurrency behavior of a `ParallelAgent`?'}, {'citations': ['Modifies the `InvocationContext.branch` for each child agent (e.g., `ParentBranch.ChildName`), providing a distinct contextual path which can be useful for isolating history in some memory implementations.'], 'expected_answer': 'The `InvocationContext.branch` is modified for each child agent in a `ParallelAgent` to provide a distinct contextual path, which can help in isolating history within certain memory implementations.', 'query': 'Why is `InvocationContext.branch` modified for child agents in a `ParallelAgent`?'}, {'citations': ['Despite different branches, all parallel children access the *same shared* `session.state`, enabling them to read initial state and write results (use distinct keys to avoid race conditions).'], 'expected_answer': \"All parallel children agents share the same `session.state`, allowing them to read initial state and write results. However, it's important to use distinct keys when writing to avoid race conditions.\", 'query': 'How do parallel children agents interact with the `session.state`?'}, {'citations': ['Executes its `sub_agents` sequentially in a loop.'], 'expected_answer': 'A `LoopAgent` executes its `sub_agents` sequentially within a loop.', 'query': 'Describe the looping mechanism of a `LoopAgent`.'}, {'citations': ['The loop stops if the optional `max_iterations` is reached, or if any sub-agent yields an [`Event`](../events/index.md) with `actions.escalate=True`.'], 'expected_answer': 'A `LoopAgent` terminates either when it reaches the specified `max_iterations` or when any of its sub-agents yields an `Event` with `actions.escalate=True`.', 'query': 'Under what conditions does a `LoopAgent` cease execution?'}, {'citations': ['Passes the *same* `InvocationContext` in each iteration, allowing state changes (e.g., counters, flags) to persist across loops.'], 'expected_answer': 'A `LoopAgent` maintains state changes across iterations by passing the same `InvocationContext` in each loop, allowing elements like counters or flags to persist.', 'query': 'How does a `LoopAgent` ensure persistence of state changes?'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "parsed = json.loads(result)\n",
    "print(parsed[\"test_cases\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cb3d10",
   "metadata": {},
   "source": [
    "# Turn into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "264b35e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "citations",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "expected_answer",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "query",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "fa6777bb-8d01-4562-b82e-e01d80f5b5e7",
       "rows": [
        [
         "0",
         "['# Code owners file.', 'This file controls who is tagged for review for any given pull request.', '*                                      @GoogleCloudPlatform/teams/generative-ai-devrel']",
         "The code owner for all files is the GitHub team `@GoogleCloudPlatform/teams/generative-ai-devrel`.",
         "Who is the code owner for this repository?"
        ],
        [
         "1",
         "['name: Bug report', 'about: Create a report to help us improve', \"title: ''\", \"labels: ''\", \"assignees: ''\", '**Describe the bug**', 'A clear and concise description of what the bug is.', '**To Reproduce**', 'Steps to reproduce the behavior:', \"1. Go to '...'\", \"2. Click on '....'\", '3. See error', '**Expected behavior**', 'A clear and concise description of what you expected to happen.', '**Screenshots**', 'If applicable, add screenshots to help explain your problem.', '**Versions**', ' - OS: [e.g. Windows, Mac, Linux]', ' - ADK version:', ' - Python version:', '**Additional context**', 'Add any other context about the problem here.']",
         "The bug report template includes sections for describing the bug, steps to reproduce, expected behavior, screenshots, versions (OS, ADK, Python), and additional context.",
         "What information should be included in a bug report?"
        ],
        [
         "2",
         "['name: Feature request', 'about: Suggest an idea for this project', \"title: ''\", \"labels: ''\", \"assignees: ''\", '**Is your feature request related to a problem? Please describe.**', 'A clear and concise description of what the problem is.', \"**Describe the solution you'd like**\", 'A clear and concise description of what you want to happen.', \"**Describe alternatives you've considered**\", \"A clear and concise description of any alternative solutions or features you've considered.\", '**Additional context**', 'Add any other context or screenshots about the feature request here.']",
         "A feature request should describe the problem it's related to, the desired solution, any alternative solutions considered, and additional context or screenshots.",
         "What are the key sections of a feature request?"
        ],
        [
         "3",
         "['allowedCopyrightHolders:', '  - \"Google LLC\"', 'allowedLicenses:', '  - \"Apache-2.0\"', 'ignoreFiles:', '  - \"**/requirements*.txt\"', '  - \"**/__init__.py\"', '  - \"**/constraints*.txt\"', 'ignoreLicenseYear: true', 'sourceFileExtensions:', '  - \"py\"']",
         "The header checker lint configuration allows \"Google LLC\" as a copyright holder and \"Apache-2.0\" as a license. It ignores license year and processes files with the \".py\" extension. Files matching \"**/requirements*.txt\", \"**/__init__.py\", and \"**/constraints*.txt\" are ignored.",
         "What are the configurations for the header checker lint?"
        ],
        [
         "4",
         "['  \"extends\": [', '    \"config:recommended\"', '  ],', '  \"prConcurrentLimit\": 0,', '  \"rebaseWhen\": \"never\",', '  \"dependencyDashboard\": true']",
         "The Renovate configuration extends `config:recommended`, sets `prConcurrentLimit` to 0, `rebaseWhen` to \"never\", and enables `dependencyDashboard`.",
         "What are the general settings in the Renovate configuration file?"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citations</th>\n",
       "      <th>expected_answer</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[# Code owners file., This file controls who i...</td>\n",
       "      <td>The code owner for all files is the GitHub tea...</td>\n",
       "      <td>Who is the code owner for this repository?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[name: Bug report, about: Create a report to h...</td>\n",
       "      <td>The bug report template includes sections for ...</td>\n",
       "      <td>What information should be included in a bug r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[name: Feature request, about: Suggest an idea...</td>\n",
       "      <td>A feature request should describe the problem ...</td>\n",
       "      <td>What are the key sections of a feature request?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[allowedCopyrightHolders:,   - \"Google LLC\", a...</td>\n",
       "      <td>The header checker lint configuration allows \"...</td>\n",
       "      <td>What are the configurations for the header che...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[  \"extends\": [,     \"config:recommended\",   ]...</td>\n",
       "      <td>The Renovate configuration extends `config:rec...</td>\n",
       "      <td>What are the general settings in the Renovate ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           citations  \\\n",
       "0  [# Code owners file., This file controls who i...   \n",
       "1  [name: Bug report, about: Create a report to h...   \n",
       "2  [name: Feature request, about: Suggest an idea...   \n",
       "3  [allowedCopyrightHolders:,   - \"Google LLC\", a...   \n",
       "4  [  \"extends\": [,     \"config:recommended\",   ]...   \n",
       "\n",
       "                                     expected_answer  \\\n",
       "0  The code owner for all files is the GitHub tea...   \n",
       "1  The bug report template includes sections for ...   \n",
       "2  A feature request should describe the problem ...   \n",
       "3  The header checker lint configuration allows \"...   \n",
       "4  The Renovate configuration extends `config:rec...   \n",
       "\n",
       "                                               query  \n",
       "0         Who is the code owner for this repository?  \n",
       "1  What information should be included in a bug r...  \n",
       "2    What are the key sections of a feature request?  \n",
       "3  What are the configurations for the header che...  \n",
       "4  What are the general settings in the Renovate ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(parsed[\"test_cases\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df81cb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder the columns, query first, then expected_answer, then citations\n",
    "df = df[[\"query\", \"expected_answer\", \"citations\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "356cbae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"../tests/longADKDocsTestCases.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c92373",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
